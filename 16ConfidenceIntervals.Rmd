# Confidence Intervals

```{r}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction: Why Point Estimates Aren't Enough {-}

Imagine you're working on a story for the student newspaper about sleep habits on campus. You survey 50 randomly selected students and find that they sleep an average of 6.8 hours per night. You want to report this finding, but you know this is just one sample from the larger population of all students at your university. How can you report this number so that it reflects your uncertainty about the true average for all students?

This is the fundamental question that confidence intervals help us answer. While a point estimate like 6.8 hours is useful, it's incomplete. It doesn't tell us anything about the uncertainty in our estimate. A confidence interval gives us a range of plausible values for the true population parameter. And we know that this range of values has certain properties derived from probability theory.

By the end of this lesson, you'll be able to construct confidence intervals for population means under different scenarios, interpret what those intervals mean, and understand the tradeoffs involved in choosing different confidence levels.

## Review: From Samples to Populations {-}

Before we dive into confidence intervals, let's review some key concepts about statistical inference. Remember that we use sample statistics to make inferences about population parameters. The notation is important here, so let's be clear about what represents what.

When we talk about samples versus populations, we use different symbols to keep track of which is which. The sample mean is denoted $\bar{x}$ (or sometimes $\bar{y}$), while the population mean is denoted $\mu$. Similarly, the sample standard deviation is $s$, while the population standard deviation is $\sigma$. This distinction matters because we almost never know the population parameters—if we did, we wouldn't need to do statistical inference!

English term|Sample Statistic | Population parameter
:-----:|:-----:|:-----:
Mean | $\bar{x}$ | $\mu$
Standard deviation | $s$ | $\sigma$
Variance | $s^2$ | $\sigma^2$

The Central Limit Theorem tells us something remarkable: even if the population isn't normally distributed, the distribution of sample means becomes approximately normal as the sample size increases. This is why we can use the normal distribution to construct confidence intervals for means, at least when our sample size is reasonably large.

## Understanding Confidence Intervals {-}

A confidence interval is an interval estimate of a population parameter. Unlike a point estimate, which is just a single number, an interval estimate gives us a range of plausible values. But what does it mean when we say we have a "95% confidence interval"?

### The Correct Interpretation

Here's where things get a bit subtle, and it's important to understand this correctly. A 95% confidence interval does not mean there is a 95% probability that the true parameter falls within this particular interval. Once we've calculated a specific interval, the true parameter either is or isn't in that interval—we just don't know which.

Instead, the 95% refers to the procedure we used to construct the interval. If we were to take many, many random samples from the population and calculate a 95% confidence interval for each sample, then about 95% of those intervals would contain the true population parameter. It's a statement about the long-run behavior of the *method*, not about any particular interval.

Think of it this way: imagine you surveyed 100 different random samples of students about their sleep habits, and you calculated a 95% confidence interval for each sample. You'd expect that about 95 of those intervals would capture the true average sleep time for all students, while about 5 of them would miss it. Before you collect any particular sample, you can be 95% confident that the interval you calculate will be one of the ones that captures the true value.


We can see this by simulating random samples in R.  In this simulation, we take 20 different random samples and calculated a 95% confidence interval for each one. Notice that most intervals (shown in green) contain the true population mean (the blue dashed line), but one (shown in red) misses it. This is exactly what we expect—about 95% capture the truth.


```{r, echo=FALSE, fig.width=8, fig.height=5, warning=FALSE}
# Simulate confidence intervals to illustrate the concept
set.seed(123)
true_mean <- 6.8
true_sd <- 1.5
n_samples <- 20
sample_size <- 50

ci_data <- data.frame(
  sample_id = 1:n_samples,
  sample_mean = numeric(n_samples),
  lower = numeric(n_samples),
  upper = numeric(n_samples),
  contains_true = logical(n_samples)
)

for(i in 1:n_samples) {
  sample_data <- rnorm(sample_size, mean = true_mean, sd = true_sd)
  sample_mean <- mean(sample_data)
  sample_sd <- sd(sample_data)
  se <- sample_sd / sqrt(sample_size)
  margin <- 1.96 * se
  
  ci_data$sample_mean[i] <- sample_mean
  ci_data$lower[i] <- sample_mean - margin
  ci_data$upper[i] <- sample_mean + margin
  ci_data$contains_true[i] <- (ci_data$lower[i] <= true_mean) & (true_mean <= ci_data$upper[i])
}

ggplot(ci_data, aes(x = sample_id, y = sample_mean, color = contains_true)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.3) +
  geom_hline(yintercept = true_mean, linetype = "dashed", color = "blue", size = 1) +
  scale_color_manual(values = c("TRUE" = "darkgreen", "FALSE" = "red"),
                     labels = c("TRUE" = "Captures true mean", "FALSE" = "Misses true mean")) +
  coord_flip() +
  labs(title = "20 Different 95% Confidence Intervals from 20 Different Samples",
       subtitle = "Blue dashed line shows the true population mean",
       x = "Sample Number",
       y = "Sleep Hours",
       color = "") +
  theme_minimal() +
  theme(legend.position = "bottom")
```


This interpretation is admittedly a bit confusing at first, but it's the technically correct one. In practice, when we report a 95% confidence interval, we often say something like "we are 95% confident that the true population mean falls between X and Y." This is a reasonable shorthand, but just remember that it's not really correct.  Confidence is really a statement about the reliability of our method, not a probability statement about any particular interval.


## The Structure of a Confidence Interval {-}

Every confidence interval has the same basic structure. It starts with a point estimate (our best single guess at the parameter) and then adds and subtracts a margin of error. The general form is:

$$\text{point estimate} \pm \text{margin of error}$$

For means, the point estimate is the sample mean $\bar{x}$. The margin of error depends on two things: how much variability there is in the sampling distribution (measured by the standard error), and how confident we want to be (captured by a critical value from either the normal or t-distribution).

$$\text{Confidence Interval} = \bar{x} \pm (\text{critical value}) \times (\text{standard error})$$

The margin of error is calculated as the critical value multiplied by the standard error. The critical value depends on the confidence level we choose. For a 95% confidence level using the normal distribution, the critical value is approximately 1.96. For a 90% confidence level, it's about 1.645. For a 99% confidence level, it's about 2.576.

```{r, echo=FALSE}
# Create a table of common confidence levels and z-scores
conf_levels <- data.frame(
  `Confidence Level` = c("90%", "95%", "99%"),
  `Alpha` = c("0.10", "0.05", "0.01"),
  `Alpha/2` = c("0.05", "0.025", "0.005"),
  `Z Critical Value` = c("1.645", "1.960", "2.576"),
  check.names = FALSE
)
knitr::kable(conf_levels, align = 'c')
```

Notice a pattern here? As we want to be more confident that we've captured the true parameter, our critical value gets larger, which makes our margin of error larger, which makes our confidence interval wider. This is the fundamental tradeoff: we can be more confident by casting a wider net, but that wider net gives us a less precise estimate.

## Building Confidence Intervals: Three Scenarios {-}

The exact formula we use to construct a confidence interval depends on what we know about the population and how large our sample is. There are three main scenarios we need to consider, each requiring a slightly different approach.

### Scenario A: When We Know the Population Standard Deviation {-}

Sometimes—though not often in real research—we know the population standard deviation $\sigma$. In this case, we can calculate the exact standard error of the sampling distribution using the formula:

$$\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$$

This tells us how much variability we expect in sample means if we were to take many samples of size $n$ from the population. The confidence interval is then:

$$\bar{x} \pm z \times \frac{\sigma}{\sqrt{n}}$$

where $z$ is the critical value from the standard normal distribution corresponding to our desired confidence level.

#### Example: Sleep Study with Known σ

Let's return to our sleep study example. Suppose we surveyed 50 students and found a sample mean of 6.8 hours of sleep per night. Let's also suppose (unrealistically) that we somehow know the population standard deviation is exactly 1.5 hours. We want to construct a 95% confidence interval.

First, we calculate the standard error:

```{r}
n <- 50
sigma <- 1.5
x_bar <- 6.8

se_known <- sigma / sqrt(n)
print("Standard Error")
round(se_known, 3)
```

The standard error is about 0.21 hours. Now we need the critical value for a 95% confidence level. For the normal distribution, this is 1.96, but we can get R to calculate it precisely using the `qnorm()` function. For a 95% confidence interval, we want the z-score that leaves 2.5% in each tail (since 100% - 95% = 5%, and we split that equally between the two tails).

```{r}
z_critical <- qnorm(0.975)
print("Critical Value")
round(z_critical, 3)
```

Now we can calculate the margin of error and construct the confidence interval:

```{r}
margin_of_error <- z_critical * se_known

lower_bound <- x_bar - margin_of_error
upper_bound <- x_bar + margin_of_error

cat("Margin of Error:", round(margin_of_error, 3), "hours\n")
cat("95% Confidence Interval: [", round(lower_bound, 2), ",", round(upper_bound, 2), "] hours\n")
```

So our 95% confidence interval is approximately [6.38, 7.22] hours. We interpret this by saying that if we used this procedure many times with different samples, about 95% of the intervals we construct would contain the true average sleep time for all students at the university.

Notice that we used `qnorm(0.975)` rather than `qnorm(0.95)`. This is because the normal distribution is symmetric, and we want 95% in the middle, which means 2.5% in each tail. The `qnorm()` function with 0.975 gives us the z-score that has 97.5% of the distribution to the left of it, which is equivalent to leaving 2.5% in the right tail.

### Scenario B: Unknown Population Standard Deviation, Large Sample {-}

In reality, we almost never know the population standard deviation. This is much more common—we have to estimate $\sigma$ using our sample standard deviation $s$. When we do this, our estimate of the standard error becomes:

$$\hat{\sigma}_{\bar{x}} = \frac{s}{\sqrt{n}}$$

The hat notation ($\hat{\sigma}$) indicates that this is an estimate rather than a known value. Now here's a key question: should we still use the normal distribution for our critical values?

If our sample size is large (generally, $n \geq 30$), the Central Limit Theorem tells us that the sampling distribution of the mean is approximately normal, even if the population isn't normal. Moreover, when we have a large sample, our estimate $s$ is usually pretty close to the true $\sigma$, so using the normal distribution for critical values is reasonable. This is why, for large samples with unknown $\sigma$, we still use z-scores from the normal distribution.

#### Example: Sleep Study with Estimated σ

Let's continue with our sleep study example, but now assume we don't know the population standard deviation. Instead, we calculated a sample standard deviation of $s = 1.6$ hours from our 50 students.

```{r}
s <- 1.6
se_estimated <- s / sqrt(n)
round(se_estimated, 3)
```

Notice that our estimated standard error (0.226) is close to but not exactly the same as our known standard error from before (0.212). Now we construct the confidence interval the same way as before:

```{r}
margin_of_error_large <- z_critical * se_estimated
lower_bound_large <- x_bar - margin_of_error_large
upper_bound_large <- x_bar + margin_of_error_large

cat("95% Confidence Interval: [", round(lower_bound_large, 2), ",", round(upper_bound_large, 2), "] hours\n")
```

Our interval is [6.36, 7.24] hours, which is slightly wider than before because we had to estimate the standard deviation. This is the price we pay for not knowing the population parameter—a bit more uncertainty in our interval.

### Scenario C: Unknown Population Standard Deviation, Small Sample {-}

Now we come to the trickiest scenario, and one that's quite common in real research: we have a small sample (say, $n < 30$) and we don't know the population standard deviation. In this case, using the normal distribution isn't quite right. Because we're estimating the standard deviation from a small sample, there's additional uncertainty that we need to account for.

This is where the t-distribution comes in. You learned about the t-distribution in the previous lesson on probability distributions. Recall that the t-distribution looks similar to the normal distribution but has thicker tails, which reflects the extra uncertainty we have when working with small samples. As the sample size increases, the t-distribution gets closer and closer to the normal distribution.

```{r, echo=FALSE, fig.width=8, fig.height=4}
x_vals <- seq(-4, 4, length.out = 200)
normal_data <- data.frame(x = x_vals, y = dnorm(x_vals), dist = "Normal (z)")
t_data_5 <- data.frame(x = x_vals, y = dt(x_vals, df = 5), dist = "t (df = 5)")
t_data_14 <- data.frame(x = x_vals, y = dt(x_vals, df = 14), dist = "t (df = 14)")
t_data_29 <- data.frame(x = x_vals, y = dt(x_vals, df = 29), dist = "t (df = 29)")

plot_data <- rbind(normal_data, t_data_5, t_data_14, t_data_29)

ggplot(plot_data, aes(x = x, y = y, color = dist, linetype = dist)) +
  geom_line(size = 1) +
  scale_color_manual(values = c("Normal (z)" = "#2E86AB", 
                                 "t (df = 5)" = "#C73E1D",
                                 "t (df = 14)" = "#F18F01",
                                 "t (df = 29)" = "#A23B72")) +
  scale_linetype_manual(values = c("Normal (z)" = "solid",
                                    "t (df = 5)" = "dashed",
                                    "t (df = 14)" = "dotted",
                                    "t (df = 29)" = "dotdash")) +
  labs(title = "The t-Distribution Has Thicker Tails Than Normal",
       subtitle = "As degrees of freedom increase, t approaches normal",
       x = "Value",
       y = "Density",
       color = "Distribution",
       linetype = "Distribution") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

The key parameter for the t-distribution is the degrees of freedom, which for a single mean is $df = n - 1$. With fewer degrees of freedom, the t-distribution has thicker tails (more probability in the extremes), which means larger critical values and wider confidence intervals. This is exactly what we want—when we have less data, we should be less certain, and our intervals should be wider to reflect that uncertainty.

#### Example: Sleep Study with Small Sample

Let's modify our sleep study example. Suppose instead of surveying 50 students, we only managed to survey 15 students, and we found a sample mean of 6.8 hours with a sample standard deviation of 1.6 hours.

```{r}
n_small <- 15
s_small <- 1.6
x_bar_small <- 6.8

se_small <- s_small / sqrt(n_small)
se_small
```

The standard error is now larger (0.413) because we're dividing by the square root of a smaller number. Now for the critical value, we use the `qt()` function instead of `qnorm()`, and we need to specify the degrees of freedom:

```{r}
df <- n_small - 1
t_critical <- qt(0.975, df = df)
t_critical
```

Notice that the t-critical value (2.145) is larger than the z-critical value (1.96) we used before. This reflects the additional uncertainty in this setting. Now we construct the confidence interval:

```{r}
margin_of_error_small <- t_critical * se_small
lower_bound_small <- x_bar_small - margin_of_error_small
upper_bound_small <- x_bar_small + margin_of_error_small

cat("95% Confidence Interval: [", round(lower_bound_small, 2), ",", round(upper_bound_small, 2), "] hours\n")
```

Our 95% confidence interval is [5.91, 7.69] hours. This interval is much wider than what we got with 50 observations, reflecting both the larger standard error from having fewer observations and the larger critical value from using the t-distribution. This is exactly as it should be—with less data, we're less certain about where the true population mean lies.

The logic behind using `qt(0.975, df = 14)` is the same as before: we want 95% in the middle of the distribution, so 2.5% in each tail, which means we want the t-value with 97.5% of the distribution to the left of it.

## Comparing Confidence Levels {-}

So far we've focused on 95% confidence intervals, but we can construct intervals at any confidence level. The choice of confidence level involves a tradeoff between confidence and precision. Let's explore this using our large-sample sleep study example (n = 50, $\bar{x}$ = 6.8, s = 1.6).

First, let's construct a 90% confidence interval:

```{r}
z_90 <- qnorm(0.95)
se <- s / sqrt(n)

lower_90 <- x_bar - z_90 * se
upper_90 <- x_bar + z_90 * se

width_90 <- upper_90 - lower_90
cat("90% Confidence Interval: [", round(lower_90, 2), ",", round(upper_90, 2), "]\n")
cat("Width:", round(width_90, 2), "hours\n\n")
```

Now a 95% confidence interval:

```{r}
z_95 <- qnorm(0.975)

lower_95 <- x_bar - z_95 * se
upper_95 <- x_bar + z_95 * se

width_95 <- upper_95 - lower_95
cat("95% Confidence Interval: [", round(lower_95, 2), ",", round(upper_95, 2), "]\n")
cat("Width:", round(width_95, 2), "hours\n\n")
```

And finally a 99% confidence interval:

```{r}
z_99 <- qnorm(0.995)

lower_99 <- x_bar - z_99 * se
upper_99 <- x_bar + z_99 * se

width_99 <- upper_99 - lower_99
cat("99% Confidence Interval: [", round(lower_99, 2), ",", round(upper_99, 2), "]\n")
cat("Width:", round(width_99, 2), "hours\n")
```

Notice the pattern: as we increase our confidence level from 90% to 95% to 99%, our intervals get wider. This is the tradeoff: if you want to be more confident that you've captured the true parameter, you have to accept a less precise estimate.

```{r, echo=FALSE, fig.width=8, fig.height=4}
ci_comparison <- data.frame(
  level = factor(c("90%", "95%", "99%"), levels = c("90%", "95%", "99%")),
  lower = c(lower_90, lower_95, lower_99),
  upper = c(upper_90, upper_95, upper_99),
  point = c(x_bar, x_bar, x_bar)
)

ggplot(ci_comparison, aes(y = level)) +
  geom_point(aes(x = point), size = 3, color = "#2E86AB") +
  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, size = 1, color = "#2E86AB") +
  geom_vline(xintercept = x_bar, linetype = "dashed", color = "gray50") +
  labs(title = "Comparing Confidence Intervals at Different Confidence Levels",
       subtitle = "Same data, different confidence levels",
       x = "Sleep Hours",
       y = "Confidence Level") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank())
```

In practice, 95% confidence intervals are the most common, as they represent a reasonable balance between confidence and precision. However, the choice depends on the context and the consequences of being wrong. In exploratory research, 90% intervals might be fine. In medical research where decisions have serious consequences, researchers might prefer 99% intervals.


## A Complete Example: From Start to Finish {-}

Let's work through a complete example from beginning to end using real data. We'll use the congressional age data from FiveThirtyEight, which contains information about every member of the U.S. Congress from 1947 to 2013.

Suppose we're interested in estimating the average age of Congress members, but we can't analyze the entire dataset. Instead, we'll take a random sample of 25 Congress members and construct a 95% confidence interval for the true average age.

First, let's load the data and take our sample:

```{r, message=FALSE}
library(fivethirtyeight)
data("congress_age")

# Set seed for reproducibility
set.seed(42)

# Take a random sample of 25 Congress members
sample_congress <- congress_age[sample(nrow(congress_age), 25), ]

# Calculate sample statistics
sample_mean <- mean(sample_congress$age)
sample_sd <- sd(sample_congress$age)
sample_n <- nrow(sample_congress)

sample_mean
sample_sd
sample_n
```

Now we need to decide which scenario applies. We don't know the population standard deviation (we almost never do), and our sample size is 25, which is less than 30. This means we're in Scenario C—we should use the t-distribution.

Calculate the standard error:

```{r}
sample_se <- sample_sd / sqrt(sample_n)
sample_se
```

Find the critical value from the t-distribution:

```{r}
sample_df <- sample_n - 1
alpha <- 1 - 0.95
sample_t_critical <- qt(1 - alpha/2, df = sample_df)
sample_t_critical
```

Calculate the margin of error:

```{r}
sample_margin <- sample_t_critical * sample_se
sample_margin
```

Finally, construct the confidence interval:

```{r}
sample_lower <- sample_mean - sample_margin
sample_upper <- sample_mean + sample_margin

sample_lower
sample_upper
```

We can interpret this result as follows: based on our random sample of 25 Congress members, we are 95% confident that the true average age of all Congress members in our dataset is between these two values. If we repeated this sampling procedure many times and calculated a confidence interval each time, approximately 95% of those intervals would contain the true population mean.

Let's verify our interval against the actual population mean:

```{r}
# Calculate the true population mean
true_mean <- mean(congress_age$age)
true_mean

# Did our interval capture it?
sample_lower <= true_mean & true_mean <= sample_upper
```

In this case, our confidence interval successfully captured the true population mean! Of course, in real research we wouldn't know the true population mean—that's why we're constructing confidence intervals in the first place. But this exercise demonstrates how the method works.

## Decision Guide: Which Method Should You Use? {-}

Here's a systematic way to decide which approach to use when constructing a confidence interval for a mean:

**Step 1:** Do you know the population standard deviation $\sigma$? 

- **If yes** (which is rare), use Scenario A with the normal distribution
- **If no** (which is almost always the case), go to Step 2

**Step 2:** Is your sample size large ($n \geq 30$)? 

- **If yes**, use Scenario B with the normal distribution as an approximation
- **If no** (small sample), use Scenario C with the t-distribution

```{r, echo=FALSE, fig.width=8, fig.height=5}
library(DiagrammeR)
grViz("
digraph decision_tree {
  graph [rankdir = TB, fontsize = 12]
  node [shape = box, style = filled, fillcolor = lightblue]
  
  start [label = 'Do you know σ?', fillcolor = '#FFE5B4']
  yes_sigma [label = 'Scenario A:\\nUse z from\\nnormal distribution']
  no_sigma [label = 'Is n ≥ 30?', fillcolor = '#FFE5B4']
  large_n [label = 'Scenario B:\\nUse z from\\nnormal distribution']
  small_n [label = 'Scenario C:\\nUse t from\\nt-distribution']
  
  start -> yes_sigma [label = 'Yes']
  start -> no_sigma [label = 'No']
  no_sigma -> large_n [label = 'Yes']
  no_sigma -> small_n [label = 'No']
}
")
```

The boundary of $n = 30$ is somewhat arbitrary, but it's a useful rule of thumb. For sample sizes between 20 and 30, it usually doesn't matter much whether you use z or t critical values. For very small samples (say, $n < 15$), you definitely want to use the t-distribution.

One additional consideration: the Central Limit Theorem tells us that the sampling distribution of the mean is approximately normal for large samples, even if the population isn't normal. But for small samples, if you have strong reason to believe the population is very non-normal (highly skewed, for example), be cautious about constructing confidence intervals without additional analysis.

## Things to Look Out For {-}

Several common mistakes come up repeatedly when students first learn about confidence intervals. Being aware of these can help you avoid them.

### Common Mistakes

**Mistake 1: Misinterpreting the confidence level.** The most common mistake is saying "there is a 95% probability that the true mean is in this interval." Once you've calculated a specific interval, the true mean either is or isn't in it—there's no probability involved for that particular interval. The 95% refers to the long-run success rate of the procedure, not the probability for any particular interval you've calculated.

**Mistake 2: Forgetting to divide alpha by 2.** When using `qnorm()` or `qt()` to find critical values, remember that for a two-sided confidence interval, you need to split the alpha level between the two tails. For a 95% confidence interval, you use `qnorm(0.975)`, not `qnorm(0.95)`.

**Mistake 3: Using the wrong standard error formula.** Make sure you're dividing by the square root of n, not just by n. The standard error is $\frac{s}{\sqrt{n}}$, not $\frac{s}{n}$.

**Mistake 4: Using z when you should use t.** If you have a small sample and unknown population standard deviation, you should use the t-distribution. Using the normal distribution will make your confidence intervals too narrow, giving you false confidence in your estimate.

**Mistake 5: Forgetting about assumptions.** Confidence intervals for means assume that your sample is a random sample from the population. If your sample is biased or not representative, your confidence interval may not capture the true population parameter, regardless of what confidence level you choose.

### When Confidence Intervals Can Be Misleading

Confidence intervals are powerful tools, but they can be misleading in certain situations. Here's a real-world example that illustrates the importance of proper sampling.

In the run-up to the 2016 US presidential election, many polls showed Hillary Clinton ahead of Donald Trump. These polls typically reported margins of error around ±3 percentage points, which corresponds to a 95% confidence interval. Yet Trump won the election. What went wrong?

The issue wasn't with the mathematics of confidence intervals—it was with the sampling. Many polls failed to adequately represent certain groups of voters, particularly those without college degrees in key swing states. No amount of statistical sophistication can fix a biased sample. The confidence intervals accurately reflected sampling variability, but they couldn't account for the fact that the samples weren't truly representative of the voting population.

The lesson here is clear: a confidence interval is only as good as the data that goes into it. Before you trust a confidence interval, ask yourself whether the sample was truly random and representative of the population you care about.


## Review Questions: Confidence Intervals

### Conceptual Understanding

1. Explain in your own words what a 95% confidence interval means. Why is it incorrect to say "there is a 95% probability that the true mean is in this interval"?

2. What are the two components needed to calculate the margin of error? How does each component contribute to the width of the interval?

3. Why do confidence intervals get wider as we increase the confidence level (e.g., from 90% to 95% to 99%)? What is the tradeoff?

4. Explain the difference between precision and accuracy in the context of confidence intervals. Can you have a precise but inaccurate confidence interval?

5. What is the Central Limit Theorem and why is it important for constructing confidence intervals?

### Decision Making

6. You have a sample of n = 305 with unknown population standard deviation. Which distribution should you use to construct a confidence interval: normal (z) or t? Explain your reasoning.

7. You have a sample of n = 18 with unknown population standard deviation. Which distribution should you use? Why?

8. Under what circumstances (if any) would you know the population standard deviation σ in real research?

### Calculation Practice

9. A researcher samples 40 voters and finds a mean trust rating in Congress of 32.5 (on a 0-100 scale) with a standard deviation of 15.3. 
   - What is the standard error?
   - Calculate the 95% confidence interval.
   - Interpret this interval in a complete sentence.

10. A small pilot study surveys 10 students about sleep hours. The sample mean is 6.5 hours with a standard deviation of 1.8 hours.
    - Why must you use the t-distribution for this problem?
    - What are the degrees of freedom?
    - Construct a 90% confidence interval (you'll need to use R or a t-table).

11. You construct two confidence intervals from the same data: [45.2, 54.8] and [43.1, 56.9]. Without doing any calculations, which interval has the higher confidence level? How do you know?

### R Programming

12. What's wrong with this code for finding a 95% confidence interval critical value, and how would you fix it?
    ```r
    z_critical <- qnorm(0.95)
    ```

13. Write R code to construct a 99% confidence interval for a sample with mean = 78, sd = 12, and n = 50. Show all steps.


### Interpretation

14. A researcher reports: "The 95% confidence interval for average study time is [12.3, 18.7] hours per week." Explain what this means to someone who hasn't taken statistics.

15. Two studies both construct 95% confidence intervals for the same population parameter where we know the population variance $\sigam^2$. Study A gets [40, 60] while Study B gets [48, 52]. 
    - Which study had the larger sample size? How do you know?
    - Which interval is more useful? Why?

### Critical Thinking

16. A political poll reports a candidate's support at 52% with a margin of error of ±3 percentage points. The candidate loses the election with 48% of the vote. Does this mean the poll was wrong? Explain your reasoning considering what you know about confidence intervals.

17. Why do confidence intervals assume random sampling? What happens to the interpretation if your sample is biased, even if you calculate the interval correctly?

18. A researcher collects data from n = 25 with $\bar{x}$ = 100 and s = 15. They want to report a confidence interval but are unsure whether to use 90%, 95%, or 99% confidence. What factors should influence this decision?

