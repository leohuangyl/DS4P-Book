[["index.html", "Data Science for Politics 1 Preface", " Data Science for Politics Professor Montgomery 2025 1 Preface Welcome to the class. "],["introduction-to-r.html", "2 Introduction to R 2.1 Learning Objectives 2.2 Preliminary Notes 2.3 R as a calculator 2.4 Assigning values to objects 2.5 The global environment and how to clean it 2.6 Getting help 2.7 Installing packages", " 2 Introduction to R 2.1 Learning Objectives Learn how to do simple arithmetic in R. Learn how to assign a value to an object in R. What is the global environment? Help files Installing packages 2.2 Preliminary Notes This document is an “R Markdown” document, turned into a pdf file. R Markdown combines text and R code in a single file. We will cover R Markdown in lab sessions this week, and you will use these kind of files for both in-class and homework assignments. You will also be able to create, edit, and annotate these files yourself. Please type the code included in this file yourself into R Studio, rather than copy-paste it, to improve your learning experience. 2.3 R as a calculator The simplest way to use R is to do very basic calculations. R knows how to handle all of the basic math operations. (Side note: to comment a single line of code in R, which means to add notes that are not themselves part of the code, use the hashtag or pound sign at the beginning of the line. You can also add it after your code in the same line to comment out notes after a portion of code. If you’re working in an R script, everything after the # will not be read by R.) 5+4 # Addition ## [1] 9 6-3 # Subtraction ## [1] 3 34 / 6 # Division ## [1] 5.666667 5 * 3 # Multiplication ## [1] 15 5^4 # Exponents ## [1] 625 625^(1/4) # More exponents ## [1] 5 R comes with a number of constants pre-stored that you can use 6.25 # numbers pi # And a few other specialty items NA # Missing value NULL # Nothing. 0/0 # NaN means &quot;Not a number&quot; 1/0 # Inf means infinity R follows the order of operations (Please Excuse My Dear Aunt Sally). 2*(3-4)+2 ## [1] 0 2*(3-4)+2*(4 + 3)^(1/3) ## [1] 1.825862 Side note - if you have multiple exponentiation, they execute right to left. 2.4 Assigning values to objects R is a programming language that is based on objects. Pretty much everything can be saved as an object, and you can refer to objects to re-use this saved information. You can assign values to objects in two ways, as shown below. The assignment arrow is functionally equivalent to the equal sign. When assigning values to objects, R will always take the value on the right side of the assignment operator (&lt;- or =) and store it in the object on the left side of the assignment operator. This means that the two lines of commented code in the following snippet do different things. One assigns the value 4 to an object called x, one assigns the value 6 to an object called y. If you type the name of the object, R will return the information saved in that object. x &lt;- 4 x ## [1] 4 y = 6 y ## [1] 6 It is best practice when coding to avoid “magic numbers” - i.e. all numbers should be stored in named variables so that if we want to change the value, we only have to do so once. This also removes any ambiguity for someone else reading your code who might wonder what the number represents. So imagine that we know 238,307 voted in Alaska out of 496,387 people old enough to vote (the voting age population). We could calculate that directly, but it can make your code brittle. That is, if later on you find out that 238,407 voted you would have to replace that everywhere in your script. So instead we can assign these values to a variable name. We can then use these “objects” in other places in the script. total.votes.ak &lt;- 238307 voting.age.population.ak &lt;- 496387 turnout.ak &lt;-total.votes.ak/voting.age.population.ak turnout.ak ## [1] 0.4800831 Students new to R have a hard time understanding the importance of “assignment.” Roughly speaking, if you have R do something and don’t use an assignment operator (&lt;- or =), all you have done is print out the results. You haven’t actually done anything. For example, let’s say we want to change voting.age.population.ak to be 496388. A beginner’s mistake might be to use the code: voting.age.population.ak+1 ## [1] 496388 This looks like the right number, but because there was no assignment, the change wasn’t saved. If we go back and look at the value again, print(voting.age.population.ak) ## [1] 496387 we have the wrong number again. The right way to do this is to “overwrite” the original object this way: voting.age.population.ak &lt;- voting.age.population.ak+1 Now this change has been saved, because the object itself has been replaced. This example also illustrates why you might want to use &lt;- for assignments, rather than =, even though they are functionally equivalent in R. We are not writing mathematical equations. The above would be a problematic equation, but it is perfectly fine as an assignment. print(voting.age.population.ak) ## [1] 496388 As a general rule of thumb: if you did not assign, you did not save. We can also assign an ordered list of items to an object, using the concatenate function c(). As an example, exam.scores &lt;- c(58, 62, 43, 50) assigns a list with three numbers to the object exam.scores. Let’s say you wanted to scale these exam scores to be out of a total of 100. You could now write exam.scores &lt;- exam.scores/100 exam.scores ## [1] 0.58 0.62 0.43 0.50 It is incredibly easy to overwrite objects, which can be both useful and frustrating. Let’s say you made a mistake in the above code snippet and instead wrote exam.scores &lt;- exam.scores[100] As you can verify for yourself, the content of your object exam.scores now looks very different, and you can’t retrieve the prior information from the object. You can also combine objects into new objects. This is useful if you want to append one list to another, for example. Let’s say you have two more exam scores. You can then piece them together like this: exam.scores &lt;- c(58, 62, 43, 50) exam.scores.add &lt;- c(59, 72) exam.scores &lt;- c(exam.scores, exam.scores.add) exam.scores ## [1] 58 62 43 50 59 72 In case you have not noticed this already: You just input data into R! The objects above contain values, which are information, which are data. Whether that information is useful is perhaps a different question altogether. For now, the important point is that we now have data in R that we can use. Of course, inputting data like this is neither practical nor robust (it’s very easy to make typos). Just imagine you’d have to input data for infant mortality rates for 168 countries, over a 30 year period, using this approach. In the future, we will therefore learn how to load, or import, data sets into R. 2.5 The global environment and how to clean it Named objects are stored in the “global environment”, which means that they can be accessed at any time by any function you might run. The commands ls() and rm() are used to show or remove variables from the global environment respectively. a &lt;- 1 # Make an object &#39;a&#39; b &lt;- 2 # Make an object &#39;b&#39; ls() # Let&#39;s print out all the objects in our global environment. ## [1] &quot;a&quot; &quot;ability&quot; &quot;Abram&quot; ## [4] &quot;actual_2016&quot; &quot;adults&quot; &quot;age&quot; ## [7] &quot;age_table&quot; &quot;all_highlight_pairs&quot; &quot;alpha&quot; ## [10] &quot;anes&quot; &quot;anes_data&quot; &quot;anes2020&quot; ## [13] &quot;anova_result&quot; &quot;approval_model&quot; &quot;available_pairs&quot; ## [16] &quot;b&quot; &quot;back_to_value&quot; &quot;biden_clean&quot; ## [19] &quot;both_mean&quot; &quot;cces&quot; &quot;cces_with_dummies&quot; ## [22] &quot;ci_comparison&quot; &quot;ci_data&quot; &quot;civic_turnout&quot; ## [25] &quot;coef_clustered&quot; &quot;coef_original&quot; &quot;coef_robust&quot; ## [28] &quot;coef_robust_only&quot; &quot;collect1&quot; &quot;collect2&quot; ## [31] &quot;collect3&quot; &quot;comparison&quot; &quot;conf_levels&quot; ## [34] &quot;congress_age&quot; &quot;control_scores&quot; &quot;correlation_builtin&quot; ## [37] &quot;correlation_manual&quot; &quot;countries&quot; &quot;current_year&quot; ## [40] &quot;cutoff&quot; &quot;data&quot; &quot;democracy_scores&quot; ## [43] &quot;demonstrate_clt&quot; &quot;df&quot; &quot;df_comparison&quot; ## [46] &quot;dice_probs&quot; &quot;diff_in_means&quot; &quot;diff_means&quot; ## [49] &quot;errors&quot; &quot;est1_sample_mean&quot; &quot;est2_sample_median&quot; ## [52] &quot;est3_first_obs&quot; &quot;est4_maximum&quot; &quot;est5_partial_mean&quot; ## [55] &quot;estimated_ate&quot; &quot;estimated_effect&quot; &quot;even_data&quot; ## [58] &quot;exam_scores&quot; &quot;exam.scores&quot; &quot;exam.scores.add&quot; ## [61] &quot;f_stat&quot; &quot;family_income&quot; &quot;fitted_vals&quot; ## [64] &quot;genderCorona&quot; &quot;gss_clean&quot; &quot;height&quot; ## [67] &quot;highlight_pairs&quot; &quot;i&quot; &quot;income_by_education&quot; ## [70] &quot;iqr_biden&quot; &quot;largest_errors&quot; &quot;lower_5&quot; ## [73] &quot;lower_90&quot; &quot;lower_95&quot; &quot;lower_99&quot; ## [76] &quot;lower_bound&quot; &quot;lower_bound_large&quot; &quot;lower_bound_small&quot; ## [79] &quot;margin&quot; &quot;margin_of_error&quot; &quot;margin_of_error_large&quot; ## [82] &quot;margin_of_error_small&quot; &quot;matrix1&quot; &quot;matrix2&quot; ## [85] &quot;matrix3&quot; &quot;mean_control&quot; &quot;mean_rdi4&quot; ## [88] &quot;mean_treated&quot; &quot;mean_treatment&quot; &quot;mean_vote&quot; ## [91] &quot;means&quot; &quot;means_by_category&quot; &quot;model&quot; ## [94] &quot;model_age&quot; &quot;model_age_party&quot; &quot;model_binary&quot; ## [97] &quot;model_categorical&quot; &quot;model_categorical_v2&quot; &quot;model_full&quot; ## [100] &quot;model_good&quot; &quot;model_interaction&quot; &quot;model_interaction_v2&quot; ## [103] &quot;model_multiple&quot; &quot;model_none&quot; &quot;model_party&quot; ## [106] &quot;model_restricted&quot; &quot;model_simple&quot; &quot;model_twoway&quot; ## [109] &quot;model_unrestricted&quot; &quot;model_year&quot; &quot;model_year_fe&quot; ## [112] &quot;model1&quot; &quot;model2&quot; &quot;model3&quot; ## [115] &quot;model4&quot; &quot;motivation&quot; &quot;MovieData&quot; ## [118] &quot;mtcars&quot; &quot;mu&quot; &quot;mu_0&quot; ## [121] &quot;n&quot; &quot;n_10pct&quot; &quot;n_control&quot; ## [124] &quot;n_people&quot; &quot;n_respondents&quot; &quot;n_samples&quot; ## [127] &quot;n_simulations&quot; &quot;n_small&quot; &quot;n_treatment&quot; ## [130] &quot;n_years&quot; &quot;neighbors_turnout&quot; &quot;neither_mean&quot; ## [133] &quot;normal_data&quot; &quot;normal_df&quot; &quot;numbers&quot; ## [136] &quot;observed&quot; &quot;odd_data&quot; &quot;one_mean&quot; ## [139] &quot;orig&quot; &quot;original_value&quot; &quot;p&quot; ## [142] &quot;p_value&quot; &quot;p_value_normal&quot; &quot;p_value_t&quot; ## [145] &quot;p1&quot; &quot;p2&quot; &quot;p3&quot; ## [148] &quot;party_data&quot; &quot;party_table&quot; &quot;percentile_75&quot; ## [151] &quot;plane&quot; &quot;plot_data&quot; &quot;pop_bimodal&quot; ## [154] &quot;pop_skewed&quot; &quot;pop_uniform&quot; &quot;population&quot; ## [157] &quot;pred_2016_multiple&quot; &quot;pred_2016_simple&quot; &quot;pred_data&quot; ## [160] &quot;prob&quot; &quot;prob_below_40&quot; &quot;prob_below_50&quot; ## [163] &quot;prob_interval&quot; &quot;prob_right&quot; &quot;r_squared_manual&quot; ## [166] &quot;r2_model1&quot; &quot;r2_model2&quot; &quot;r2_model3&quot; ## [169] &quot;random_pairs&quot; &quot;random_scores&quot; &quot;range_biden&quot; ## [172] &quot;real_scores&quot; &quot;residual_table&quot; &quot;result&quot; ## [175] &quot;result_two&quot; &quot;results&quot; &quot;results_first&quot; ## [178] &quot;results_max&quot; &quot;results_mean&quot; &quot;results_median&quot; ## [181] &quot;roommate&quot; &quot;row_indices&quot; &quot;s&quot; ## [184] &quot;s_small&quot; &quot;s3d&quot; &quot;salaries1&quot; ## [187] &quot;salaries2&quot; &quot;sample_congress&quot; &quot;sample_data&quot; ## [190] &quot;sample_df&quot; &quot;sample_i&quot; &quot;sample_lower&quot; ## [193] &quot;sample_margin&quot; &quot;sample_mean&quot; &quot;sample_n&quot; ## [196] &quot;sample_pairs&quot; &quot;sample_sd&quot; &quot;sample_se&quot; ## [199] &quot;sample_size&quot; &quot;sample_t_critical&quot; &quot;sample_upper&quot; ## [202] &quot;scenarios&quot; &quot;school_spend&quot; &quot;school_spend_with_na&quot; ## [205] &quot;score_table&quot; &quot;sd_biden&quot; &quot;sd_control&quot; ## [208] &quot;sd_rdi4&quot; &quot;sd_treatment&quot; &quot;sd_vote&quot; ## [211] &quot;se&quot; &quot;se_comparison&quot; &quot;se_estimated&quot; ## [214] &quot;se_known&quot; &quot;se_small&quot; &quot;shade_above&quot; ## [217] &quot;shade_below&quot; &quot;shade_df&quot; &quot;sigma&quot; ## [220] &quot;social&quot; &quot;special_points&quot; &quot;SSE&quot; ## [223] &quot;students&quot; &quot;students_sample&quot; &quot;summary_full&quot; ## [226] &quot;t_critical&quot; &quot;t_data_14&quot; &quot;t_data_29&quot; ## [229] &quot;t_data_5&quot; &quot;t_stat&quot; &quot;t_test_result&quot; ## [232] &quot;test_scores&quot; &quot;threshold&quot; &quot;top_10_cutoff&quot; ## [235] &quot;total.votes.ak&quot; &quot;treated_scores&quot; &quot;treatment&quot; ## [238] &quot;true_ate&quot; &quot;true_mean&quot; &quot;true_sd&quot; ## [241] &quot;TSS&quot; &quot;turnout.ak&quot; &quot;typical_donors&quot; ## [244] &quot;upper_90&quot; &quot;upper_95&quot; &quot;upper_99&quot; ## [247] &quot;upper_bound&quot; &quot;upper_bound_large&quot; &quot;upper_bound_small&quot; ## [250] &quot;vanilla_lovers&quot; &quot;var_biden&quot; &quot;votes&quot; ## [253] &quot;voting.age.population.ak&quot; &quot;width_90&quot; &quot;width_95&quot; ## [256] &quot;width_99&quot; &quot;with_wealthy&quot; &quot;wto&quot; ## [259] &quot;wto_complete&quot; &quot;wto_highlight&quot; &quot;wto_sample&quot; ## [262] &quot;x&quot; &quot;x_bar&quot; &quot;x_bar_small&quot; ## [265] &quot;x_bias&quot; &quot;x_dep&quot; &quot;x_good&quot; ## [268] &quot;x_het&quot; &quot;x_nonlin&quot; &quot;x_novar&quot; ## [271] &quot;x_vals&quot; &quot;xx&quot; &quot;y&quot; ## [274] &quot;y_bias&quot; &quot;y_dep&quot; &quot;y_good&quot; ## [277] &quot;y_het&quot; &quot;y_mean&quot; &quot;y_nonlin&quot; ## [280] &quot;y_novar&quot; &quot;y_observed&quot; &quot;y_unbias&quot; ## [283] &quot;y_vals&quot; &quot;y0&quot; &quot;y1&quot; ## [286] &quot;z_90&quot; &quot;z_95&quot; &quot;z_99&quot; ## [289] &quot;z_critical&quot; &quot;z_stat&quot; &quot;zip_character&quot; ## [292] &quot;zip_numeric&quot; Sometimes you might want to remove a specific object. For this you can use the rm function. rm(a) # this removes a from the global environment rm(B) # ha - R is case sensitive, and &quot;b&quot; is different from &quot;B&quot; ## Warning in rm(B): object &#39;B&#39; not found ls() # check that a is gone, b is still there ## [1] &quot;ability&quot; &quot;Abram&quot; &quot;actual_2016&quot; ## [4] &quot;adults&quot; &quot;age&quot; &quot;age_table&quot; ## [7] &quot;all_highlight_pairs&quot; &quot;alpha&quot; &quot;anes&quot; ## [10] &quot;anes_data&quot; &quot;anes2020&quot; &quot;anova_result&quot; ## [13] &quot;approval_model&quot; &quot;available_pairs&quot; &quot;b&quot; ## [16] &quot;back_to_value&quot; &quot;biden_clean&quot; &quot;both_mean&quot; ## [19] &quot;cces&quot; &quot;cces_with_dummies&quot; &quot;ci_comparison&quot; ## [22] &quot;ci_data&quot; &quot;civic_turnout&quot; &quot;coef_clustered&quot; ## [25] &quot;coef_original&quot; &quot;coef_robust&quot; &quot;coef_robust_only&quot; ## [28] &quot;collect1&quot; &quot;collect2&quot; &quot;collect3&quot; ## [31] &quot;comparison&quot; &quot;conf_levels&quot; &quot;congress_age&quot; ## [34] &quot;control_scores&quot; &quot;correlation_builtin&quot; &quot;correlation_manual&quot; ## [37] &quot;countries&quot; &quot;current_year&quot; &quot;cutoff&quot; ## [40] &quot;data&quot; &quot;democracy_scores&quot; &quot;demonstrate_clt&quot; ## [43] &quot;df&quot; &quot;df_comparison&quot; &quot;dice_probs&quot; ## [46] &quot;diff_in_means&quot; &quot;diff_means&quot; &quot;errors&quot; ## [49] &quot;est1_sample_mean&quot; &quot;est2_sample_median&quot; &quot;est3_first_obs&quot; ## [52] &quot;est4_maximum&quot; &quot;est5_partial_mean&quot; &quot;estimated_ate&quot; ## [55] &quot;estimated_effect&quot; &quot;even_data&quot; &quot;exam_scores&quot; ## [58] &quot;exam.scores&quot; &quot;exam.scores.add&quot; &quot;f_stat&quot; ## [61] &quot;family_income&quot; &quot;fitted_vals&quot; &quot;genderCorona&quot; ## [64] &quot;gss_clean&quot; &quot;height&quot; &quot;highlight_pairs&quot; ## [67] &quot;i&quot; &quot;income_by_education&quot; &quot;iqr_biden&quot; ## [70] &quot;largest_errors&quot; &quot;lower_5&quot; &quot;lower_90&quot; ## [73] &quot;lower_95&quot; &quot;lower_99&quot; &quot;lower_bound&quot; ## [76] &quot;lower_bound_large&quot; &quot;lower_bound_small&quot; &quot;margin&quot; ## [79] &quot;margin_of_error&quot; &quot;margin_of_error_large&quot; &quot;margin_of_error_small&quot; ## [82] &quot;matrix1&quot; &quot;matrix2&quot; &quot;matrix3&quot; ## [85] &quot;mean_control&quot; &quot;mean_rdi4&quot; &quot;mean_treated&quot; ## [88] &quot;mean_treatment&quot; &quot;mean_vote&quot; &quot;means&quot; ## [91] &quot;means_by_category&quot; &quot;model&quot; &quot;model_age&quot; ## [94] &quot;model_age_party&quot; &quot;model_binary&quot; &quot;model_categorical&quot; ## [97] &quot;model_categorical_v2&quot; &quot;model_full&quot; &quot;model_good&quot; ## [100] &quot;model_interaction&quot; &quot;model_interaction_v2&quot; &quot;model_multiple&quot; ## [103] &quot;model_none&quot; &quot;model_party&quot; &quot;model_restricted&quot; ## [106] &quot;model_simple&quot; &quot;model_twoway&quot; &quot;model_unrestricted&quot; ## [109] &quot;model_year&quot; &quot;model_year_fe&quot; &quot;model1&quot; ## [112] &quot;model2&quot; &quot;model3&quot; &quot;model4&quot; ## [115] &quot;motivation&quot; &quot;MovieData&quot; &quot;mtcars&quot; ## [118] &quot;mu&quot; &quot;mu_0&quot; &quot;n&quot; ## [121] &quot;n_10pct&quot; &quot;n_control&quot; &quot;n_people&quot; ## [124] &quot;n_respondents&quot; &quot;n_samples&quot; &quot;n_simulations&quot; ## [127] &quot;n_small&quot; &quot;n_treatment&quot; &quot;n_years&quot; ## [130] &quot;neighbors_turnout&quot; &quot;neither_mean&quot; &quot;normal_data&quot; ## [133] &quot;normal_df&quot; &quot;numbers&quot; &quot;observed&quot; ## [136] &quot;odd_data&quot; &quot;one_mean&quot; &quot;orig&quot; ## [139] &quot;original_value&quot; &quot;p&quot; &quot;p_value&quot; ## [142] &quot;p_value_normal&quot; &quot;p_value_t&quot; &quot;p1&quot; ## [145] &quot;p2&quot; &quot;p3&quot; &quot;party_data&quot; ## [148] &quot;party_table&quot; &quot;percentile_75&quot; &quot;plane&quot; ## [151] &quot;plot_data&quot; &quot;pop_bimodal&quot; &quot;pop_skewed&quot; ## [154] &quot;pop_uniform&quot; &quot;population&quot; &quot;pred_2016_multiple&quot; ## [157] &quot;pred_2016_simple&quot; &quot;pred_data&quot; &quot;prob&quot; ## [160] &quot;prob_below_40&quot; &quot;prob_below_50&quot; &quot;prob_interval&quot; ## [163] &quot;prob_right&quot; &quot;r_squared_manual&quot; &quot;r2_model1&quot; ## [166] &quot;r2_model2&quot; &quot;r2_model3&quot; &quot;random_pairs&quot; ## [169] &quot;random_scores&quot; &quot;range_biden&quot; &quot;real_scores&quot; ## [172] &quot;residual_table&quot; &quot;result&quot; &quot;result_two&quot; ## [175] &quot;results&quot; &quot;results_first&quot; &quot;results_max&quot; ## [178] &quot;results_mean&quot; &quot;results_median&quot; &quot;roommate&quot; ## [181] &quot;row_indices&quot; &quot;s&quot; &quot;s_small&quot; ## [184] &quot;s3d&quot; &quot;salaries1&quot; &quot;salaries2&quot; ## [187] &quot;sample_congress&quot; &quot;sample_data&quot; &quot;sample_df&quot; ## [190] &quot;sample_i&quot; &quot;sample_lower&quot; &quot;sample_margin&quot; ## [193] &quot;sample_mean&quot; &quot;sample_n&quot; &quot;sample_pairs&quot; ## [196] &quot;sample_sd&quot; &quot;sample_se&quot; &quot;sample_size&quot; ## [199] &quot;sample_t_critical&quot; &quot;sample_upper&quot; &quot;scenarios&quot; ## [202] &quot;school_spend&quot; &quot;school_spend_with_na&quot; &quot;score_table&quot; ## [205] &quot;sd_biden&quot; &quot;sd_control&quot; &quot;sd_rdi4&quot; ## [208] &quot;sd_treatment&quot; &quot;sd_vote&quot; &quot;se&quot; ## [211] &quot;se_comparison&quot; &quot;se_estimated&quot; &quot;se_known&quot; ## [214] &quot;se_small&quot; &quot;shade_above&quot; &quot;shade_below&quot; ## [217] &quot;shade_df&quot; &quot;sigma&quot; &quot;social&quot; ## [220] &quot;special_points&quot; &quot;SSE&quot; &quot;students&quot; ## [223] &quot;students_sample&quot; &quot;summary_full&quot; &quot;t_critical&quot; ## [226] &quot;t_data_14&quot; &quot;t_data_29&quot; &quot;t_data_5&quot; ## [229] &quot;t_stat&quot; &quot;t_test_result&quot; &quot;test_scores&quot; ## [232] &quot;threshold&quot; &quot;top_10_cutoff&quot; &quot;total.votes.ak&quot; ## [235] &quot;treated_scores&quot; &quot;treatment&quot; &quot;true_ate&quot; ## [238] &quot;true_mean&quot; &quot;true_sd&quot; &quot;TSS&quot; ## [241] &quot;turnout.ak&quot; &quot;typical_donors&quot; &quot;upper_90&quot; ## [244] &quot;upper_95&quot; &quot;upper_99&quot; &quot;upper_bound&quot; ## [247] &quot;upper_bound_large&quot; &quot;upper_bound_small&quot; &quot;vanilla_lovers&quot; ## [250] &quot;var_biden&quot; &quot;votes&quot; &quot;voting.age.population.ak&quot; ## [253] &quot;width_90&quot; &quot;width_95&quot; &quot;width_99&quot; ## [256] &quot;with_wealthy&quot; &quot;wto&quot; &quot;wto_complete&quot; ## [259] &quot;wto_highlight&quot; &quot;wto_sample&quot; &quot;x&quot; ## [262] &quot;x_bar&quot; &quot;x_bar_small&quot; &quot;x_bias&quot; ## [265] &quot;x_dep&quot; &quot;x_good&quot; &quot;x_het&quot; ## [268] &quot;x_nonlin&quot; &quot;x_novar&quot; &quot;x_vals&quot; ## [271] &quot;xx&quot; &quot;y&quot; &quot;y_bias&quot; ## [274] &quot;y_dep&quot; &quot;y_good&quot; &quot;y_het&quot; ## [277] &quot;y_mean&quot; &quot;y_nonlin&quot; &quot;y_novar&quot; ## [280] &quot;y_observed&quot; &quot;y_unbias&quot; &quot;y_vals&quot; ## [283] &quot;y0&quot; &quot;y1&quot; &quot;z_90&quot; ## [286] &quot;z_95&quot; &quot;z_99&quot; &quot;z_critical&quot; ## [289] &quot;z_stat&quot; &quot;zip_character&quot; &quot;zip_numeric&quot; You can combine these two to clean out the global environment for a fresh start. rm(list = ls()) #this removes all global variables 2.6 Getting help Learning about functions and how to specify them correctly is half the battle. If you know the name of the function you want, you can access the help files in two ways (try running these yourself) help(sqrt) # help w/ functions ?sqrt # same thing If you can’t quite remember the name of the function, you can try searching for it. help.search(&quot;sqrt&quot;) # what am I looking for? Fuzzy matching Many, but not all, functions also have examples showing you how to use them correctly. example(sqrt) ## ## sqrt&gt; require(stats) # for spline ## ## sqrt&gt; require(graphics) ## ## sqrt&gt; xx &lt;- -9:9 ## ## sqrt&gt; plot(xx, sqrt(abs(xx)), col = &quot;red&quot;) ## ## sqrt&gt; lines(spline(xx, sqrt(abs(xx)), n=101), col = &quot;pink&quot;) Remember that these help menus are usually written by the same people who wrote the functions you are using. It turns out that being good at making software does not mean you are good at documenting software. Help files are almost uniformly not helpful unless you already know a good bit about computer programming and (in some cases) a lot about the function itself. The same thing goes for examples. The code above for the sqrt function probably means nothing to you. So, first, if the help files confuse you that’s OK. They confuse most of us. Later in the class we will try to explain how they are structured, but for the most part you can follow (and modify) the examples we give you. Second, don’t rely on the help files alone. There are TONS of resources online helping students understand the basic functions we use in the course that are much more helpful than standard ‘help’ files. 2.7 Installing packages The beauty of R is that there are packages. Packages, in turn, contain functions, and functions take arguments. The first time you use a package you are going to have to install it from the comprehensive R archiving network (CRAN). install.packages(&quot;dplyr&quot;) # This will prompt a user interface to choose the &quot;mirror&quot; or repository For the most part, you only need to run this command once. Once you have a package installed, it is on your computer, similar to any other piece of software (like R Studio). But when you want to use a function from the package you are going to have to do one of two things. First you can use the library function at the top of your script to load the package. This, again, is similar to any other piece of software, in that you are effectively starting the software and making it available. library(&quot;dplyr&quot;) # This will prompt a user interface to choose the repository The first time you install a package, R will ask you to specify from where it should download the data. Simply choose one of the options, such as the server located in St. Louis, the content is the same for all of them. Your second option is to specify the package for a function using the :: operator. For instance, I could use the mutate function from dplyr like this. dplyr::mutate() # Don&#39;t run this line, it won&#39;t work because we have no data yet. Lastly, packages come with example data sets built in that you can access using the data command. data(rock) # load the dataset on rocks from the MASS package ?rock # there are help files for these ls() # there&#39;s rock -- in the global environment data(road, package=&quot;MASS&quot;) # you can load these data sets without loading the package A useful package for learning R is swirl. It contains short exercises for learning R. To get started, type: install.packages(swirl) library(&quot;swirl&quot;) swirl() Again, the first line, to install the package, only needs to be run once. The second line, loading the package, needs to be done every time you restart R. Note that some packages will require you to install additional packages; in that case, R will prompt you and ask for your permission. Whenever you have a few minutes, you can pick up some R skills with swirl. "],["r-markdown-basics.html", "3 R Markdown Basics 3.1 What is R Markdown? Why R Markdown? 3.2 Prerequisites 3.3 Create an R Markdown document in RStudio 3.4 R Markdown Basics 3.5 YAML Header 3.6 Code Chunks 3.7 Text formatting 3.8 Include a plot generated from R code 3.9 Get the Output 3.10 Advanced Topic: Include a Local Image 3.11 References", " 3 R Markdown Basics 3.1 What is R Markdown? Why R Markdown? When you create a R Markdown document in RStudio, the template tells you the basics: “Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents… When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.” More specifically, R Markdown can do the following: Synthesize multiple components, including text, tables, figures, code chunks, and code outputs into a single document (just like how this document is produced!); Create a fully reproducible document and support multiple output formats (HTML, PDF, and MS Word); “As an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking.” (R for Data Science) We will focus on outputs as HTML documents in this page. 3.2 Prerequisites You need the rmarkdown package, but you don’t need to explicitly install it or load it as RStudio automatically does both when needed. 3.3 Create an R Markdown document in RStudio Open RStudio; Click “+” (under File) to create a document; Click “R Markdown” Enter the title and the author of the document, leave the default output as html, and click ”OK”. 3.4 R Markdown Basics Here is a typical R Markdown document: It contains three important elements: An (optional) YAML header surrounded by ---. If there is one, it is always at the beginning of the document. Chunks of R code surrounded by ```{r} on the first line and ``` on the last line. Text mixed with simple text formatting like **# heading** and _italics_. 3.5 YAML Header You can control many other “whole document” settings by tweaking the parameters of the YAML header. You might wonder what YAML stands for: it’s “yet another markup language”, which is designed for representing hierarchical data in a way that’s easy for humans to read and write. R Markdown uses it to control many details of the output, such as document parameters and bibliography. At this time, you only need to know that you can change the information of title, author and date in the YAML Header. 3.6 Code Chunks 3.6.1 Create a code chunk To embed/run code inside an R Markdown document, you need to insert a chunk. There are three ways to do so: The keyboard shortcut Cmd/Ctrl + Alt + I The “Insert” button icon in the editor toolbar. By manually typing the chunk delimiters ```{r} in the beginning and ``` in the end (another line). You run a code chunk with a shortcut cmd/ctrl + shift + enter. 3.6.2 Chunk Options Chunk output can be customized with options, arguments supplied to chunk header. Knitr provides almost 60 options that you can use to customize your code chunks. Here we’ll cover the most important chunk options that you’ll use frequently. See the full list at http://yihui.name/knitr/options/. Here are some of the most important chunk arguments: eval = FALSE prevents code from being evaluated (and obviously if the code is not run, no results will be generated). This is useful for displaying example code, or for disabling a large block of code without commenting each line. include = FALSE runs the code, but doesn’t show the code or results in the final document. Use this for setup code that you don’t want cluttering your report. echo = FALSE prevents code, but not the results, from appearing in the finished file. Use this when writing reports aimed at people who don’t want to see the underlying R code. message = FALSE or warning = FALSE prevents messages or warnings from appearing in the finished file. results = 'hide' hides printed output; fig.show = 'hide' hides plots. It is easy to specify these arguments, simply delimit the arguments with a comma. For example, if you would like to specify warning = FALSE and echo = TRUE, you can specify: ```{r, warning = FALSE, echo = TRUE}. Remember to end the chunk in a new line with ```. 3.7 Text formatting Another essential component of a R Markdown document is plain text (like the text in this sentence!). Text formatting in R Markdown is not as straightforward as MS Word or Google Docs, but it is not difficult. Here are some important text formatting: Bold text: Try **Bold** or __Bold__ Italic text: Try *italic* or _italic_ code: Try `code` to make the text look like R code like this. math symbols: Use $: Try $y=ax+b$ for inline math Try $$y=ax+b$$ for formula that take up a row (and centered) See this page for more math symbols (this will be useful!). superscript and subscript: Try x^2^ and x~2~ (2 will be superscripted/subscripted). Line break: Insert an empty line = line break; To add extra line breaks, add &lt;br&gt; in another new lines (note that it only works in html output). Headings: 1st level header # Heading 2nd level header ## Heading 3rd level header ### Heading Note: Don’t leave a space or indentation before all the #s! Bulleted lists: - item 1 - item 2 - item 2a (Use two tab indentations here) - item 2b (Use two tab indentations here) item 2b-1 (Use two tab indentations here) item 2b-2 (Use two tab indentations here) (Besides starting with -, * and + also works). Ordered lists: 1. item 1 2. item 2 a. item 2a (Use two tab indentations here) b. item 2b (Use two tab indentations here) 3.8 Include a plot generated from R code R has a number of built-in tools for basic visualizations such as histograms, scatterplots, bar charts, boxplots and much more. Rather than going through all the different types, we will focus on the basic scatterplot. We use the plot() command to generate a simple scatterplot as shown below: ## Create two vectors for plotting: age &lt;- c(10,12,14,17,19,21,25) height &lt;- c(150,160,165,175,177,178,178) ## A scatterplot with age on the x axis and height on the y axis; ## Specify different arguments to enrich the plot, such as `main=` to add a title, `xlab` to add a x-axis label, `ylab` to add a y-axis label plot(age, height, main=&quot;Scatterplot&quot;, xlab=&quot;Age&quot;, ylab=&quot;Height&quot;) Look at this page for more details of scatterplots! 3.9 Get the Output Once if you finish typesetting the document or you would like to preview your output document, click the Knit button on the editor toolbar. 3.10 Advanced Topic: Include a Local Image You can also include images (like jpegs or PNG files) in your document. But to do that, you need to be sure you can point R to the correct path. That is, you need to tell R where the image is stored on your computers. If you do not know how to find the path for a file, we’ll come back to that later. You can do that using the markdown language like this: ![Input a caption for the image](local path for the image){width=250px}. Note that you do not need to use any quotation marks here. You can also do this with R code using the knitr package. For example, we can use the following R code: knitr::include_graphics(path = &quot;images/SATS.png&quot;) Figure 3.1: Sentiment Analysis of Taylor Swift’s songs This assumes that the file SATS.png is stored in a subdirectory called images. Note that: The path= argument specifies a local path where the image is stored. You need to use quotation marks if you use R code to display local images. 3.11 References The website “R for Data Science”: https://r4ds.had.co.nz/r-markdown.html Plotting: https://hbctraining.github.io/Intro-to-R/lessons/basic_plots_in_r.html "],["data-basics.html", "4 Data Basics 4.1 Learning Objectives 4.2 What Is This For? 4.3 Importing Data 4.4 Looking at your data 4.5 Understand the basic data structure 4.6 Saving data 4.7 Vectors, Matrices, and Data Frames 4.8 Study Questions for Data Basics 1", " 4 Data Basics 4.1 Learning Objectives Learn how to load data into R, inspect data, and save data. Learn what observations and variables represent. Learn how to add a variable to a datast using R. Learn the basic structure of vectors, matrices, and data frames. 4.2 What Is This For? Many people think that data science is all about computer wizardry, artificial intelligence, and elaborate data architectures. Instead, on a day-to-day basis many of the most important data skills are much more mundane. We need to find the right data, get it onto our machines, look at it, understand it, and clean it. These data management steps all come before any data analysis, and they are essential to the success of any data analysis project. A common adage in the world of data analysis is, “garbage in, garbage out.” This means that no matter how fancy your machine learning, if you are analyzing bad data, your results will still not be worth very much. In this session, we will learn the basics of what data looks like in R. 4.3 Importing Data We will work primarily with .csv files in this course. In this session, we will work with an example data set, genderCorona.csv, which I have created for this class (it’s real data). You can download the data set from Canvas or here at Dropbox. We will cover two ways of getting these data into R, or importing them. The first is to download the data to your computer and then to load it into R. The second is to load the data directly into R using the Dropbox link above. 4.3.1 Download data and load into R First, download the file genderCorona.csv from Canvas or Dropbox and save it in the folder you created for this class. Now let’s make sure R knows to look for the file in your folder for this class. That is, let’s make sure that you set the working directory to the correct folder. We can use the getwd to find out R’s current working directory. getwd() Verify whether this is the correct folder for your setup. The output from this command always tells you what directory R is looking in for files. This means, that whenever you tell R to read files it will look for them here (unless you specifically give R a path). Likewise, when you save files this is where they will go. If your current working directory is not the folder where you save material for this class, you need to change the working directory. To change where R is looking, we will use the setwd command. For example, if you are on a Mac and you created a folder DS4P_2024 on your desktop, your working directory might look something like this: setwd(&quot;~/Desktop/DS4P_2024/&quot;) If you are on Windows, your folder location will look slightly differently. For example, if your windows login name is abcd and you saved the folder in your documents folder, you might type something like setwd(&quot;C:/users/abcd/Documents/DS4P_2024/&quot;) You will need to adjust your code accordingly if your folder is somewhere else or if you named your folder something else. Either way, by the end of this week, you must have such a folder on your computer and be able to direct R to this folder using the setwd command. If you still struggle with this step, revisit the material in the readings or on canvas. If none of that works for you, watch this short tutorial on how to set your working directory using RStudio. Once you have set your working directory and downloaded the data genderCorona.csv, getting it into R is relatively straightforward. We use the read.csv function. Remember that R is based on objects, and that you need to assign stuff to objects. That is also true for data. You can choose the object name yourself, subject to the constraints we encountered last week (you cannot start with a number, for one). In the following, I will simply create an object genderCorona, but you should feel free to change the name. genderCorona &lt;- read.csv(&quot;genderCorona.csv&quot;) ASIDE: If you stored your data in a subfolder within your working directory, for example a subfolder “data”, you would have to adjust the code accordingly: genderCorona &lt;- read.csv(&quot;data/genderCorona.csv&quot;) You now have your data loaded into R, as an object genderCorona. You can verify that this object is, indeed, a data frame! 4.3.2 Download data directly into R A second option to get data into R is to download it directly. This works if you have a link, such as the link to a file saved on Dropbox posted above. I will generally make links to Dropbox files available as part of this class. You can then download the file directly into R. For example, using the link I gave you, you can download the data and assign it to an object genderCorona with the following code: genderCorona &lt;- read.csv(url(&quot;https://www.dropbox.com/scl/fi/uowu2hj9nwmzu6i79cr1s/genderCorona.csv?rlkey=7bhhnt3vxsupfc91zu1n2rr4y&amp;dl=1&quot;)) Note the ending of the link, which is “dl=1”. This tells Dropbox to directly download the file. Sometimes, a shared link to a file will have a 0 in the end, which only directs your computer to look at it, not to download it. Replacing the 0 with a 1 will allow R to download it directly. 4.4 Looking at your data Once we have the data in R, we can look at it as well. head(genderCorona) ## Showing the first few rows ## country iso3n WomanLeader Death DeathCases DeathMillion CapacityHigh Trust TrustHigh ## 1 Australia 36 0 907 3.3 0.363 1 45.7 1 ## 2 Austria 40 0 1377 0.9 1.556 0 45.3 1 ## 3 Belgium 56 1 12907 2.6 11.300 1 43.7 1 ## 4 Brazil 76 0 162269 2.9 7.747 0 20.0 0 ## 5 Canada 124 0 10547 4.0 2.846 0 62.7 1 ## 6 Chile 152 0 14499 2.8 7.741 0 26.7 0 tail(genderCorona) ## Showing the last few rows ## country iso3n WomanLeader Death DeathCases ## 34 Sweden 752 0 6022 4.1 ## 35 Switzerland 756 0 2749 1.3 ## 36 Turkey 792 0 10803 2.8 ## 37 Russia 810 0 30010 1.7 ## 38 United Kingdom of Great Britain and Northern Ireland (the) 826 0 48978 4.2 ## 39 United States of America (the) 840 0 237113 2.4 ## DeathMillion CapacityHigh Trust TrustHigh ## 34 5.914 1 51.3 1 ## 35 3.228 1 82.3 1 ## 36 1.312 0 56.0 1 ## 37 2.077 0 53.3 1 ## 38 7.366 0 42.3 0 ## 39 7.247 0 33.3 0 You can also use the View and fix commands to either see the entire dataset (like a spreadsheet) or even manually edit values. Try these on your own. View(genderCorona) ## Opening the data like a spreadsheet fix(genderCorona) ## You can edit data directly this way 4.5 Understand the basic data structure Let’s pause for a moment to understand what you are looking at. First we want to know what each row in the data represents. This is what we term the “unit of analysis.” We refer to each row (or unit) as an observaction. Here, we have a data set of 39 countries. Each country represents one observation. Each country is identified with a string variable country and a numeric code iso3n. The latter are standardized codes for all countries in the world, similar to two-letter acronyms for U.S. states. Next, we have a set of variables: WomanLeader tells us whether a country’s government is run by a woman; the variables Death, DeathCases, and DeathMillion capture deaths from Covid19, deaths from Covid19 relative to cases of Covid19, and deaths from Covid19 relative to 1 million citizens; CapacityHigh captures whether a country performs above-average on a measure of state capacity; Trust captures the percent of people who reported in a standardized survey to have high trust in their government; and TrustHigh is coded 1 for countries that perform above-average on the Trust variable. The function summary creates a summary table for all of the variables. summary(genderCorona) ## country iso3n WomanLeader Death DeathCases ## Length:39 Min. : 36.0 Min. :0.0000 Min. : 18.0 Min. :0.400 ## Class :character 1st Qu.:239.5 1st Qu.:0.0000 1st Qu.: 504.5 1st Qu.:1.200 ## Mode :character Median :392.0 Median :0.0000 Median : 2749.0 Median :1.700 ## Mean :425.6 Mean :0.1795 Mean : 21360.8 Mean :2.218 ## 3rd Qu.:618.0 3rd Qu.:0.0000 3rd Qu.: 13703.0 3rd Qu.:2.850 ## Max. :840.0 Max. :1.0000 Max. :237113.0 Max. :9.900 ## DeathMillion CapacityHigh Trust TrustHigh ## Min. : 0.051 Min. :0.0000 Min. :14.30 Min. :0.0000 ## 1st Qu.: 0.677 1st Qu.:0.0000 1st Qu.:31.85 1st Qu.:0.0000 ## Median : 2.568 Median :0.0000 Median :41.70 Median :0.0000 ## Mean : 3.344 Mean :0.3077 Mean :43.04 Mean :0.4615 ## 3rd Qu.: 5.959 3rd Qu.:1.0000 3rd Qu.:55.35 3rd Qu.:1.0000 ## Max. :11.300 Max. :1.0000 Max. :82.30 Max. :1.0000 You can see that we obtain for each variable a few key pieces of information. We will talk more about the items listed here in the next couple of weeks. We can also look at just one variable by specifying the name of a variable using the dollar sign, like this: summary(genderCorona$WomanLeader) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.1795 0.0000 1.0000 RStudio also makes this easy. In your ‘Global Environment’ window you will see an object called genderCorona, or whichever name you assigned to this object. If you click on the triangle next to the name it will show you basic information about the data. If you click on the symbol it will open up the entire spreadsheet for you to look at. 4.5.1 Variable names Sometimes all you want to know are the names of the variables in your datset without any information on the data. To do this you can use the names function. names(genderCorona) ## [1] &quot;country&quot; &quot;iso3n&quot; &quot;WomanLeader&quot; &quot;Death&quot; &quot;DeathCases&quot; &quot;DeathMillion&quot; ## [7] &quot;CapacityHigh&quot; &quot;Trust&quot; &quot;TrustHigh&quot; 4.6 Saving data After we change the data, we may want to save it for later use. Here I am assuming we want to save a new file with a new file name. You might also want to overwrite the original file, but that makes it very hard to go back if you find a mistake in your code. In general, it is good practice to keep your input files separate from your output files. That way, you can always re-create what you have done or change any intermediate steps. We can actually save the file in lots of different formats. To save this as a CSV file we can use the function write.csv. In the following, we will change the file name to genderCorona_output.csv. write.csv(genderCorona, file=&quot;genderCorona_output.csv&quot;) Note that unless you specify otherwise it will be saved into your working directory. If you want it in a subfolder Data inside your working directory, you need to specify this: write.csv(genderCorona, file=&quot;Data/genderCorona_output.csv&quot;) The write.csv function has other options controlling how the data is saved. Here is a video introducing some of these options. 4.6.1 A note on Windows machines and paths If you use a computer with a Windows operating system you may sometimes have trouble with setting your paths. R wants you to use the “/” in path names but windows will give you path names with the “\\”. You are going to need to replace any “\\” in a path name you get with the “/”. (The reason is that R is built for Unix machines and Windows runs on DOS. Macs are also based in Unix, which is one of the few valid reasons why data science types sometimes prefer Macs.) 4.6.2 Adding a variable to your data The variable WomanLeader captures whether a woman is governing a country: it is 1 when a woman is governing the country, and 0 when a man is governing the country. Let’s create a new variable, which is 0 when a woman is governing the country, and 1 when a man is governing the country. While we are at it, we will rescale the variable Trust, such that it runs from 0 to 1 instead of 0 to 100. genderCorona$ManLeader &lt;- 1 - genderCorona$WomanLeader genderCorona$Trust &lt;- genderCorona$Trust/100 We can see the results if we again look at the summary of the data or at the spreadsheet using the View function. We will return to this next week, but you might already recognize one important point: We have not actually changed the information in our data set, in that these two new variables tell us nothing about the world that we did not know already from the existing variables. We just rescaled the existing information. It’s a bit like measuring temperature in Fahrenheit or in Celsius. Converting the measure from one to the other doesn’t actually change the information we have, just how we express it. 4.7 Vectors, Matrices, and Data Frames So far, we’ve been working with data that’s already organized in a nice, tidy format - the genderCorona data frame. But to truly understand how R handles data, we need to understand the building blocks that make up these data structures. Think of it this way: if data frames are like completed buildings, then vectors and matrices are like the bricks and beams that hold everything together. Understanding these fundamental structures will help you: Troubleshoot problems - When R gives you an error message about “incompatible types” or “incorrect dimensions,” you’ll know what’s going wrong Create your own data - You won’t always import pre-made datasets; sometimes you’ll need to build data structures from scratch Manipulate data more effectively - Many R operations work on vectors, and understanding this will help you transform variables and create new ones Read and understand R documentation - Most R functions are described in terms of what types of inputs they accept (vectors, matrices, or data frames) 4.7.1 Vectors Vectors are the simplest type of data structure in R. You can think of a vector as a one-dimensional data structure that contains a collection of things, like a sequence of numbers (1,3,2,54,100) or of string characters (“Tesla”, “Honda”, “Hyundai”). A vector collects a set of attributes, in a specific order. When creating a vector, you always type c() and include the items inside the parentheses, separated by a comma. collect1 &lt;- c(1,3,2,54,100) # sequence of numbers class(collect1) ## [1] &quot;numeric&quot; collect2 &lt;- c(&quot;Tesla&quot;, &quot;Honda&quot;, &quot;Hyundai&quot;, &quot;Toyota&quot;, &quot;Chrysler&quot;) # collection of strings class(collect2) ## [1] &quot;character&quot; collect3 &lt;- c(NA, &quot;Cars&quot;, 2, 5) # mixture of different object types class(collect3) # but they are all converted into character ## [1] &quot;character&quot; 4.7.2 Matrix Matrices are two-dimensional data structures that contain the same type of data (e.g., all numeric, character, or logical). Matrices are truly amazing things, and you can consider them from many different perspectives. One way to think of a matrix is as a collection of vectors, each vector containing information on a set of attributes. To create a new matrix, use the function matrix() as shown below. Because all elements inside the matrix must be of the same data type, all elements will be converted to character when you try to combine different object types inside a matrix. You can also use the dim() function to examine the dimensions of the matrix, which are the number of rows and columns in the matrix. matrix1 &lt;- matrix(1:12, nrow = 4, ncol = 3) # matrix of numbers matrix1 ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 matrix2 &lt;- matrix(c(&quot;Apple&quot;, &quot;Dell&quot;, &quot;HP&quot;, &quot;Samsung&quot;), nrow = 2, ncol = 2) # matrix of characters matrix2 ## [,1] [,2] ## [1,] &quot;Apple&quot; &quot;HP&quot; ## [2,] &quot;Dell&quot; &quot;Samsung&quot; matrix3 &lt;- matrix(c(NA, 3, 4, &quot;Nice&quot;), nrow = 2, ncol = 2) # mixture of different object types matrix3 # but they are all converted into character (with parentheses) ## [,1] [,2] ## [1,] NA &quot;4&quot; ## [2,] &quot;3&quot; &quot;Nice&quot; dim(matrix1) # first number is the number of rows, second number is number of columns ## [1] 4 3 4.7.3 Data frames Data frames are like matrices, in that they are also two-dimensional data structures. But a key distinction between the two is that data frames can accommodate different data types. Using the data.frame() function, we’ve created a fictitious data frame called exam_scores below. Note that exam_scores has two character variables called name and school, and three numeric variables called scoreMidterm, scoreFinal, and gender. exam_scores &lt;- data.frame(name = c(&quot;Jerry&quot;, &quot;Jason&quot;, &quot;Ambrose&quot;, &quot;Anna&quot;, &quot;Sarah&quot;, &quot;Jennifer&quot;), scoreMidterm = c(70, 80, 64, 75, 56, 93), scoreFinal = c(64, 83, 62, 78, 52, 88), gender = c(0,0,0,1,1,1), school = c(&quot;B&quot;, &quot;A&quot;, &quot;A&quot;, NA, NA, &quot;B&quot;)) class(exam_scores) ## [1] &quot;data.frame&quot; str(exam_scores) # note that the four variables have different data types ## &#39;data.frame&#39;: 6 obs. of 5 variables: ## $ name : chr &quot;Jerry&quot; &quot;Jason&quot; &quot;Ambrose&quot; &quot;Anna&quot; ... ## $ scoreMidterm: num 70 80 64 75 56 93 ## $ scoreFinal : num 64 83 62 78 52 88 ## $ gender : num 0 0 0 1 1 1 ## $ school : chr &quot;B&quot; &quot;A&quot; &quot;A&quot; NA ... head(exam_scores) # view first six rows of the data frame ## name scoreMidterm scoreFinal gender school ## 1 Jerry 70 64 0 B ## 2 Jason 80 83 0 A ## 3 Ambrose 64 62 0 A ## 4 Anna 75 78 1 &lt;NA&gt; ## 5 Sarah 56 52 1 &lt;NA&gt; ## 6 Jennifer 93 88 1 B We will commonly refer to one column in a data frame as a variable. A variable captures information on one characteristic for multiple individuals or entities (you might note the similarity to a vector above). For example, in the data frame exam_scores you just created, the variable scoreMidterm captures the information on midterm exam scores for individual students. Each row in this data frame represents an observation. Each variable, in turn, then contains information for that variable for each individual observation. For example, in the data frame exam_scores, each row captures information for one student, named in the variable name. For each student, we have information for the variables scoreMidterm, scoreFinal, gender, and school. We will generally indicate a generic observation with the letter \\(i\\). The number of observations is often described by \\(N\\). In total, the data frame above contains information on \\(N = 6\\) observations. Note an important concept in a data frame: Because each row belongs to one observation, we can’t just change the order of the values in one arbitrary variable and keep the others fixed. If you were to change the order in one of the variables, but not all others, you’d mix up the exam scores, or the gender, or the school of the students. We will learn below how to manipulate data frames, including how to create new variables. In the data frame exam_scores, for example, you might want to create a new variable that represents the average of the midterm and the final exam scores. Before we do this, however, we will get a real data set into R. It is generally not feasible or practical to enter a data set by hand, as we did above for exam_scores. It is also error-prone. Instead, we load or import data sets into R. 4.8 Study Questions for Data Basics 1 4.8.1 Importing Data What are the two methods for importing data into R that are covered in this reading? What does the getwd() function do, and why is it important to know your working directory? How do you change your working directory using R? Why might Windows users need to be careful about path formatting? 4.8.2 Inspecting and Understanding Data What functions can you use to view the first and last few rows of a dataset? What is the “unit of analysis” in a dataset? What is the unit of analysis in the genderCorona dataset? What is the difference between an observation and a variable in a data frame? How do you access a specific variable in a data frame using R syntax? What information does the summary() function provide about your data? 4.8.3 Saving and Modifying Data Why is it good practice to save modified data with a new filename rather than overwriting the original file? How do you create a new variable in an existing data frame? 4.8.4 Data Structures in R What are the three main data structures discussed in this reading, and how do they differ in dimensions? What is a vector in R? How do you create one? What happens when you try to combine different data types (numeric, character, NA) in a single vector? How are matrices different from vectors? What restriction do matrices have that data frames do not? Why are data frames particularly useful for data analysis compared to matrices? In the exam_scores data frame example, how many observations (N) are there? How many variables? Why can’t you change the order of values in just one variable of a data frame without affecting the others? "],["subsetting.html", "5 Subsetting 5.1 Learning Objectives 5.2 What Is This For? 5.3 Creating a Practice Dataset 5.4 Method 1: Subsetting with Square Brackets 5.5 Using Logical Conditions to Filter Rows 5.6 Method 2: The subset() Function 5.7 Method 3: Modern Subsetting with dplyr 5.8 Important Considerations 5.9 Which Method Should You Use? 5.10 Common Pitfalls and How to Avoid Them 5.11 Summary 5.12 Study Questions for Data Basics 2", " 5 Subsetting 5.1 Learning Objectives Learn what subsetting means and why it’s essential for data analysis Master three approaches to subsetting: bracket notation, the subset() function, and dplyr functions Understand how to filter rows based on conditions using logical operators Learn how to select specific columns from a data frame Practice combining row and column selection to extract exactly the data you need 5.2 What Is This For? In the previous session, we learned how to load data into R and understand its basic structure. But real-world data analysis rarely uses an entire dataset as-is. Instead, we need to focus on specific parts of our data – perhaps we only want to analyze data from certain countries, or we need to examine relationships between specific variables. This process of extracting portions of your data is called subsetting. Subsetting is fundamental to everything you’ll do in data analysis. Whether you’re cleaning messy data, creating visualizations for specific groups, or preparing data for statistical analysis, you’ll need to master the art of selecting exactly the data you need. Think of subsetting as using a precision tool rather than a sledgehammer - it allows you to work with exactly the right piece of data for your specific question. 5.3 Creating a Practice Dataset To learn subsetting, we’ll create a small dataset that’s easy to work with and verify our results. This dataset contains information about eight people’s favorite colors, flavors, and their ages: data &lt;- data.frame( colors = c(&quot;Green&quot;,&quot;Blue&quot;,&quot;Blue&quot;,&quot;Red&quot;,&quot;Green&quot;,&quot;Blue&quot;,&quot;Red&quot;,&quot;Red&quot;), flavors = c(&quot;Chocolate&quot;,&quot;Vanilla&quot;,&quot;Mint&quot;,&quot;Mint&quot;,&quot;Vanilla&quot;,&quot;Chocolate&quot;,&quot;Vanilla&quot;,&quot;Mint&quot;), ages = c(16, 20, 21, 21, 13, 15, 22, NA) ) rownames(data) &lt;- c(&quot;Jack&quot;,&quot;Mia&quot;,&quot;Josh&quot;,&quot;Kush&quot;,&quot;Isabel&quot;,&quot;Alex&quot;,&quot;Peter&quot;,&quot;Max&quot;) data ## colors flavors ages ## Jack Green Chocolate 16 ## Mia Blue Vanilla 20 ## Josh Blue Mint 21 ## Kush Red Mint 21 ## Isabel Green Vanilla 13 ## Alex Blue Chocolate 15 ## Peter Red Vanilla 22 ## Max Red Mint NA Notice that this dataset has 8 observations (rows) and 3 variables (columns). We’ve also assigned row names to make it easier to refer to specific people. Note that Max’s age is recorded as NA, which means it’s missing - this is common in real data and we’ll learn how to handle it. 5.4 Method 1: Subsetting with Square Brackets The most fundamental way to subset data in R uses square brackets: data[rows, columns]. This notation might look strange at first, but it follows a simple pattern: The first position (before the comma) specifies which rows you want The second position (after the comma) specifies which columns you want Leaving either position empty means “give me all” Let’s see this in action: # Get data for Mia and Josh (all columns) data[c(&quot;Mia&quot;, &quot;Josh&quot;), ] ## colors flavors ages ## Mia Blue Vanilla 20 ## Josh Blue Mint 21 Notice how we left the column position empty (after the comma), which means “all columns”. We used c() to combine multiple row names. # Get just the flavors column (all rows) data[, &quot;flavors&quot;] ## [1] &quot;Chocolate&quot; &quot;Vanilla&quot; &quot;Mint&quot; &quot;Mint&quot; &quot;Vanilla&quot; &quot;Chocolate&quot; &quot;Vanilla&quot; ## [8] &quot;Mint&quot; Here we left the row position empty (before the comma), which means “all rows”. But notice something important: R returned a vector, not a data frame! This automatic simplification can sometimes cause problems in your code. 5.4.1 Keeping the Data Frame Structure When you select a single column, R tries to be helpful by simplifying the result to a vector. But sometimes you need to keep the data frame structure. Use drop = FALSE to prevent this simplification: # Keep flavors as a one-column data frame data[, &quot;flavors&quot;, drop = FALSE] ## flavors ## Jack Chocolate ## Mia Vanilla ## Josh Mint ## Kush Mint ## Isabel Vanilla ## Alex Chocolate ## Peter Vanilla ## Max Mint 5.4.2 Combining Row and Column Selection You can specify both rows and columns to get very specific subsets: # Get Mia and Josh&#39;s favorite flavors data[c(&quot;Mia&quot;, &quot;Josh&quot;), &quot;flavors&quot;, drop = FALSE] ## flavors ## Mia Vanilla ## Josh Mint 5.4.3 Using Numbers Instead of Names If your data doesn’t have row names (or you don’t know them), you can use row numbers: # Get rows 2 and 3 data[c(2, 3), ] ## colors flavors ages ## Mia Blue Vanilla 20 ## Josh Blue Mint 21 5.5 Using Logical Conditions to Filter Rows One of the most powerful features of subsetting is the ability to select rows based on conditions. To do this effectively, we need to understand logical operators. 5.5.1 Understanding Logical Operators Logical operators compare values and return TRUE or FALSE: # Comparison operators 21 &gt; 20 # greater than ## [1] TRUE 21 &lt; 20 # less than ## [1] FALSE 21 &gt;= 21 # greater than or equal to ## [1] TRUE 21 &lt;= 21 # less than or equal to ## [1] TRUE 21 == 21 # exactly equal to ## [1] TRUE 21 != 20 # not equal to ## [1] TRUE Important distinction: - Use = (or &lt;-) to assign values to variables - Use == to test if two values are equal 5.5.2 Filtering Rows with Conditions We can use these logical operators to create conditions that filter our data. The $ operator extracts a column as a vector: # Which people are exactly 21 years old? data$ages == 21 ## [1] FALSE FALSE TRUE TRUE FALSE FALSE FALSE NA This creates a logical vector with TRUE for rows where the condition is met. We can use this to subset: # Get data for people who are exactly 21 data[data$ages == 21, ] ## colors flavors ages ## Josh Blue Mint 21 ## Kush Red Mint 21 ## NA &lt;NA&gt; &lt;NA&gt; NA 5.5.3 Combining Multiple Conditions Use &amp; (AND) when both conditions must be true: # People who are at least 21 AND like Red data[data$ages &gt;= 21 &amp; data$colors == &quot;Red&quot;, ] ## colors flavors ages ## Kush Red Mint 21 ## Peter Red Vanilla 22 ## NA &lt;NA&gt; &lt;NA&gt; NA Use | (OR) when at least one condition must be true: # People who like Green OR Vanilla data[data$colors == &quot;Green&quot; | data$flavors == &quot;Vanilla&quot;, ] ## colors flavors ages ## Jack Green Chocolate 16 ## Mia Blue Vanilla 20 ## Isabel Green Vanilla 13 ## Peter Red Vanilla 22 5.6 Method 2: The subset() Function While bracket notation is fundamental, R provides the subset() function to make subsetting more readable. It lets you refer to column names directly without using $: # Get rows where color is Green subset(data, colors == &quot;Green&quot;) ## colors flavors ages ## Jack Green Chocolate 16 ## Isabel Green Vanilla 13 5.6.1 Selecting Specific Columns with subset() The subset() function has a select parameter for choosing columns: # Select only the ages column subset(data, select = ages) ## ages ## Jack 16 ## Mia 20 ## Josh 21 ## Kush 21 ## Isabel 13 ## Alex 15 ## Peter 22 ## Max NA Note that inside select, you use bare column names (no quotes). Even when selecting one column, subset() returns a data frame, not a vector. You can select multiple columns: # Select colors and ages columns subset(data, select = c(colors, ages)) ## colors ages ## Jack Green 16 ## Mia Blue 20 ## Josh Blue 21 ## Kush Red 21 ## Isabel Green 13 ## Alex Blue 15 ## Peter Red 22 ## Max Red NA Or exclude columns using a minus sign: # Keep all columns except ages and colors subset(data, select = -c(ages, colors)) ## flavors ## Jack Chocolate ## Mia Vanilla ## Josh Mint ## Kush Mint ## Isabel Vanilla ## Alex Chocolate ## Peter Vanilla ## Max Mint 5.6.2 Combining Row and Column Operations You can filter rows and select columns in one command: # Get colors and ages for people who like Vanilla subset(data, flavors == &quot;Vanilla&quot;, select = c(colors, ages)) ## colors ages ## Mia Blue 20 ## Isabel Green 13 ## Peter Red 22 5.6.3 Handling Missing Values Real data often contains missing values. Here’s how to exclude rows with missing ages: # Remove rows where age is NA subset(data, !is.na(ages)) ## colors flavors ages ## Jack Green Chocolate 16 ## Mia Blue Vanilla 20 ## Josh Blue Mint 21 ## Kush Red Mint 21 ## Isabel Green Vanilla 13 ## Alex Blue Chocolate 15 ## Peter Red Vanilla 22 The ! means “not”, so !is.na(ages) means “ages is not NA”. 5.7 Method 3: Modern Subsetting with dplyr The dplyr package provides a modern, readable approach to data manipulation. Its functions are designed to work together in a pipeline using the %&gt;% operator (pronounced “pipe”). First, let’s load the package: library(dplyr) 5.7.1 Understanding the Pipe Operator The pipe (%&gt;%) takes the output from the left side and passes it as the first argument to the function on the right side. Think of it as “then”: # Traditional approach filter(data, colors == &quot;Red&quot;) ## colors flavors ages ## Kush Red Mint 21 ## Peter Red Vanilla 22 ## Max Red Mint NA # Pipeline approach: &quot;Take data, THEN filter it&quot; data %&gt;% filter(colors == &quot;Red&quot;) ## colors flavors ages ## Kush Red Mint 21 ## Peter Red Vanilla 22 ## Max Red Mint NA 5.7.2 Filtering Rows with filter() The filter() function selects rows based on conditions: # Keep only adults (age &gt;= 18) data %&gt;% filter(ages &gt;= 18) ## colors flavors ages ## Mia Blue Vanilla 20 ## Josh Blue Mint 21 ## Kush Red Mint 21 ## Peter Red Vanilla 22 5.7.3 Selecting Columns with select() The select() function chooses which columns to keep: # Keep only colors and ages columns data %&gt;% select(colors, ages) ## colors ages ## Jack Green 16 ## Mia Blue 20 ## Josh Blue 21 ## Kush Red 21 ## Isabel Green 13 ## Alex Blue 15 ## Peter Red 22 ## Max Red NA 5.7.4 Chaining Operations The real power of dplyr comes from chaining multiple operations: # First filter for adults, then select only colors data %&gt;% filter(ages &gt;= 18) %&gt;% select(colors) ## colors ## Mia Blue ## Josh Blue ## Kush Red ## Peter Red This reads naturally: “Take the data, filter for ages 18 and up, then select just the colors column.” 5.7.5 Complex Conditions You can use all the logical operators we learned earlier: # Complex filtering with parentheses for clarity data %&gt;% filter((ages &gt;= 21 &amp; colors == &quot;Red&quot;) | flavors == &quot;Vanilla&quot;) %&gt;% select(colors, flavors, ages) ## colors flavors ages ## Mia Blue Vanilla 20 ## Kush Red Mint 21 ## Isabel Green Vanilla 13 ## Peter Red Vanilla 22 5.8 Important Considerations 5.8.1 Saving Your Subsets Remember that printing a subset doesn’t save it. If you need to use the subset later, assign it to a new object: # This just prints but doesn&#39;t save data %&gt;% filter(flavors == &quot;Vanilla&quot;) ## colors flavors ages ## Mia Blue Vanilla 20 ## Isabel Green Vanilla 13 ## Peter Red Vanilla 22 # This saves the subset for later use vanilla_lovers &lt;- data %&gt;% filter(flavors == &quot;Vanilla&quot;) # Now we can use it mean(vanilla_lovers$ages, na.rm = TRUE) ## [1] 18.33333 5.8.2 Pipes Don’t Modify the Original Data A common misconception is that pipes modify your original data. They don’t - unless you explicitly assign the result: # This doesn&#39;t change &#39;data&#39; data %&gt;% filter(ages &gt;= 21) ## colors flavors ages ## Josh Blue Mint 21 ## Kush Red Mint 21 ## Peter Red Vanilla 22 # Check - original data is unchanged nrow(data) # Still 8 rows ## [1] 8 To save changes, you must assign: # Create a new dataset with only adults adults &lt;- data %&gt;% filter(ages &gt;= 18) # Or overwrite the original (be careful!) # data &lt;- data %&gt;% filter(ages &gt;= 18) 5.9 Which Method Should You Use? Each subsetting method has its place: Bracket notation [,]: Best when you need precise control or are working with matrices subset(): Good for quick, readable subsetting in base R dplyr functions: Ideal for complex data manipulation pipelines For this course, we’ll primarily use dplyr because: It’s highly readable - code reads like a series of instructions It’s consistent - all functions work similarly It’s powerful - easy to chain complex operations It’s widely used in modern data science 5.10 Common Pitfalls and How to Avoid Them 5.10.1 1. Forgetting Quotes Around Character Values # Wrong - R looks for an object named Red # data[data$colors == Red, ] # Correct - &quot;Red&quot; is a character string data[data$colors == &quot;Red&quot;, ] ## colors flavors ages ## Kush Red Mint 21 ## Peter Red Vanilla 22 ## Max Red Mint NA 5.10.2 2. Mixing Data Types in Comparisons # This will never match - comparing character to number data$colors == 21 # All FALSE or NA ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE 5.10.3 3. Not Handling Missing Values # This includes NA in the result data[data$ages &gt; 18, ] ## colors flavors ages ## Mia Blue Vanilla 20 ## Josh Blue Mint 21 ## Kush Red Mint 21 ## Peter Red Vanilla 22 ## NA &lt;NA&gt; &lt;NA&gt; NA # This explicitly removes NA values first data %&gt;% filter(!is.na(ages), ages &gt; 18) ## colors flavors ages ## Mia Blue Vanilla 20 ## Josh Blue Mint 21 ## Kush Red Mint 21 ## Peter Red Vanilla 22 5.11 Summary Subsetting is fundamental to data analysis in R. We’ve learned three approaches: Bracket notation: data[rows, columns] - the foundation of all subsetting subset() function: More readable for simple operations dplyr functions: Modern, chainable approach for complex operations The key concepts to remember: - Use logical operators (==, !=, &gt;, &lt;, &amp;, |) to create conditions - Missing values (NA) require special handling - Saving subsets requires explicit assignment with &lt;- - Different methods suit different situations As you work with larger, more complex datasets, these subsetting skills will become second nature. Practice with different combinations of row and column selection until you’re comfortable extracting exactly the data you need. 5.12 Study Questions for Data Basics 2 5.12.1 Understanding Subsetting Concepts What does “subsetting” mean in the context of data analysis? Why is it important? In the bracket notation data[rows, columns], what does it mean to leave one position empty? What’s the difference between getting data[, \"flavors\"] and data[, \"flavors\", drop = FALSE]? When would you use each? 5.12.2 Logical Operators and Conditions What’s the difference between = and == in R? Write a logical expression that would find all people in our dataset who are teenagers (ages 13-19). What’s the difference between &amp; (AND) and | (OR) when combining conditions? How would you select all people who are NOT 21 years old? 5.12.3 Using subset() What are the two main parameters of the subset() function and what does each do? How do you exclude columns using subset()? Give an example. Why don’t you use quotes around column names inside subset()’s select parameter? 5.12.4 Working with dplyr What does the pipe operator %&gt;% do? How would you read it in plain English? What’s the dplyr equivalent of subset(data, ages &gt;= 21)? Write a dplyr pipeline that: (a) removes rows with missing ages, (b) keeps only people 18 or older, and (c) selects only the colors column. 5.12.5 Handling Missing Values How would you create a subset that excludes all rows where age is NA? What function do you use to test if a value is missing? 5.12.6 Common Mistakes Why doesn’t data %&gt;% filter(ages &gt;= 21) permanently change the data object? What’s wrong with this code: data[data$flavors == Vanilla, ]? How would you fix it? If you want to analyze just the Mint lovers later in your script, what must you remember to do after subsetting? "],["data-types-and-levels-of-measurement.html", "6 Data Types and Levels of Measurement 6.1 Learning Objectives 6.2 What Is This For? 6.3 Basic Data Types in R 6.4 Working with Data Types 6.5 Levels of Measurement 6.6 Discrete vs. Continuous Variables 6.7 The Information Hierarchy 6.8 Practical Guidelines 6.9 Summary 6.10 Study Questions", " 6 Data Types and Levels of Measurement 6.1 Learning Objectives Understand the five basic data types in R: numeric, integer, factor, character, and logical Learn how to check and convert between different data types using R functions Master the four levels of measurement: nominal, ordinal, interval, and ratio Understand the difference between discrete and continuous variables Apply these concepts to make informed decisions about how to store and analyze your data 6.2 What Is This For? When you open a new dataset, one of your first tasks is understanding what kinds of variables you’re working with. Not all data is created equal - some variables represent categories (like political party affiliation), others represent ordered rankings (like movie ratings), and still others represent precise measurements (like vote percentages). Understanding data types and measurement levels is crucial because: It determines which statistical analyses you can perform It affects how R stores and processes your data It helps you avoid common errors like trying to calculate the average political party It ensures you’re interpreting your results correctly Think of this as learning the “grammar” of data - just as you need to know whether a word is a noun or verb to use it properly in a sentence, you need to know whether a variable is nominal or ratio to analyze it properly. 6.3 Basic Data Types in R R recognizes several fundamental data types. Each column in your dataset can be a different type, and R needs to know which type each variable is to handle it correctly. 6.3.1 The Five Main Data Types 1. Numeric: Real numbers with decimal places Examples: 3.14, 98.6, 0.5 Political science example: Vote share (0.523 means 52.3% of votes) 2. Integer: Whole numbers without decimal places Examples: 1, 42, -10 Political science example: Number of Senate seats (100) 3. Character: Text strings Examples: “Democrat”, “Republican”, “Independent” Political science example: Country names, survey responses 4. Factor: Categorical data with defined levels Like character, but R knows there are only specific possible values Political science example: Region (Northeast, South, Midwest, West) 5. Logical: TRUE/FALSE values Only two possible values: TRUE or FALSE Political science example: Whether a bill passed (TRUE) or failed (FALSE) 6.3.2 Why Data Types Matter Functions in R only work with appropriate data types. You can’t calculate the mean of country names, and R prevents you from trying: # This won&#39;t work - and that&#39;s good! countries &lt;- c(&quot;USA&quot;, &quot;Canada&quot;, &quot;Mexico&quot;) mean(countries) ## Warning in mean.default(countries): argument is not numeric or logical: returning NA ## [1] NA 6.4 Working with Data Types Let’s load the movies dataset to explore data types in practice. You can find this dataset on canvas in the datasets folder. # Load the movies dataset MovieData &lt;- read.csv(&quot;movies.csv&quot;) # Look at the structure str(MovieData) ## &#39;data.frame&#39;: 651 obs. of 32 variables: ## $ title : chr &quot;Filly Brown&quot; &quot;The Dish&quot; &quot;Waiting for Guffman&quot; &quot;The Age of Innocence&quot; ... ## $ title_type : chr &quot;Feature Film&quot; &quot;Feature Film&quot; &quot;Feature Film&quot; &quot;Feature Film&quot; ... ## $ genre : chr &quot;Drama&quot; &quot;Drama&quot; &quot;Comedy&quot; &quot;Drama&quot; ... ## $ runtime : int 80 101 84 139 90 78 142 93 88 119 ... ## $ mpaa_rating : chr &quot;R&quot; &quot;PG-13&quot; &quot;R&quot; &quot;PG&quot; ... ## $ studio : chr &quot;Indomina Media Inc.&quot; &quot;Warner Bros. Pictures&quot; &quot;Sony Pictures Classics&quot; &quot;Columbia Pictures&quot; ... ## $ thtr_rel_year : int 2013 2001 1996 1993 2004 2009 1986 1996 2012 2012 ... ## $ thtr_rel_month : int 4 3 8 10 9 1 1 11 9 3 ... ## $ thtr_rel_day : int 19 14 21 1 10 15 1 8 7 2 ... ## $ dvd_rel_year : int 2013 2001 2001 2001 2005 2010 2003 2004 2013 2012 ... ## $ dvd_rel_month : int 7 8 8 11 4 4 2 3 1 8 ... ## $ dvd_rel_day : int 30 28 21 6 19 20 18 2 21 14 ... ## $ imdb_rating : num 5.5 7.3 7.6 7.2 5.1 7.8 7.2 5.5 7.5 6.6 ... ## $ imdb_num_votes : int 899 12285 22381 35096 2386 333 5016 2272 880 12496 ... ## $ critics_rating : chr &quot;Rotten&quot; &quot;Certified Fresh&quot; &quot;Certified Fresh&quot; &quot;Certified Fresh&quot; ... ## $ critics_score : int 45 96 91 80 33 91 57 17 90 83 ... ## $ audience_rating : chr &quot;Upright&quot; &quot;Upright&quot; &quot;Upright&quot; &quot;Upright&quot; ... ## $ audience_score : int 73 81 91 76 27 86 76 47 89 66 ... ## $ best_pic_nom : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ best_pic_win : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ best_actor_win : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... ## $ best_actress_win: chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ best_dir_win : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... ## $ top200_box : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ director : chr &quot;Michael D. Olmos&quot; &quot;Rob Sitch&quot; &quot;Christopher Guest&quot; &quot;Martin Scorsese&quot; ... ## $ actor1 : chr &quot;Gina Rodriguez&quot; &quot;Sam Neill&quot; &quot;Christopher Guest&quot; &quot;Daniel Day-Lewis&quot; ... ## $ actor2 : chr &quot;Jenni Rivera&quot; &quot;Kevin Harrington&quot; &quot;Catherine O&#39;Hara&quot; &quot;Michelle Pfeiffer&quot; ... ## $ actor3 : chr &quot;Lou Diamond Phillips&quot; &quot;Patrick Warburton&quot; &quot;Parker Posey&quot; &quot;Winona Ryder&quot; ... ## $ actor4 : chr &quot;Emilio Rivera&quot; &quot;Tom Long&quot; &quot;Eugene Levy&quot; &quot;Richard E. Grant&quot; ... ## $ actor5 : chr &quot;Joseph Julian Soria&quot; &quot;Genevieve Mooy&quot; &quot;Bob Balaban&quot; &quot;Alec McCowen&quot; ... ## $ imdb_url : chr &quot;http://www.imdb.com/title/tt1869425/&quot; &quot;http://www.imdb.com/title/tt0205873/&quot; &quot;http://www.imdb.com/title/tt0118111/&quot; &quot;http://www.imdb.com/title/tt0106226/&quot; ... ## $ rt_url : chr &quot;//www.rottentomatoes.com/m/filly_brown_2012/&quot; &quot;//www.rottentomatoes.com/m/dish/&quot; &quot;//www.rottentomatoes.com/m/waiting_for_guffman/&quot; &quot;//www.rottentomatoes.com/m/age_of_innocence/&quot; ... 6.4.1 Checking Data Types Use the class() function to check any variable’s type: class(MovieData$title) # Character - movie titles are text ## [1] &quot;character&quot; class(MovieData$runtime) # Integer - runtime in whole minutes ## [1] &quot;integer&quot; class(MovieData$imdb_rating) # Numeric - ratings with decimals ## [1] &quot;numeric&quot; 6.4.2 Converting Between Types Use functions that start with as. to convert between types: # Convert runtime from integer to numeric MovieData$runtime &lt;- as.numeric(MovieData$runtime) class(MovieData$runtime) ## [1] &quot;numeric&quot; # But be careful - nonsensical conversions create NAs as.numeric(&quot;Hello&quot;) # Can&#39;t turn text into numbers! ## Warning: NAs introduced by coercion ## [1] NA 6.4.3 Understanding Factors Factors are R’s way of handling categorical data efficiently. They’re stored as numbers internally but displayed as text: # Convert genre to a factor MovieData$genre &lt;- as.factor(MovieData$genre) # See the levels levels(MovieData$genre) ## [1] &quot;Action &amp; Adventure&quot; &quot;Animation&quot; &quot;Art House &amp; International&quot; ## [4] &quot;Comedy&quot; &quot;Documentary&quot; &quot;Drama&quot; ## [7] &quot;Horror&quot; &quot;Musical &amp; Performing Arts&quot; &quot;Mystery &amp; Suspense&quot; ## [10] &quot;Other&quot; &quot;Science Fiction &amp; Fantasy&quot; # How many movies in each genre? table(MovieData$genre) ## ## Action &amp; Adventure Animation Art House &amp; International ## 65 9 14 ## Comedy Documentary Drama ## 87 52 305 ## Horror Musical &amp; Performing Arts Mystery &amp; Suspense ## 23 12 59 ## Other Science Fiction &amp; Fantasy ## 16 9 Factors are useful because: They prevent typos (R knows the valid categories) They save memory (stored as numbers internally) They prevent inappropriate calculations # R won&#39;t let you do math on factors - good! mean(MovieData$genre) ## Warning in mean.default(MovieData$genre): argument is not numeric or logical: returning NA ## [1] NA 6.5 Levels of Measurement Beyond R’s data types, we need to understand the conceptual level at which variables are measured. This determines what analyses make sense. 6.5.1 1. Nominal (Categorical) Scale Definition: Categories with no inherent order Can distinguish between groups No meaningful ranking Numbers are just labels Examples: Political party affiliation (Democrat, Republican, Independent) Country of origin Religion Movie genre What you CAN do: Count frequencies, find the mode What you CAN’T do: Calculate means, say one is “greater than” another 6.5.2 2. Ordinal Scale Definition: Categories with a meaningful order but no consistent distance between values Clear ranking exists Distances between ranks aren’t equal or known Can say “more than” but not “how much more” Examples: Education level (High school &lt; Bachelor’s &lt; Master’s &lt; PhD) Survey responses (Strongly disagree to Strongly agree) Military ranks Movie ratings (1 star to 5 stars) What you CAN do: Everything from nominal plus find the median, percentiles What you CAN’T do: Calculate meaningful means or standard deviations 6.5.3 3. Interval Scale Definition: Ordered values with consistent, meaningful distances but no true zero Equal intervals between values Zero doesn’t mean “absence of” Can’t make ratio statements Examples: Temperature in Celsius or Fahrenheit Years (2020, 2021, 2022) IQ scores Key insight: 80°F is 20° warmer than 60°F, but it’s NOT “33% warmer” 6.5.4 4. Ratio Scale Definition: Interval scale with a true zero point Zero means complete absence All mathematical operations valid Can make ratio statements Examples: Income in dollars Age in years Vote percentage Population size Distance in miles Key insight: $100 is twice as much as $50 (meaningful ratio) 6.6 Discrete vs. Continuous Variables Another important distinction cuts across measurement levels: 6.6.1 Discrete Variables Can only take specific, countable values Gaps between possible values Examples: Number of children, shoe sizes, number of wars 6.6.2 Continuous Variables Can take any value within a range Infinite possible values between any two points Examples: Height, time, temperature, vote percentage # Discrete: Number of Oscars won (can&#39;t win 2.5 Oscars) table(MovieData$best_pic_win) ## ## no yes ## 644 7 # Continuous: IMDB ratings (can be any value from 0-10) summary(MovieData$imdb_rating) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.900 5.900 6.600 6.493 7.300 9.000 6.7 The Information Hierarchy Think of measurement levels as an information hierarchy: Image Credit: GPT Key principle: You can always move DOWN the hierarchy (lose information) but never UP (can’t create information that wasn’t collected). 6.7.1 Example: Income Data Imagine surveying income: Ratio approach: “What was your income last year? $_____” Can calculate exact averages, ratios, all statistics Can later group into brackets if needed Ordinal approach: “Select your income bracket: Low / Medium / High” Can only find which bracket is most common Can NEVER recover exact income values Lost information forever! 6.8 Practical Guidelines 6.8.1 Choosing How to Measure When designing data collection: Collect at the highest level possible - you can always simplify later Consider your analysis needs - what statistics will you want to calculate? Balance precision with feasibility - will respondents know/share exact values? 6.8.2 Common Pitfalls to Avoid Treating ordinal as interval: Just because you code “Strongly Agree” as 5 doesn’t mean it’s 5 times more than “Strongly Disagree” (1) Over-aggregating: Converting exact ages to “Young/Old” loses valuable information Wrong data type in R: Zip codes stored as numeric (leading zeros disappear!) # Bad: Zip code as numeric loses leading zeros zip_numeric &lt;- 01234 zip_numeric # Prints as 1234! ## [1] 1234 # Good: Zip code as character preserves format zip_character &lt;- &quot;01234&quot; zip_character # Keeps the leading zero ## [1] &quot;01234&quot; 6.9 Summary Understanding data types and measurement levels is fundamental to data analysis: R data types determine how your computer stores and processes data Measurement levels determine what analyses are mathematically meaningful Always collect data at the highest level possible - you can simplify but can’t add detail Match your analysis to your measurement level to avoid nonsensical results Remember: Just because R lets you calculate something doesn’t mean it’s meaningful. Understanding these concepts helps you avoid the trap of “garbage in, garbage out” in your analyses. 6.10 Study Questions 6.10.1 Data Types in R What are the five main data types in R? Give an example of each from political science. Why does R prevent you from calculating the mean of a character variable? What function do you use to check a variable’s data type? What function converts a variable to numeric? What happens when you try to convert the text “Democracy” to numeric? Why? 6.10.2 Factors What are the advantages of using factors instead of character variables for categorical data? How are factors stored internally in R? How can you see this internal storage? If you have a factor variable for U.S. regions with levels “Northeast”, “South”, “Midwest”, and “West”, what number would R internally use to store “Midwest”? 6.10.3 Levels of Measurement What are the four levels of measurement? List them from least to most information. For each level of measurement, give an example NOT mentioned in the reading. Why can temperature in Fahrenheit not be measured on a ratio scale? What statistical measures are appropriate for ordinal but NOT nominal data? 6.10.4 Converting Between Levels Can you convert a nominal variable to a ratio variable? Why or why not? If you collected exact ages but later grouped them into “Young/Middle-aged/Old”, what level of measurement did you convert FROM and TO? Give an example where converting from a higher to lower measurement level would be scientifically justified. 6.10.5 Discrete vs. Continuous Is the number of bills passed by Congress discrete or continuous? Why? Is voter turnout percentage discrete or continuous? Why? 6.10.6 Practical Applications You’re designing a survey question about income. What are the trade-offs between asking for exact income (ratio) versus income brackets (ordinal)? A student codes survey responses “Strongly Disagree=1” through “Strongly Agree=5” and calculates the mean as 3.2. What’s wrong with reporting “the average response was 3.2”? Why might you store zip codes as character variables rather than numeric, even though they’re “numbers”? You have a dataset where religious affiliation is coded as: Christian=1, Jewish=2, Muslim=3, Other=4, None=5. A colleague suggests analyzing this as numeric to see if “religious affiliation increases” in your sample. What’s wrong with this approach? "],["creating-and-transforming-variables-in-r.html", "7 Creating and Transforming Variables in R 7.1 Learning Objectives 7.2 What Is This For? 7.3 The CCES Dataset 7.4 Creating New Variables from Scratch 7.5 Mathematical Transformations 7.6 Recoding Variables 7.7 Creating Categorical Variables from Continuous Data 7.8 Creating Dummy/Indicator Variables 7.9 Conditional Variable Creation 7.10 Best Practices and Common Pitfalls 7.11 Summary 7.12 Study Questions", " 7 Creating and Transforming Variables in R 7.1 Learning Objectives Learn how to create new variables in a data frame using both base R and dplyr Master different approaches to recoding existing variables Understand how to create categorical variables from continuous data Learn to create dummy/indicator variables Practice using conditional logic to create complex variables 7.2 What Is This For? When working with real data, you’ll rarely find variables in exactly the format you need for analysis. Survey data might code responses as numbers when you need meaningful labels. You might need to combine multiple variables to create a composite measure. Or you might need to transform continuous data into categories for certain analyses. This page teaches you the essential skills for transforming and creating variables - a crucial step between loading raw data and conducting meaningful analysis. These skills will help you: Make your data more interpretable Prepare variables for specific statistical techniques Create new measures that better capture your theoretical concepts Handle different coding schemes across datasets 7.3 The CCES Dataset For this session, we’ll use data from the Cooperative Congressional Election Study (CCES), one of the largest academic surveys in the United States. This dataset contains responses from 124,600 Americans about their political attitudes, behaviors, and demographics. # Load necessary packages library(dplyr) library(tidyverse) # Load the CCES data cces &lt;- read.csv(&quot;cces.csv&quot;, row.names = 1) # Explore the structure dim(cces) ## [1] 124600 23 names(cces) ## [1] &quot;weight&quot; &quot;year&quot; &quot;atts&quot; &quot;pid7&quot; &quot;aware&quot; &quot;aware.p&quot; ## [7] &quot;lgbt&quot; &quot;race&quot; &quot;religion&quot; &quot;female&quot; &quot;union.member&quot; &quot;veteran&quot; ## [13] &quot;age&quot; &quot;married&quot; &quot;income&quot; &quot;educ&quot; &quot;rel.atd&quot; &quot;region&quot; ## [19] &quot;ideo&quot; &quot;vote&quot; &quot;lgb&quot; &quot;trans&quot; &quot;interest&quot; Key variables we’ll work with include: age: Respondent’s age in years pid7: Party identification (0-1 scale, higher = more Democratic) aware: Political awareness score (0-1 scale) income: Household income quintile vote: Presidential vote choice (1 = Democrat, 0 = Republican) 7.4 Creating New Variables from Scratch The simplest form of variable creation involves mathematical operations on existing variables. We’ll show both base R and dplyr approaches. 7.4.1 Adding Constants Sometimes you need a constant value for all observations: # Base R approach cces$survey_year &lt;- 2020 cces$respondent_id &lt;- 1:nrow(cces) # dplyr approach cces &lt;- cces %&gt;% mutate(survey_year_2 = 2020, respondent_id_2 = row_number()) # Verify they&#39;re the same identical(cces$survey_year, cces$survey_year_2) ## [1] TRUE 7.4.2 Simple Arithmetic Combinations Create new variables by combining existing ones: # Base R approach: years until retirement cces$years_to_retire &lt;- 65 - cces$age cces$years_to_retire[cces$years_to_retire &lt; 0] &lt;- 0 # Code all cases where they are over 65 as zero # dplyr approach: same calculation cces &lt;- cces %&gt;% mutate(years_to_retire_2 = pmax(65 - age, 0)) # pmax ensures no negative values # Convert awareness to percentage - Base R cces$aware_pct &lt;- cces$aware * 100 # Convert awareness to percentage - dplyr cces &lt;- cces %&gt;% mutate(aware_pct_2 = aware * 100) # Compare the two approaches cces$aware_pct[1:10] # show first 10 observations ## [1] 40.000000 86.666667 96.666667 90.000000 100.000000 96.666667 26.666667 100.000000 ## [9] 100.000000 3.333333 cces$aware_pct_2[1:10] # show same first 10 observations ## [1] 40.000000 86.666667 96.666667 90.000000 100.000000 96.666667 26.666667 100.000000 ## [9] 100.000000 3.333333 7.5 Mathematical Transformations Sometimes we need to transform variables to different scales or distributions. 7.5.1 Rescaling Variables Political scientists often need to rescale variables to make them comparable: # Base R: Rescale party ID from 0-1 to 1-7 cces$pid7_traditional &lt;- round(cces$pid7 * 6 + 1) # dplyr approach cces &lt;- cces %&gt;% mutate(pid7_traditional_2 = round(pid7 * 6 + 1)) # Check both transformations table(Base_R = cces$pid7_traditional, dplyr = cces$pid7_traditional_2) ## dplyr ## Base_R 1 2 3 4 5 6 7 ## 1 18536 0 0 0 0 0 0 ## 2 0 12741 0 0 0 0 0 ## 3 0 0 11290 0 0 0 0 ## 4 0 0 0 19025 0 0 0 ## 5 0 0 0 0 12432 0 0 ## 6 0 0 0 0 0 15841 0 ## 7 0 0 0 0 0 0 30473 7.6 Recoding Variables Recoding changes the values of a variable according to specified rules. 7.6.1 Using Base R Methods The simplest approach uses logical indexing: # Base R: Create text labels for vote choice cces$vote_label &lt;- NA # Start with NA cces$vote_label[cces$vote == 1] &lt;- &quot;Democrat&quot; cces$vote_label[cces$vote == 0] &lt;- &quot;Republican&quot; # Alternative base R using ifelse cces$vote_label_alt &lt;- ifelse(cces$vote == 1, &quot;Democrat&quot;, ifelse(cces$vote == 0, &quot;Republican&quot;, NA)) table(cces$vote_label) ## ## Democrat Republican ## 22136 18755 7.6.2 Using dplyr Methods The dplyr package offers multiple approaches for recoding: # Using recode() cces &lt;- cces %&gt;% mutate(vote_label_2 = dplyr::recode(vote, `1` = &quot;Democrat&quot;, `0` = &quot;Republican&quot;)) # Using case_when() - more flexible cces &lt;- cces %&gt;% mutate(vote_label_3 = case_when( vote == 1 ~ &quot;Democrat&quot;, vote == 0 ~ &quot;Republican&quot;, TRUE ~ NA_character_ # Catch all other cases )) # Recode education levels cces &lt;- cces %&gt;% mutate(educ_label = dplyr::recode(educ, `1` = &quot;No HS&quot;, `2` = &quot;High school&quot;, `3` = &quot;Some college&quot;, `4` = &quot;2-year degree&quot;, `5` = &quot;4-year degree&quot;, `6` = &quot;Post-grad&quot; )) 7.6.3 Reversing Scales Sometimes you need to reverse a scale’s direction: # Base R: Create Republican partisanship cces$pid7_republican &lt;- 1 - cces$pid7 # dplyr approach cces &lt;- cces %&gt;% mutate(pid7_republican_2 = 1 - pid7) 7.7 Creating Categorical Variables from Continuous Data The cut() function divides continuous variables into categories. 7.7.1 Equal-Width Categories # Base R approach cces$age_groups &lt;- cut(cces$age, breaks = c(18, 38, 58, 78, 100), labels = c(&quot;18-37&quot;, &quot;38-57&quot;, &quot;58-77&quot;, &quot;78+&quot;), include.lowest = TRUE) # dplyr approach - same cut() function within mutate cces &lt;- cces %&gt;% mutate(age_groups_2 = cut(age, breaks = c(18, 38, 58, 78, 100), labels = c(&quot;18-37&quot;, &quot;38-57&quot;, &quot;58-77&quot;, &quot;78+&quot;), include.lowest = TRUE)) table(cces$age_groups) ## ## 18-37 38-57 58-77 78+ ## 43901 41754 34784 4161 7.7.2 Creating Meaningful Breakpoints Use substantively meaningful cutoffs: # Calculate birth year first current_year &lt;- 2020 # Base R approach cces$birth_year &lt;- current_year - cces$age cces$generation &lt;- cut(cces$birth_year, breaks = c(1900, 1945, 1964, 1980, 1996, 2010), labels = c(&quot;Silent&quot;, &quot;Boomer&quot;, &quot;Gen X&quot;, &quot;Millennial&quot;, &quot;Gen Z&quot;)) # dplyr approach - all in one pipeline cces &lt;- cces %&gt;% mutate(birth_year_2 = current_year - age, generation_2 = cut(birth_year_2, breaks = c(1900, 1945, 1964, 1980, 1996, 2010), labels = c(&quot;Silent&quot;, &quot;Boomer&quot;, &quot;Gen X&quot;, &quot;Millennial&quot;, &quot;Gen Z&quot;))) table(cces$generation, cces$generation_2) ## ## Silent Boomer Gen X Millennial Gen Z ## Silent 8285 0 0 0 0 ## Boomer 0 38456 0 0 0 ## Gen X 0 0 32097 0 0 ## Millennial 0 0 0 36271 0 ## Gen Z 0 0 0 0 9491 7.8 Creating Dummy/Indicator Variables Dummy variables are binary (0/1) variables that indicate membership in a category. 7.8.1 Manual Creation Create dummy variables using logical conditions: # Base R approach cces$elderly &lt;- ifelse(cces$age &gt;= 65, 1, 0) cces$high_awareness &lt;- ifelse(cces$aware &gt; 0.7, 1, 0) # dplyr approach cces &lt;- cces %&gt;% mutate(elderly_2 = if_else(age &gt;= 65, 1, 0), high_awareness_2 = if_else(aware &gt; 0.7, 1, 0, missing = NA_real_)) # Check proportions mean(cces$elderly, na.rm = TRUE) ## [1] 0.1946148 mean(cces$elderly_2, na.rm = TRUE) ## [1] 0.1946148 7.8.2 Using Logical Operators for Complex Cases Often we need to create indicators based on multiple conditions combined with AND (&amp;) or OR (|) operators. Let’s create a “swing voter” indicator - these are voters who might be persuadable because they’re both ideologically moderate AND not strongly partisan. First, let’s understand our variables: ideo ranges from 0 (very conservative) to 1 (very liberal), so 0.5 is the center pid7 ranges from 0 (strong Republican) to 1 (strong Democrat), so 0.5 is independent # Base R: Swing voter indicator # We want people who are: # 1. Ideologically moderate (within 0.2 of center on ideology scale) # 2. AND politically independent (within 0.2 of center on party ID scale) cces$swing_voter &lt;- ifelse( abs(cces$ideo - 0.5) &lt; 0.2 &amp; # abs() gets distance from center abs(cces$pid7 - 0.5) &lt; 0.2, # &amp; means BOTH conditions must be true 1, # If both conditions met, assign 1 0 # Otherwise, assign 0 ) # Let&#39;s see what we created table(cces$swing_voter) ## ## 0 1 ## 93545 24681 mean(cces$swing_voter, na.rm = TRUE) # Proportion of swing voters ## [1] 0.2087612 The abs() function calculates absolute value - the distance from center regardless of direction. So abs(0.7 - 0.5) = 0.2 and abs(0.3 - 0.5) = 0.2. Both are equally far from center. Now let’s do the same thing using dplyr’s case_when(): # dplyr approach cces &lt;- cces %&gt;% mutate(swing_voter_2 = case_when( abs(ideo - 0.5) &lt; 0.2 &amp; abs(pid7 - 0.5) &lt; 0.2 ~ 1, TRUE ~ 0 # This catches all other cases )) # Verify they&#39;re identical table(BaseR = cces$swing_voter, dplyr = cces$swing_voter_2) ## dplyr ## BaseR 0 1 ## 0 93545 0 ## 1 0 24681 The power of mutate() really shines when creating multiple related variables at once. Instead of writing three separate assignment statements, we can create a whole set of age-based indicators in one clean pipeline: # Using mutate with multiple new variables at once cces &lt;- cces %&gt;% mutate( young_voter = if_else(age &lt; 30, 1, 0), senior_voter = if_else(age &gt;= 65, 1, 0), midlife_voter = if_else(age &gt;= 40 &amp; age &lt; 65, 1, 0) ) # Check our work - these should be mutually exclusive except for young/midlife table(Young = cces$young_voter, Senior = cces$senior_voter) ## Senior ## Young 0 1 ## 0 76986 24249 ## 1 23365 0 table(Young = cces$young_voter, Midlife = cces$midlife_voter) ## Midlife ## Young 0 1 ## 0 46646 54589 ## 1 23365 0 # We can also use OR conditions - let&#39;s identify priority outreach targets cces &lt;- cces %&gt;% mutate( # Target young OR senior voters (two key demographics) priority_demo = if_else(age &lt; 30 | age &gt;= 65, 1, 0) ) # What proportion of voters are in our priority demographics? mean(cces$priority_demo, na.rm = TRUE) ## [1] 0.3821348 Note the difference: &amp; (AND): Both conditions must be true | (OR): At least one condition must be true This becomes powerful when combining multiple characteristics to identify specific voter segments for analysis or targeting. 7.8.3 Using the fastDummies Package For creating multiple dummy variables at once: library(fastDummies) # Create dummy variables for all regions cces_with_dummies &lt;- dummy_cols(cces, select_columns = &quot;region&quot;, remove_first_dummy = TRUE) # Alternative: Manual dplyr approach for specific dummies cces &lt;- cces %&gt;% mutate( region_northeast = if_else(region == &quot;Northeast&quot;, 1, 0), region_midwest = if_else(region == &quot;Midwest&quot;, 1, 0), region_south = if_else(region == &quot;South&quot;, 1, 0), region_west = if_else(region == &quot;West&quot;, 1, 0) ) 7.9 Conditional Variable Creation For complex conditions, case_when() is the most powerful tool. 7.9.1 Creating Voter Typologies # Base R approach using nested ifelse (gets messy quickly!) cces$voter_type_base &lt;- ifelse( cces$aware &gt; 0.8 &amp; cces$interest &gt; 0.8, &quot;Highly Engaged&quot;, ifelse(cces$aware &gt; 0.5 &amp; cces$interest &gt; 0.5, &quot;Moderately Engaged&quot;, ifelse(cces$aware &lt; 0.3 &amp; cces$interest &lt; 0.3, &quot;Disengaged&quot;, ifelse(cces$aware &gt; 0.7 &amp; cces$interest &lt; 0.3, &quot;Informed but Apathetic&quot;, ifelse(cces$aware &lt; 0.3 &amp; cces$interest &gt; 0.7, &quot;Interested but Uninformed&quot;, &quot;Mixed&quot;))))) # dplyr approach - much cleaner! cces &lt;- cces %&gt;% mutate(voter_type = case_when( aware &gt; 0.8 &amp; interest &gt; 0.8 ~ &quot;Highly Engaged&quot;, aware &gt; 0.5 &amp; interest &gt; 0.5 ~ &quot;Moderately Engaged&quot;, aware &lt; 0.3 &amp; interest &lt; 0.3 ~ &quot;Disengaged&quot;, aware &gt; 0.7 &amp; interest &lt; 0.3 ~ &quot;Informed but Apathetic&quot;, aware &lt; 0.3 &amp; interest &gt; 0.7 ~ &quot;Interested but Uninformed&quot;, TRUE ~ &quot;Mixed&quot; )) table(cces$voter_type) ## ## Disengaged Highly Engaged Informed but Apathetic ## 7923 39116 629 ## Interested but Uninformed Mixed Moderately Engaged ## 1129 38058 37745 7.10 Best Practices and Common Pitfalls 7.10.1 Document Your Transformations # Well-documented transformation cces &lt;- cces %&gt;% mutate( # Political knowledge scale: combines awareness (0-1) and interest (0-1) # Rescaled to 0-100 for easier interpretation knowledge_scale = ((aware + interest) / 2) * 100, # Binary indicator for high political knowledge (top 25%) high_knowledge = knowledge_scale &gt; quantile(knowledge_scale, 0.75, na.rm = TRUE) ) 7.10.2 Chain Multiple Transformations dplyr makes it easy to create multiple related variables: # Create a series of related variables in one pipeline cces &lt;- cces %&gt;% mutate( # First create age categories age_cat = cut(age, breaks = c(18, 30, 45, 65, 100), labels = c(&quot;Young&quot;, &quot;Middle&quot;, &quot;Older&quot;, &quot;Senior&quot;)), # Then create dummies based on those categories is_young = if_else(age_cat == &quot;Young&quot;, 1, 0), is_senior = if_else(age_cat == &quot;Senior&quot;, 1, 0), # And create interaction variables young_democrat = is_young * (pid7 &gt; 0.6), senior_republican = is_senior * (pid7 &lt; 0.4) ) 7.10.3 Handle Missing Values Explicitly # Base R approach sum(is.na(cces$aware)) ## [1] 0 # Problem: NAs propagate cces$problem_var &lt;- cces$aware * 2 # NAs remain NAs # Solution 1: Exclude NAs in creation cces$high_aware_base &lt;- ifelse(!is.na(cces$aware) &amp; cces$aware &gt; 0.7, 1, ifelse(!is.na(cces$aware), 0, NA)) # Solution 2: dplyr with explicit NA handling cces &lt;- cces %&gt;% mutate( high_aware_clean = case_when( is.na(aware) ~ NA_real_, # Preserve NAs aware &gt; 0.7 ~ 1, TRUE ~ 0 ), # Or use if_else with missing parameter high_aware_alt = if_else(aware &gt; 0.7, 1, 0, missing = NA_real_) ) 7.11 Summary Creating and transforming variables is an essential data preparation skill. Key takeaways: Both approaches work - Base R is direct, dplyr is often cleaner for complex operations dplyr excels at - Multiple transformations, complex conditions, readable code Use case_when() - For anything more complex than a simple if-else Chain operations - dplyr’s %&gt;% makes sequential transformations clear Be explicit with NAs - Both approaches require careful handling of missing values The dplyr approach generally produces more readable code, especially for complex transformations. However, knowing both methods makes you a more versatile data analyst. 7.12 Study Questions 7.12.1 Understanding Variable Creation Basics What operator do you use to create a new variable in base R? What function do you use in dplyr? If you have a variable income in thousands and want to convert it to actual dollars, write the code using both base R and dplyr. 7.12.2 Simple Recoding Explain what this code does in plain English: cces$vote_label &lt;- ifelse(cces$vote == 1, &quot;Democrat&quot;, &quot;Republican&quot;) What are the three arguments in the ifelse() function? What does each one do? If you have a party ID scale from 0 to 1, how would you reverse it so that 0 becomes 1 and 1 becomes 0? 7.12.3 Using cut() for Categories What does cut() do to a continuous variable like age? In cut(age, breaks = c(18, 35, 65, 100)), what are the three categories that will be created? 7.12.4 Creating Dummy Variables What values does a dummy variable contain? Write the code to create a dummy variable is_adult that equals 1 if age is 18 or older, and 0 otherwise. What does the &amp; operator do when creating dummy variables? What about |? 7.12.5 Understanding dplyr Basics What does %&gt;% do in dplyr code? What’s the advantage of using mutate() to create multiple variables at once? Convert this base R code to dplyr: r cces$age_group &lt;- ifelse(cces$age &gt;= 65, \"Senior\", \"Non-senior\") 7.12.6 Working with Missing Values If a variable contains some NA values and you multiply it by 100, what happens to the NA values? In ifelse(), what happens if the condition (cces$age &gt; 65) is NA? "],["measures-of-central-tendency.html", "8 Measures of Central Tendency 8.1 Learning Objectives 8.2 What Is This For? 8.3 The Mean: Adding It All Up 8.4 The Median: Finding the Middle 8.5 The Mode: Most Common Value 8.6 Comparing the Three Measures 8.7 Practical Guidelines 8.8 Things to Look Out For 8.9 Study Questions for Central Tendency", " 8 Measures of Central Tendency 8.1 Learning Objectives Understand what the mean is, how to calculate it, and when to use it Learn about the median as a measure of central tendency and its relationship to percentiles Discover how the mode captures the most common value in your data Compare these three measures and understand when each is most appropriate Practice calculating central tendency measures in R with real political data 8.2 What Is This For? When political scientists analyze data, one of the first questions they ask is: “What’s typical?” Whether we’re studying voter ages, campaign contributions, or democracy scores across countries, we need ways to summarize large amounts of information into single, meaningful numbers. Imagine you’re a campaign manager trying to understand your district. You have data on thousands of voters’ ages. You can’t look at each individual age, so you need a summary. Should you use the average age? The middle age? The most common age? Each of these measures tells a different story, and choosing the right one can mean the difference between understanding your constituency and missing the mark entirely. These measures of central tendency – mean, median, and mode – are fundamental tools in data analysis. They help us: Summarize complex datasets with simple numbers Compare different groups (Are Democratic voters older than Republican voters?) Detect outliers and data quality issues Make informed decisions based on what’s “typical” Understanding when to use each measure is crucial. As we’ll see, the presence of billionaire donors can make average campaign contributions misleading, while the median might better represent a typical donor. Let’s explore how these measures work and when to use each one. 8.3 The Mean: Adding It All Up 8.3.1 Understanding the Mean The mean (often called the average) is calculated by adding up all values and dividing by the number of observations. We denote it with \\(\\bar{x}\\) (pronounced “x-bar”). Mathematically: \\[\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\] Where: \\(x_i\\) represents each individual observation \\(n\\) is the total number of observations \\(\\sum\\) (sigma) means “sum up” 8.3.2 When to Use the Mean The mean works best for: Interval or ratio data (like age, income, vote percentages) Symmetric distributions without extreme outliers When you need to account for every value in your dataset 8.3.3 The Mean’s Achilles’ Heel: Outliers The mean has a critical weakness – it’s heavily influenced by extreme values. Consider this example: # Typical campaign contributions typical_donors &lt;- c(25, 50, 70, 100, 100, 99, 150, 200, 250) mean(typical_donors) ## [1] 116 # Now add one wealthy donor with_wealthy &lt;- c(typical_donors, 10000) mean(with_wealthy) ## [1] 1104.4 One $10,000 donation changed our “typical” donation from about $116 to over $1,100! This is why news reports about “average” wealth or income can be misleading. 8.3.4 The Trimmed Mean: A Compromise To reduce outlier influence, we can use a trimmed mean, which removes a percentage of extreme values: # Regular mean mean(with_wealthy) ## [1] 1104.4 # 10% trimmed mean (removes top and bottom 10%) mean(with_wealthy, trim = 0.1) ## [1] 127.375 8.4 The Median: Finding the Middle 8.4.1 Understanding the Median The median is the middle value when data is ordered from smallest to largest. Here’s the key insight: the median is actually the 50th percentile of your data. But what’s a percentile? A percentile tells you what percentage of observations fall below a certain value. If you score in the 90th percentile on a test, 90% of students scored the same or lower than you. The median is special because exactly 50% of observations fall above it and 50% fall below it. For odd numbers of observations: the middle value For even numbers: the average of the two middle values # Odd number of values odd_data &lt;- c(1, 3, 5, 7, 9) median(odd_data) # Middle value is 5 ## [1] 5 # Even number of values even_data &lt;- c(1, 3, 5, 7, 9, 17) median(even_data) # Average of 5 and 7 ## [1] 6 8.4.2 The Median and Other Percentiles Understanding the median as the 50th percentile helps us see it’s part of a larger family of position measures. In R, we can find any percentile using the quantile() function: library(fivethirtyeight) data(&quot;congress_age&quot;) # The median is the 50th percentile (0.50 quantile) median(congress_age$age) ## [1] 53 quantile(congress_age$age, probs = 0.50) ## 50% ## 53 # We can find other percentiles too quantile(congress_age$age, probs = c(0.25, 0.50, 0.75)) ## 25% 50% 75% ## 45.40 53.00 60.55 This tells us: 25% of Congress members are younger than about 45 (25th percentile) 50% are younger than about 53 (median) 75% are younger than about 61 (75th percentile) 8.4.3 The Interquartile Range (IQR) The Interquartile Range captures the middle 50% of your data: IQR = 75th percentile - 25th percentile IQR(congress_age$age) ## [1] 15.15 The IQR helps us understand spread and identify outliers. Values more than 1.5 × IQR beyond the quartiles are often considered outliers. 8.4.4 When to Use the Median The median excels when: Data is skewed (has a long tail on one side) There are outliers you can’t remove You have ordinal data (like survey responses: strongly disagree to strongly agree) You want a value that’s actually in your dataset (for odd n) 8.4.5 The Median’s Robustness Unlike the mean, the median doesn’t use exact values – it only cares about their order and finds the middle of that ordering: # Original data salaries1 &lt;- c(30000, 35000, 40000, 45000, 50000) median(salaries1) ## [1] 40000 # Replace highest salary with CEO salary salaries2 &lt;- c(30000, 35000, 40000, 45000, 5000000) median(salaries2) # Still the same! ## [1] 40000 8.5 The Mode: Most Common Value 8.5.1 Understanding the Mode The mode is simply the most frequently occurring value. Unlike mean and median, it can be used with any type of data, including nominal data. Remember: the mode is the value itself, not how often it appears. 8.5.2 Finding the Mode in R R doesn’t have a built-in mode function, but we can find it using frequency tables: # Example: Party affiliations party_data &lt;- c(&quot;Dem&quot;, &quot;Rep&quot;, &quot;Dem&quot;, &quot;Dem&quot;, &quot;Ind&quot;, &quot;Rep&quot;, &quot;Dem&quot;, &quot;Rep&quot;, &quot;Dem&quot;) # Create frequency table party_table &lt;- table(party_data) party_table ## party_data ## Dem Ind Rep ## 5 1 3 # Use sort() to order from most to least common sort(party_table, decreasing = TRUE) ## party_data ## Dem Rep Ind ## 5 3 1 # The mode is &quot;Dem&quot; with 5 occurrences 8.5.3 Finding the Mode with Numeric Data The same approach works with numbers: # Example: Test scores test_scores &lt;- c(85, 90, 85, 75, 85, 90, 95, 90, 85, 80) # Create frequency table score_table &lt;- table(test_scores) score_table ## test_scores ## 75 80 85 90 95 ## 1 1 4 3 1 # Sort to see which score is most common sort(score_table, decreasing = TRUE) ## test_scores ## 85 90 75 80 95 ## 4 3 1 1 1 # We can see that 85 appears most often (4 times) Using sort() with decreasing = TRUE puts the most frequent values at the top, making it easy to identify the mode at a glance. 8.5.4 Bimodal Distributions When data has two modes (two peaks), it often indicates two distinct groups or polarization. This is especially relevant in political science. This histogram shows the change in polarization in the public between 1993 and 2014 You can learn more about this data at Pew Research. In this visualization, we can see how public opinion has shifted from having one central peak (unimodal) to having two distinct peaks (bimodal) – one on the left and one on the right. This bimodal pattern reveals increasing political polarization, with fewer people in the middle and more people at the ideological extremes. Bimodal distributions can indicate: Polarization around two extremes Two distinct subgroups in your data When you see a bimodal distribution, it’s worth investigating what might be causing the two peaks. 8.6 Comparing the Three Measures 8.6.1 How Skew Affects Central Tendency The relationship between mean, median, and mode tells us about the shape of our data: Symmetric distribution: Mean ≈ Median ≈ Mode Right-skewed (tail to the right): Mode &lt; Median &lt; Mean Left-skewed (tail to the left): Mean &lt; Median &lt; Mode This image shows three distributions and how the mean, median, and mode relate in each case 8.6.2 Detecting Skew Without Drawing the Distribution You can identify skew just by comparing the mean and median: If mean &gt; median: The data is likely right-skewed (positive skew) Example: Income data where most people earn modest amounts but a few earn millions If mean &lt; median: The data is likely left-skewed (negative skew) Example: Age at retirement where most retire around 65 but some retire very early If mean ≈ median: The data is likely symmetric Example: Heights in a population, test scores on a well-designed exam The mean gets “pulled” toward the tail of the distribution because it accounts for every value, while the median stays centered on the middle observation. This is why comparing these two measures is such a powerful diagnostic tool – you can understand your data’s shape without even looking at a graph! 8.6.3 Real Example: Congressional Ages Let’s analyze the ages of U.S. Congress members to see how these measures compare: # Calculate mean and median mean(congress_age$age) ## [1] 53.31373 median(congress_age$age) ## [1] 53 # For mode, we&#39;ll use whole years congress_age$age_years &lt;- floor(congress_age$age) age_table &lt;- table(congress_age$age_years) # Sort to find the most common age sort(age_table, decreasing = TRUE) ## ## 51 54 49 52 56 53 55 47 50 57 48 58 45 46 59 60 43 44 62 61 42 41 64 40 ## 649 649 647 646 646 643 637 626 624 613 612 589 572 558 547 546 533 519 485 480 454 448 436 416 ## 63 39 66 38 65 37 68 67 36 69 70 35 71 34 72 73 32 74 33 75 76 31 77 30 ## 396 361 357 343 327 275 269 249 245 218 202 189 169 160 155 122 115 115 113 86 85 65 62 60 ## 78 79 80 28 81 82 29 83 27 84 85 26 87 86 89 25 88 90 91 92 93 94 95 96 ## 57 39 38 27 27 26 25 14 12 10 10 8 7 5 4 2 2 2 1 1 1 1 1 1 ## 98 ## 1 # Make a simple histogram to see the shape hist(congress_age$age, main = &quot;Distribution of Congressional Ages&quot;, xlab = &quot;Age&quot;) The mean (around 53.3) and median (around 53.0) are very close, which tells us the age distribution is fairly symmetric. Looking at our sorted table, we can see that ages 51 and 54 are the most common (the modes). 8.6.4 Using summary() for a Quick Overview R’s summary() function provides key statistics including several percentiles: summary(congress_age$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 25.00 45.40 53.00 53.31 60.55 98.10 This shows: Min and Max (0th and 100th percentiles) 1st Quartile (25th percentile) Median (50th percentile) 3rd Quartile (75th percentile) Mean 8.7 Practical Guidelines 8.7.1 Choosing the Right Measure Use the mean when: Data is roughly symmetric You need to account for total values (e.g., total budget divided by recipients) Working with interval/ratio data without outliers Use the median when: Data is skewed There are outliers you can’t remove You want a “typical” value that isn’t affected by extremes Working with ordinal data Use the mode when: Working with categorical data You need the most common category Looking for peaks in continuous data 8.7.2 Reporting Best Practices Always consider reporting multiple measures: For income data: Report both median (typical) and mean (affected by high earners) For nominal data: Report mode and its frequency For any data: Consider showing the distribution graphically (we’ll cover this more next week) 8.8 Things to Look Out For 8.8.1 Data Entry Errors Can Change Everything The mean is particularly vulnerable to data entry mistakes. Consider the famous case of spinach: for decades, people believed spinach had 10 times more iron than it actually does. Why? A misplaced decimal point in the 1870s turned 3.5 mg of iron per 100g into 35 mg. This single data entry error influenced nutritional recommendations for generations, even inspiring the creation of Popeye! In your own work, watch for: Numbers that seem too large or small (is that salary really $50,000,000 or should it be $50,000?) Decimal point errors (3.5 becoming 35 or 0.35) Extra zeros (100 becoming 1000) 8.8.2 When Excel Errors Shape Government Policy In 2013, economists discovered a major error in an influential study about government debt. The researchers had accidentally excluded several countries from their Excel calculation of average GDP growth. This wasn’t just an academic mistake – governments around the world had used this study to justify austerity policies during the financial crisis. The lesson? Even professional researchers make errors, and these errors matter. When calculating means in R: Always check your data first with head() and summary() Look for values that don’t make sense Document your code so others can verify your work 8.8.3 Special Values in Older Datasets Many datasets use special numbers to indicate missing data: -99 or 999 for missing values 0 for “not applicable” -88 for “refused to answer” For example, the Polity democracy scores typically range from -10 to +10. But they use -88 for periods of government transition. If you don’t remove these special values before calculating the mean, your results will be meaningless: # Example: Democracy scores with special values democracy_scores &lt;- c(7, 8, 9, -88, 6, 7, -88, 8) # Wrong: Including the -88 values mean(democracy_scores) ## [1] -16.375 # Right: Remove special values first real_scores &lt;- democracy_scores[democracy_scores != -88] mean(real_scores) ## [1] 7.5 8.8.4 The Takeaway Before calculating any measure of central tendency: Look at your data – Use summary() to spot unusual values Check the documentation – Does the dataset use special codes? Question extreme values – Is that billionaire real or a data error? Keep your original data – Never overwrite it, so you can always go back Remember: Good data analysis isn’t just about knowing the formulas – it’s about being a detective who questions whether the numbers make sense. 8.9 Study Questions for Central Tendency 8.9.1 Understanding the Concepts What is the mean, and what symbol do we use to represent it? What does the formula \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\) tell us in plain English? Explain why the mean of a binary (0/1) variable gives us the proportion of 1s in the data. What is the median? How do you find it when you have an even number of observations? What does it mean to say the median is the “50th percentile”? Define the mode. Can you have more than one mode in a dataset? 8.9.2 Understanding Position What is a percentile? If someone scores in the 75th percentile, what does that mean? What is the Interquartile Range (IQR)? How do you calculate it? Using the 1.5 × IQR rule, when is a value considered an outlier? 8.9.3 Comparing Measures In a perfectly symmetric distribution, what is the relationship between the mean, median, and mode? If a distribution is right-skewed (long tail to the right), how do the mean and median compare? Why? Why is the median considered more “robust” than the mean? Give an example. When would the mode be the only appropriate measure of central tendency? What is a trimmed mean? When would you use it instead of a regular mean? 8.9.4 Practical Applications A news report says the “average American household income is $106,000.” Another report says the “typical American household income is $71,000.” Which measure is each report likely using, and why might they differ? A dataset of city populations includes New York (8 million), Los Angeles (4 million), and several cities around 100,000. Which measure of central tendency best represents a “typical” city? Why? 8.9.5 Working with R What does na.rm = TRUE do in functions like mean() and median()? How can you find the mode using R? What’s the difference between median() and quantile(x, probs = 0.50)? If you calculate mean(c(1, 2, 3, NA)) without any additional arguments, what will R return? Why? 8.9.6 Interpretation Congressional ages have a mean of 53.3 and median of 53.0. What does this tell you about the age distribution? What is a bimodal distribution? What might it indicate in political data? If a dataset has mean = 85 and median = 65, what can you conclude about its skew? 8.9.7 Critical Thinking A politician claims their policies benefit the “average American.” Why should you ask whether they mean the mean or median American? Why might the Popeye-spinach story be a cautionary tale about the mean? Calculate BY HAND the mean, median, and mode of this dataset (show your work): Data: 7, 9, 7, 3, 7, 5, 9, 10, 5 Then verify your answers using R. "],["measures-of-dispersion.html", "9 Measures of Dispersion 9.1 Learning Objectives 9.2 What Is This For? 9.3 Simple Measures of Spread 9.4 Understanding Deviations 9.5 Variance: The Foundation of Statistical Spread 9.6 Standard Deviation: Variance Made Practical 9.7 The Empirical Rule (68-95-99.7 Rule) 9.8 Calculating Dispersion in R 9.9 Which Measure Should You Use? 9.10 Common Pitfalls 9.11 Summary 9.12 Study Questions", " 9 Measures of Dispersion 9.1 Learning Objectives Understand why measures of dispersion are essential for data analysis Calculate and interpret the range and interquartile range (IQR) Understand the concept of deviation from the mean Calculate and interpret variance and standard deviation Apply the Empirical Rule to normally distributed data Use R to calculate all measures of dispersion 9.2 What Is This For? Imagine two classes where students have the same average grade of 75%. In Class A, everyone scored between 73% and 77%. In Class B, scores ranged from 45% to 95%. While both classes have the same mean, they tell very different stories about student performance. This is why we need measures of dispersion (also called spread or variability). Measures of central tendency (mean, median, mode) tell us about the “typical” value in our data. But they don’t tell us how well that typical value represents the entire dataset. Measures of dispersion answer questions like: How spread out are the data points? How representative is the mean? Are there extreme values we should worry about? How much variation should we expect? Understanding dispersion is crucial for: Risk assessment: Higher dispersion often means higher uncertainty Quality control: Consistent products have low dispersion Statistical inference: Many statistical tests assume certain levels of dispersion Data description: Dispersion helps paint a complete picture of your data 9.3 Simple Measures of Spread 9.3.1 Range The range is the simplest measure of dispersion: Range = Maximum value - Minimum value While easy to calculate, the range has significant limitations: It only uses two data points (ignoring everything in between) It’s extremely sensitive to outliers It tells us nothing about how data is distributed within that range For example, these two datasets have the same range (10): Dataset A: 0, 5, 5, 5, 5, 10 Dataset B: 0, 1, 2, 8, 9, 10 But Dataset B is clearly more spread out than Dataset A. 9.3.2 Interquartile Range (IQR) The interquartile range is more robust because it focuses on the middle 50% of the data: IQR = Q3 (75th percentile) - Q1 (25th percentile) The IQR has several advantages: Not affected by extreme outliers Gives a sense of the “typical” spread Used to identify outliers (values beyond Q1 - 1.5×IQR or Q3 + 1.5×IQR) 9.4 Understanding Deviations To truly measure spread, we need to consider how far each data point is from the center. A deviation is the distance between an individual observation and the mean: \\[\\text{Deviation} = \\bar{x}-x_i\\] Where: \\(x_i\\) is an individual observation \\(\\bar{x}\\) is the mean However, if we simply average all deviations, we get zero (positive and negative deviations cancel out). This leads us to more sophisticated measures. 9.5 Variance: The Foundation of Statistical Spread Variance solves the cancellation problem by squaring each deviation before averaging: \\[s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\] Breaking this down: 1. Calculate each deviation: (xᵢ - x̄) 2. Square each deviation: (xᵢ - x̄)² 3. Sum all squared deviations 4. Divide by (n-1) Why divide by (n-1)? This is called Bessel’s correction. When we use the sample mean (x̄) instead of the true population mean, we slightly underestimate the true variance. Dividing by (n-1) corrects this bias. 9.5.1 Understanding Variance Units Variance has squared units, which can be confusing: If measuring height in inches, variance is in inches² If measuring temperature in °C, variance is in (°C)² If measuring test scores in points, variance is in points² These squared units make variance difficult to interpret directly, which leads us to… 9.6 Standard Deviation: Variance Made Practical The standard deviation is simply the square root of variance: \\[s = \\sqrt{s^2} = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\\] Standard deviation brings us back to the original units, making it much more interpretable. It represents the “typical” distance from the mean. 9.6.1 Interpreting Standard Deviation Small standard deviation: Data points cluster closely around the mean Large standard deviation: Data points are spread far from the mean But what counts as “small” or “large”? Context matters: Human height (sd ≈ 3 inches): relatively consistent Stock prices (sd can be 20%+ of mean): highly variable Manufacturing tolerances (sd &lt; 0.001 inches): extremely precise 9.6.2 Standard Deviation and Outliers Both variance and standard deviation are heavily influenced by outliers because: Outliers create large deviations Squaring magnifies large values These inflated values dominate the sum This sensitivity can be a feature (detecting unusual variation) or a bug (overstating typical spread). 9.7 The Empirical Rule (68-95-99.7 Rule) For approximately normal (bell-shaped) distributions, the Empirical Rule provides powerful insights: About 68% of data falls within 1 standard deviation of the mean (x̄ ± 1s) About 95% of data falls within 2 standard deviations (x̄ ± 2s) About 99.7% of data falls within 3 standard deviations (x̄ ± 3s) 9.7.1 When to Use the Empirical Rule Use it when: Your histogram looks roughly bell-shaped Data is symmetric around the mean You need quick estimates of data spread Don’t use it when: Data is clearly skewed There are multiple peaks Data has hard boundaries (like percentages that can’t exceed 100%) 9.7.2 Video: Understanding the Empirical Rule For a visual explanation of measures of dispersion and the Empirical Rule, watch this video. 9.8 Calculating Dispersion in R Let’s practice with real political science data. We’ll use the 2020 American National Election Study (ANES) data, focusing on how Americans rated Joe Biden on a 0-100 “feeling thermometer” scale. First, load the data: # Load data from my computer, since the data is in the same folder anes2020 &lt;- read.csv(&quot;anes2020.csv&quot;) # Examine structure str(anes2020) ## &#39;data.frame&#39;: 8280 obs. of 12 variables: ## $ rid : int 1 2 3 4 5 6 7 8 9 10 ... ## $ age : int 46 37 40 41 72 71 37 45 70 43 ... ## $ race : int 3 4 1 4 5 1 1 1 1 3 ... ## $ latino : int 1 2 2 2 2 2 2 2 2 1 ... ## $ vote12 : int 1 1 1 1 1 1 1 2 1 2 ... ## $ vote16 : int 1 1 1 1 1 2 1 2 NA NA ... ## $ media : int 2 3 1 3 2 2 3 4 3 2 ... ## $ partyid : int 2 5 3 2 3 3 2 2 3 1 ... ## $ income : int 21 13 17 7 22 3 4 3 10 11 ... ## $ obama.th: int 0 50 90 85 10 60 15 50 60 100 ... ## $ trump.th: int 100 0 0 15 85 0 75 100 0 0 ... ## $ biden.th: int 0 0 65 70 15 85 50 50 85 85 ... 9.8.1 Calculating Basic Measures # Summary statistics including range and quartiles summary(anes2020$biden.th) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 15.00 55.00 49.24 85.00 100.00 220 # Range (manual calculation) range_biden &lt;- max(anes2020$biden.th, na.rm = TRUE) - min(anes2020$biden.th, na.rm = TRUE) range_biden #print it out ## [1] 100 # Interquartile range iqr_biden&lt;-IQR(anes2020$biden.th, na.rm = TRUE) iqr_biden # print it out ## [1] 70 9.8.2 Variance and Standard Deviation Now we can calculate the variance. # Variance var_biden &lt;- var(anes2020$biden.th, na.rm = TRUE) var_biden ## [1] 1190.024 # Standard deviation sd_biden &lt;- sd(anes2020$biden.th, na.rm = TRUE) sd_biden ## [1] 34.49672 # Verify: sd is square root of variance sqrt(var_biden) ## [1] 34.49672 9.8.3 Comparing Measures of Spread Different measures tell different stories. Here we are using the cat() function to combine the text and the numbers together. Note that the \"\\n\" syntax is used to indicate a “return” or a line break. # Create a comparison cat(&quot;Range:&quot;, range_biden, &quot;\\n&quot;) ## Range: 100 cat(&quot;IQR:&quot;, iqr_biden, &quot;\\n&quot;) ## IQR: 70 cat(&quot;Standard Deviation:&quot;, sd_biden, &quot;\\n&quot;) ## Standard Deviation: 34.49672 9.9 Which Measure Should You Use? Choose your measure of dispersion based on your data and goals: 9.9.1 Use Range when: You need a quick, rough estimate Describing the full extent of data The minimum and maximum have special meaning 9.9.2 Use IQR when: Your data has outliers You’re using median as your measure of center You want a robust measure unaffected by extremes 9.9.3 Use Standard Deviation when: Your data is roughly normal You’re using mean as your measure of center You need to make statistical inferences You want to apply the Empirical Rule 9.10 Common Pitfalls 9.10.1 1. Forgetting na.rm = TRUE # This will return NA if any values are missing sd(anes2020$biden.th) # Returns NA ## [1] NA # This removes NA values before calculating sd(anes2020$biden.th, na.rm = TRUE) # Returns the standard deviation ## [1] 34.49672 9.10.2 2. Misinterpreting Large Standard Deviations A “large” standard deviation depends on context. Always compare it to: The mean (coefficient of variation = sd/mean) The possible range of values Domain knowledge about typical variation 9.10.3 3. Using Standard Deviation with Skewed Data The Empirical Rule assumes normality. With skewed data: More or fewer observations may fall within ±1 sd Consider using IQR instead Transform data or use different methods (covered much later) 9.11 Summary Measures of dispersion complete our statistical picture: Central tendency tells us the typical value Dispersion tells us how typical that typical value is Key concepts to remember: Range and IQR are simple but limited Variance measures average squared deviation Standard deviation is the square root of variance (back to original units) The Empirical Rule applies only to normal distributions Different measures suit different situations Together with measures of central tendency, these tools allow you to summarize any dataset effectively and identify when simple summaries might be misleading. 9.12 Study Questions 9.12.1 Conceptual Understanding Why do we need measures of dispersion in addition to measures of central tendency? Explain why the range might be misleading as a measure of spread. Give an example. What is a deviation? Why can’t we just average all deviations to measure spread? Why do we square deviations when calculating variance? What’s the relationship between variance and standard deviation? Explain why we divide by (n-1) rather than n when calculating sample variance. 9.12.2 Interpretation If a dataset is bell shaped and has a mean of 50 and a standard deviation of 10, what does this tell you about the data? Dataset A has a standard deviation of 5, and Dataset B has a standard deviation of 15. Which dataset has more consistent values? How do you know? When would the IQR be a better measure of spread than the standard deviation? What conditions must be met to apply the Empirical Rule? Give an example of when you shouldn’t use it. 9.12.3 Calculation Practice Given the values: 2, 4, 6, 8, 10 Calculate the range Calculate the variance (show your work) Calculate the standard deviation If a normally distributed dataset has a mean of 100 and standard deviation of 15: What range of values contains about 68% of the data? What range contains about 95% of the data? Would a value of 140 be considered unusual? Why? 9.12.4 R Programming What’s wrong with this code, and how would you fix it? data &lt;- c(1, 2, 3, NA, 5) sd(data) How would you calculate the coefficient of variation (sd/mean) for a variable in R? Write R code to determine what percentage of observations in a dataset fall within 2 standard deviations of the mean. 9.12.5 Critical Thinking A company reports that the average salary is $75,000 with a standard deviation of $50,000. What does this large standard deviation suggest about the salary distribution? How would outliers affect the range, IQR, and standard deviation differently? Which is most robust to outliers? "],["using-summarise-and-group_by-for-descriptive-statistics.html", "10 Using summarise() and group_by() for Descriptive Statistics 10.1 Learning Objectives 10.2 Getting Started with the Data 10.3 Basic Statistics with summarise() 10.4 Common Statistical Functions 10.5 Group-wise Statistics with group_by() 10.6 Practical Examples 10.7 Important Notes 10.8 Study Questions for Using summarise() and group_by()", " 10 Using summarise() and group_by() for Descriptive Statistics knitr::opts_chunk$set(echo = TRUE) 10.1 Learning Objectives Learn how to use summarise() to calculate descriptive statistics Master calculating means, medians, standard deviations, and variances Learn how to calculate multiple statistics at once Understand how to use group_by() to calculate statistics by groups Practice combining group_by() and summarise() for comparative analysis 10.2 Getting Started with the Data Today we’ll work with the Anscombe dataset, which provides information about education expenditure for each US state in 1970. It contains four variables: education: per capita education expenditure (in dollars) income: per capita income (in dollars) young: proportion under 18 years old (per 1000 residents) urban: proportion living in urban areas (per 1000 residents) library(tidyverse) library(car) # for the Anscombe dataset school_spend &lt;- Anscombe 10.3 Basic Statistics with summarise() The summarise() function (also spelled summarize()) is a powerful tool for calculating statistics. It creates a new data frame containing the summary statistics you request. 10.3.1 Calculating a Single Statistic Let’s start by calculating the mean income across all states: school_spend %&gt;% summarise(mean_income = mean(income)) ## mean_income ## 1 3225.294 Notice that: We use the pipe operator %&gt;% to pass the data to summarise() Inside summarise(), we create a new variable name (mean_income) We specify what to calculate using mean(income) 10.3.2 Multiple Statistics for One Variable We can calculate several statistics for the same variable: school_spend %&gt;% summarise( mean_education = mean(education), median_education = median(education), sd_education = sd(education), var_education = var(education) ) ## mean_education median_education sd_education var_education ## 1 196.3137 192 46.45449 2158.02 10.3.3 Multiple Variables, Multiple Statistics We can also calculate statistics for different variables in one command: school_spend %&gt;% summarise( mean_income = mean(income), mean_education = mean(education), median_urban = median(urban), sd_young = sd(young) ) ## mean_income mean_education median_urban sd_young ## 1 3225.294 196.3137 664 23.95998 10.4 Common Statistical Functions Here are the most common functions you’ll use with summarise(): mean(): average value median(): middle value sd(): standard deviation var(): variance min(): minimum value max(): maximum value n(): count of observations 10.4.1 Example: Complete Summary school_spend %&gt;% summarise( n_states = n(), min_education = min(education), max_education = max(education), mean_education = mean(education), median_education = median(education), sd_education = sd(education) ) ## n_states min_education max_education mean_education median_education sd_education ## 1 51 112 372 196.3137 192 46.45449 10.5 Group-wise Statistics with group_by() The real power comes when we combine summarise() with group_by(). This allows us to calculate statistics separately for different groups in our data. 10.5.1 Creating Groups for Analysis First, let’s create a categorical variable that groups states by their education spending level: school_spend &lt;- school_spend %&gt;% mutate( education_level = case_when( education &lt; 150 ~ &quot;Low&quot;, education &gt;= 150 &amp; education &lt; 200 ~ &quot;Medium&quot;, education &gt;= 200 ~ &quot;High&quot; ) ) 10.5.2 Calculating Statistics by Group Now we can calculate statistics for each spending level: school_spend %&gt;% group_by(education_level) %&gt;% # This is where we specify the group -- note the %&gt;% summarise( n_states = n(), mean_income = mean(income), median_income = median(income) ) ## # A tibble: 3 × 4 ## education_level n_states mean_income median_income ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 High 22 3518. 3466 ## 2 Low 9 2548. 2470 ## 3 Medium 20 3208. 3224 Notice how: group_by() comes before summarise() The output has one row for each group n() tells us how many states are in each group 10.5.3 Multiple Grouping Variables We can also create groups based on multiple variables. Let’s categorize states by both education spending and urbanization: school_spend &lt;- school_spend %&gt;% mutate( urban_level = case_when( urban &lt; 500 ~ &quot;Rural&quot;, urban &gt;= 500 &amp; urban &lt; 700 ~ &quot;Mixed&quot;, urban &gt;= 700 ~ &quot;Urban&quot; ) ) school_spend %&gt;% group_by(education_level, urban_level) %&gt;% summarise( n_states = n(), mean_income = mean(income), sd_income = sd(income) ) ## `summarise()` has grouped output by &#39;education_level&#39;. You can override using the `.groups` ## argument. ## # A tibble: 8 × 5 ## # Groups: education_level [3] ## education_level urban_level n_states mean_income sd_income ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 High Mixed 7 3153. 264. ## 2 High Rural 2 3609 759. ## 3 High Urban 13 3700. 441. ## 4 Low Mixed 6 2667 349. ## 5 Low Rural 3 2310. 204. ## 6 Medium Mixed 8 2994. 306. ## 7 Medium Rural 3 2757. 108. ## 8 Medium Urban 9 3550. 403. 10.6 Practical Examples 10.6.1 Example 1: Comparing Young Population by Education Spending Do states that spend more on education have different age demographics? school_spend %&gt;% group_by(education_level) %&gt;% summarise( n_states = n(), mean_young = mean(young), sd_young = sd(young) ) ## # A tibble: 3 × 4 ## education_level n_states mean_young sd_young ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 High 22 366. 30.2 ## 2 Low 9 353. 19.2 ## 3 Medium 20 354. 15.6 10.6.2 Example 2: Income Statistics by Urbanization How does income vary with urbanization level? school_spend %&gt;% group_by(urban_level) %&gt;% summarise( n_states = n(), mean_income = mean(income), median_income = median(income), sd_income = sd(income) ) ## # A tibble: 3 × 5 ## urban_level n_states mean_income median_income sd_income ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mixed 21 2953. 2942 351. ## 2 Rural 8 2802. 2697 623. ## 3 Urban 22 3639. 3682. 423. Look at the output? Do high spending states or low spending states have more young people? 10.6.3 Example 3: Complete Summary Table Let’s create a comprehensive summary combining multiple groupings: school_spend %&gt;% group_by(education_level, urban_level) %&gt;% summarise( n = n(), avg_income = mean(income), avg_urban = mean(urban), avg_young = mean(young), income_variation = sd(income) ) %&gt;% arrange(desc(avg_income)) ## `summarise()` has grouped output by &#39;education_level&#39;. You can override using the `.groups` ## argument. ## # A tibble: 8 × 7 ## # Groups: education_level [3] ## education_level urban_level n avg_income avg_urban avg_young income_variation ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 High Urban 13 3700. 812. 362. 441. ## 2 High Rural 2 3609 403 394. 759. ## 3 Medium Urban 9 3550. 796. 345. 403. ## 4 High Mixed 7 3153. 629 366. 264. ## 5 Medium Mixed 8 2994. 602. 360. 306. ## 6 Medium Rural 3 2757. 446. 364. 108. ## 7 Low Mixed 6 2667 582. 348. 349. ## 8 Low Rural 3 2310. 437 364. 204. Look at the output? urban or rural areas have higher incomes? 10.7 Important Notes 10.7.1 Saving Your Results Remember that summarise() creates a new data frame. If you want to use these results later, you need to save them: income_by_education &lt;- school_spend %&gt;% group_by(education_level) %&gt;% summarise( mean_income = mean(income), sd_income = sd(income) ) # Now we can use income_by_education later print(income_by_education) ## # A tibble: 3 × 3 ## education_level mean_income sd_income ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 High 3518. 474. ## 2 Low 2548. 344. ## 3 Medium 3208. 459. 10.7.2 Handling Missing Values If your data has missing values (NA), you’ll need to add na.rm = TRUE to your statistical functions: # Example with simulated missing data school_spend_with_na &lt;- school_spend school_spend_with_na$income[5] &lt;- NA school_spend_with_na %&gt;% summarise( mean_income_wrong = mean(income), # This returns NA mean_income_correct = mean(income, na.rm = TRUE) # This calculates correctly ) ## mean_income_wrong mean_income_correct ## 1 NA 3218.82 10.8 Study Questions for Using summarise() and group_by() 10.8.1 Understanding summarise() What does the summarise() function do to a data frame? Specifically, how does it transform your data from many rows to fewer rows? If you have a data frame with 51 rows (one for each state) and you use summarise() without group_by(), how many rows will the output have? Why? Inside summarise(), you write something like mean_income = mean(income). What does the part before the = do? What does the part after the = do? What will happen if you run this code: school_spend %&gt;% summarise(mean(income))? How could you modify it to give the output a meaningful column name? 10.8.2 Statistical Functions in summarise() List at least 5 statistical functions you can use inside summarise() and what each calculates. What does the n() function do inside summarise()? Why is it useful? Write a single summarise() statement that calculates: (a) the minimum education spending, (b) the maximum education spending, and (c) the difference between max and min (the range). 10.8.3 Multiple Statistics and Variables Can you calculate statistics for multiple variables in one summarise() call? Give an example. Can you calculate multiple statistics for the same variable in one summarise() call? Give an example. 10.8.4 Understanding group_by() When you use group_by() before summarise(), what does it tell R to do? Does it change the original data or just change how summarise() behaves? In the pipeline data %&gt;% group_by(variable) %&gt;% summarise(...), which function comes first and why does the order matter? You have 51 states in your data. If you use group_by(education_level) where education_level has categories “Low” (15 states), “Medium” (25 states), and “High” (11 states), then use summarise(), how many rows will your final output have? 10.8.5 Practical Applications What’s wrong with this code and how would you fix it? school_spend %&gt;% summarise(mean_income = mean(income)) %&gt;% group_by(education_level) 10.8.6 Multiple Grouping Variables Can you use more than one variable in group_by()? What happens to your output when you do? If you group by education_level (3 categories) and urban_level (3 categories), what’s the maximum number of rows your summary could have? Will it always have that many? 10.8.7 Saving and Using Results What type of object does summarise() return - a vector, a data frame, or something else? 10.8.8 Conceptual Understanding Explain in your own words what this code does: school_spend %&gt;% group_by(urban_level) %&gt;% summarise( states = n(), avg_education = mean(education), education_spread = sd(education) ) ## # A tibble: 3 × 4 ## urban_level states avg_education education_spread ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mixed 21 184. 42.9 ## 2 Rural 8 194. 78.4 ## 3 Urban 22 209. 31.8 What’s the difference between using summary(school_spend$income) and using school_spend %&gt;% summarise(mean_income = mean(income))? When would you use each? "],["data-visualization-basics---single-variables.html", "11 Data Visualization Basics - Single Variables 11.1 Learning Objectives 11.2 What Is This For? 11.3 Principles of Effective Data Visualization 11.4 Getting Started with ggplot2 11.5 Bar Plots for Categorical Data 11.6 Histograms for Continuous Data 11.7 Density Plots for Smooth Distributions 11.8 Putting It All Together: Best Practices 11.9 Summary 11.10 Study Questions", " 11 Data Visualization Basics - Single Variables 11.1 Learning Objectives Understand and apply fundamental principles of effective data visualization Create and customize bar plots for categorical data using ggplot2 Construct histograms and density plots for continuous variables Use visual elements (color, shape, text) strategically to highlight key patterns Build visualizations layer by layer using the grammar of graphics 11.2 What Is This For? Data visualization transforms numbers into pictures that tell stories. A well-crafted visualization can reveal patterns invisible in tables of numbers, communicate complex findings to diverse audiences, and help us make better decisions. Poor visualizations, however, can mislead, confuse, or simply waste everyone’s time. In this session, we’ll focus on visualizing single variables - the foundation of exploratory data analysis. Whether you’re examining voter demographics, survey responses, or policy outcomes, you need to understand your variables individually before exploring relationships between them. Think of it as getting to know each character in your data’s story before watching them interact. We’ll use ggplot2, R’s premier visualization package, which implements the “grammar of graphics” - a systematic approach to building visualizations layer by layer. This approach might feel different from clicking buttons in Excel, but it gives you precise control and reproducibility. Once you understand the grammar, you can create any visualization you can imagine. 11.3 Principles of Effective Data Visualization Before we dive into making graphs, let’s understand how people actually look at and process visual information. This knowledge will guide every design decision we make. 11.3.1 How People Read Graphics Research on eye tracking shows that when viewers look at a figure, their eyes follow a predictable pattern. The following image illustrates typical eye movements: Order of eye movement when viewing a figure Understanding this helps us design more effective visualizations: Color - Bright or contrasting colors draw attention first. This is why we should use color strategically, not decoratively. Shape - Unusual or prominent shapes catch the eye second. Large bars, outliers, or unique patterns stand out. Pattern - Repeated elements or breaks in pattern come next. Our brains are wired to notice when something doesn’t fit. Text - Titles, labels, and annotations are read last, but they’re crucial for understanding. Use these elements strategically. If you want viewers to compare two groups, use contrasting colors. If one category is most important, make it visually prominent while others fade to gray. Here’s an example of using color effectively to highlight a comparison: Good use of colors to highlight key comparison Notice how the strategic use of color draws your eye to the specific comparison the author wants you to make. The other elements fade into the background. Compare this to a less effective use of color: Poor use of colors that creates confusion In this figure, the color for “B team” is also used for the axes and labels, creating visual confusion. The fix is simple - keep non-data elements (axes, labels) in neutral colors like black or gray. 11.3.2 Three Core Principles 1. Declutter Remove unnecessary elements that don’t contribute to understanding. Every line, label, and pixel should have a purpose. Common clutter includes: Grid lines that are too prominent Redundant labels (if your title says “2020 Presidential Election Results”, you don’t need “2020” on every bar) Decorative elements that don’t encode information Default software outputs that you haven’t consciously chosen 2. Emphasize Guide viewers to your main point through visual hierarchy: Use color intensity to highlight key categories (bright red for the important group, light gray for context) Make important comparisons visually obvious (place them next to each other) Use size to show importance (bigger = more important) Position crucial information prominently (top left for Western readers) 3. Clarify Ensure your message is unmistakable: Write descriptive titles that state the main finding, not just describe the data Bad: “Revenue by Quarter” Good: “Revenue Increased 45% in Q4 After Marketing Campaign” Label axes clearly with units Add annotations for important features Choose the right chart type for your data 11.3.3 A Transformation Example Let’s see these principles in action. Here’s a before and after of a real visualization: Ineffective visualization of ticket data This figure has data but no clear message. The viewer has to work hard to extract meaning. Now look at the improved version: Effective visualization with clear message The transformation includes: A clear title that states the finding Strategic use of color to highlight the key comparison Removed unnecessary elements Added context through annotations The second figure tells a story; the first just displays numbers. 11.3.4 Common Pitfalls to Avoid Let’s learn from others’ mistakes. Here are real examples of what not to do: Don’t use decorative shapes that distort data: Bad visualization forcing data into foot shape Forcing data into the shape of a foot might seem creative, but it makes comparisons nearly impossible. Viewers spend more time figuring out the shape than understanding the data. Don’t create meaningless bar heights: Bars with inconsistent scaling In this figure, bars representing 29.4% and 2.4% are the same height! The visualization completely fails at its basic job of showing relative quantities. Don’t use 3D effects or pie charts for complex data: Pie chart with too many categories Pie charts fail when you have many categories because: Humans are bad at comparing angles Small slices become invisible Labels overlap and become unreadable The legend becomes a decoding puzzle Remember: If viewers need to work hard to understand your visualization, you’ve failed - not them. 11.4 Getting Started with ggplot2 Now that we understand the principles, let’s put them into practice using ggplot2. This package revolutionized R graphics by implementing a consistent “grammar” for building visualizations. 11.4.1 Understanding the Grammar of Graphics The key insight of the grammar of graphics is that any statistical graphic can be built from the same basic components: Data: The information you want to visualize Aesthetic mappings: How data variables connect to visual properties (position, color, size, etc.) Geometric objects: The shapes that represent data (bars, points, lines, etc.) Scales: How data values translate to visual values Facets: How to split data into subplots Coordinate system: How x and y aesthetics combine to position elements Theme: Overall visual defaults In ggplot2, we build graphics by adding these components as layers. This is fundamentally different from software like Excel where you pick a chart type and fill in options. Here, you construct your visualization piece by piece, giving you complete control. 11.4.2 The Layering System Think of creating a ggplot like painting: you start with a blank canvas, then add layers of paint. Each layer adds new information or modifies what’s already there. This approach has several advantages: Flexibility: You can create any visualization by combining simple pieces Iterative: You can build incrementally, testing as you go Reusable: You can save a basic plot and add different layers for different purposes Readable: The code describes what you’re doing step by step Let’s load the necessary packages: # Load the ggplot2 package for visualization library(ggplot2) # Load tidyverse for data manipulation # This includes dplyr for data wrangling and other useful tools library(tidyverse) 11.5 Bar Plots for Categorical Data Bar plots excel at showing frequencies or counts for categorical variables. They’re ideal when you have distinct groups and want to compare their sizes. Unlike continuous data, categorical data has natural divisions that bars represent perfectly. 11.5.1 Understanding the Roommate Study Data For our bar plot examples, we’ll use data from a natural experiment. Strother et al. (2021) studied whether college students’ political views are influenced by their randomly assigned roommates. This is a great research design - since roommates are randomly assigned, we can study peer influence without worrying that similar people chose to live together. The researchers surveyed students at two times: Baseline survey: Before students met their roommates Follow-up survey: After living together for a year Let’s load and explore this data: # Load the haven package to read Stata files library(haven) # Read the data file # This data comes in Stata format (.dta), common in social science roommate &lt;- read_dta(&quot;RoommateIdeologyAbridged.dta&quot;) # Let&#39;s see what variables we have glimpse(roommate) ## Rows: 7,040 ## Columns: 14 ## $ respid &lt;chr&gt; &quot;1000371&quot;, &quot;1000528&quot;, &quot;1000685&quot;, &quot;1000842&quot;, &quot;1000999&quot;, &quot;1001156&quot;, … ## $ female &lt;dbl&gt; 1, 1, 1, 0, 0, 1, NA, NA, 0, 0, 1, 0, 1, NA, NA, 1, NA, NA, 1, NA,… ## $ race_white &lt;dbl&gt; 1, 0, 1, 0, 1, 1, NA, NA, 1, 1, 1, 1, 1, NA, NA, 1, NA, NA, 1, NA,… ## $ race_black &lt;dbl&gt; 0, 0, 0, 0, 0, 0, NA, NA, 0, 0, 0, 0, 0, NA, NA, 0, NA, NA, 0, NA,… ## $ race_hispanic &lt;dbl&gt; 0, 0, 0, 0, 0, 0, NA, NA, 0, 0, 0, 0, 0, NA, NA, 0, NA, NA, 0, NA,… ## $ race_other &lt;dbl&gt; 0, 0, 0, 0, 0, 0, NA, NA, 0, 0, 0, 0, 0, NA, NA, 0, NA, NA, 0, NA,… ## $ race_asian &lt;dbl&gt; 0, 1, 0, 1, 0, 0, NA, NA, 0, 0, 0, 0, 0, NA, NA, 0, NA, NA, 0, NA,… ## $ educ_parents &lt;dbl+lbl&gt; 7, 7, 6, 6, 4, 6, NA, NA, 4, 6, 7, 4, 7, NA, NA, 7… ## $ b_religiosity &lt;dbl+lbl&gt; 3, 3, 4, 3, 2, NA, 4, 1, 3, 2, 2, NA, 4, NA, 2, 4… ## $ b_polit_views &lt;dbl+lbl&gt; 1, 3, 2, 2, 3, NA, 2, 3, 2, 2, 3, NA, 2, NA, 2, 2… ## $ f_religiosity &lt;dbl+lbl&gt; 4, 4, 4, 3, 2, 2, NA, NA, 3, 2, 3, 1, 3, NA, NA, 4… ## $ f_polit_views &lt;dbl+lbl&gt; 2, 2, 2, 3, 3, 3, NA, NA, 3, 2, 2, 4, 2, NA, NA, 2… ## $ rm_b_polit_views_mean &lt;dbl&gt; 3.000000, 2.000000, 2.500000, 2.000000, NA, NA, 2.000000, NA, 4.00… ## $ rm_f_polit_views_mean &lt;dbl&gt; NA, 2.0, 3.0, NA, NA, NA, NA, NA, 4.0, 3.0, 2.0, 3.0, 2.0, NA, NA,… # Check the structure Key variables for our visualizations: b_polit_views: Baseline political ideology (1 = extremely left, 5 = extremely right) b_religiosity: Baseline religiosity (1 = very religious, 4 = not religious) female: Gender indicator (1 = female, 0 = male) 11.5.2 Preparing Data for Visualization One crucial step before visualization is ensuring our categorical variables are properly formatted. R needs to know these are categories, not just numbers. We’ll also add meaningful labels: # First, convert political views to a factor # as.factor() tells R these are categories, not continuous numbers roommate$b_polit_views &lt;- as.factor(roommate$b_polit_views) # Check what we have table(roommate$b_polit_views) ## ## 1 2 3 4 5 ## 167 1954 1749 865 47 # Now add meaningful labels using dplyr&#39;s recode function roommate &lt;- roommate %&gt;% mutate( # Create a new variable with descriptive labels b_polit_views_named = dplyr::recode(b_polit_views, &quot;1&quot; = &quot;1. Extremely left&quot;, &quot;2&quot; = &quot;2. Left&quot;, &quot;3&quot; = &quot;3. Moderate&quot;, &quot;4&quot; = &quot;4. Right&quot;, &quot;5&quot; = &quot;5. Extremely right&quot; ) ) # Verify our recoding worked table(roommate$b_polit_views_named) ## ## 1. Extremely left 2. Left 3. Moderate 4. Right 5. Extremely right ## 167 1954 1749 865 47 Let’s do the same for religiosity: # Convert to factor roommate$b_religiosity &lt;- as.factor(roommate$b_religiosity) # Add labels (just endpoints for cleaner plots) roommate &lt;- roommate %&gt;% mutate( b_religiosity = dplyr::recode(b_religiosity, &quot;1&quot; = &quot;1. Very religious&quot;, &quot;2&quot; = &quot;2&quot;, &quot;3&quot; = &quot;3&quot;, &quot;4&quot; = &quot;4. Not religious&quot; ) ) # Remove any rows with missing data on our key variables # This ensures our visualizations aren&#39;t affected by NAs roommate &lt;- roommate %&gt;% filter(!is.na(b_polit_views_named) &amp; !is.na(b_religiosity)) # How many students do we have? nrow(roommate) ## [1] 4781 11.5.3 Building Your First ggplot Now for the exciting part - creating our first visualization! We’ll build it step by step to understand how ggplot works. First, let’s create just the canvas: # This creates a blank plotting area ggplot(data = roommate) When you run this, you’ll see an empty gray rectangle. This is your canvas, waiting for you to add data representations. Now let’s add a geometric layer - in this case, bars: # Start with the canvas and add bars ggplot(data = roommate) + # Note the + sign to add layers! geom_bar(aes(x = b_polit_views_named)) Let’s break down what just happened: ggplot(data = roommate) - Creates the base plot using our roommate data + - This plus sign is crucial! It’s how we add layers in ggplot geom_bar() - Adds a bar geometry (there’s also geom_point, geom_line, etc.) aes(x = b_polit_views_named) - The aesthetic mapping that says “map the political views variable to the x-axis position” The aes() function is particularly important. It creates the connection between your data and the visual properties of the plot. Think of it as saying “this variable controls this visual aspect.” 11.5.4 Customizing Colors and Appearance Our plot works but looks pretty bland. Let’s add some color: # Add a fixed color to all bars ggplot(data = roommate) + geom_bar(aes(x = b_polit_views_named), fill = &quot;steelblue&quot;) Notice that fill = \"steelblue\" is outside the aes() function. This means we’re setting a fixed color for all bars, not mapping color to data. What if we want each political group to have its own color? # Map color to the political views variable ggplot(data = roommate) + geom_bar(aes(x = b_polit_views_named, fill = b_polit_views_named)) Now fill = b_polit_views_named is inside the aes() function, creating a mapping between data and color. ggplot automatically: Assigns a different color to each category Creates a legend Uses a reasonable default color scheme This inside/outside distinction is crucial in ggplot: Inside aes(): Create mappings between data and visuals Outside aes(): Set fixed visual properties 11.5.5 Adding Labels and Titles A graph without proper labels is like a story with no proper nounds - technically complete but hard to understand. Let’s add informative text: # Build a properly labeled plot ggplot(data = roommate) + geom_bar(aes(x = b_polit_views_named), fill = &quot;steelblue&quot;) + labs( x = &quot;Political Ideology&quot;, # X-axis label y = &quot;Number of Students&quot;, # Y-axis label title = &quot;Distribution of Political Views Among College Students&quot;, subtitle = &quot;Survey conducted before random roommate assignment&quot;, caption = &quot;Source: Strother et al. (2021)&quot; ) The labs() function is your text toolkit. You can specify: x and y: Axis labels title: Main title (state your finding!) subtitle: Additional context caption: Data source or notes 11.5.6 Understanding Themes and Further Customization ggplot2 comes with several built-in themes that change the overall look: # Save base plot to reuse p &lt;- ggplot(data = roommate) + geom_bar(aes(x = b_polit_views_named), fill = &quot;steelblue&quot;) + labs(x = &quot;Political Ideology&quot;, y = &quot;Number of Students&quot;, title = &quot;Distribution of Political Views&quot;) # Default theme (gray background) p Now let’s see how different themes change the appearance: # Classic theme - removes gray background, adds axis lines p + theme_classic() # Minimal theme - clean and modern, subtle grid lines p + theme_minimal() # Black and white theme - good for publications, with border p + theme_bw() # Void theme - removes all non-data elements p + theme_void() + labs(title = &quot;Distribution of Political Views (void theme)&quot;) Notice how each theme changes: Background: gray (default) vs. white (most others) vs. none (void) Grid lines: major and minor (default) vs. subtle (minimal) vs. none (classic) Borders: full box (bw) vs. just axes (classic) vs. none (minimal) Overall feel: Excel-like (default) vs. publication-ready (bw) vs. modern (minimal) Beyond preset themes, you can customize every element: # Detailed customization ggplot(data = roommate) + geom_bar(aes(x = b_polit_views_named), fill = &quot;steelblue&quot;) + labs( x = &quot;Political Ideology&quot;, y = &quot;Number of Students&quot;, title = &quot;Most College Students Identify as Moderate or Left-Leaning&quot; ) + theme_minimal() + # Start with a clean theme theme( # Make axis text larger and bold axis.text = element_text(size = 12), axis.title = element_text(size = 14, face = &quot;bold&quot;), # Make title larger and blue plot.title = element_text(size = 16, face = &quot;bold&quot;, color = &quot;darkblue&quot;), # Rotate x-axis labels if they overlap axis.text.x = element_text(angle = 45, hjust = 1) ) Here we used the theme function to set things like font size, font type, colors and more. It would take far more than one class to go through all of these options, but suffice it to say, that there is a way to control nearly every visual element you see on a plot. 11.5.7 Using Color to Show Relationships So far we’ve created single-variable visualizations. But color can reveal relationships between variables. Let’s explore how political views relate to religiosity: # Color bars by religiosity ggplot(data = roommate) + geom_bar(aes(x = b_polit_views_named, fill = b_religiosity)) + labs( x = &quot;Political Ideology&quot;, y = &quot;Number of Students&quot;, title = &quot;Religious Students Tend to Be More Conservative&quot;, subtitle = &quot;Distribution of political views colored by religiosity&quot;, fill = &quot;Religiosity&quot; # Legend title ) + theme_minimal() This creates a “stacked” bar chart. Each bar is divided into segments showing the religious composition of that political group. We can see that very religious students (dark red) make up a larger proportion of conservative students. Sometimes stacked bars make comparisons difficult. We can place bars side-by-side instead: # Grouped bar chart ggplot(data = roommate) + geom_bar(aes(x = b_polit_views_named, fill = b_religiosity), position = &quot;dodge&quot;) + # This creates groups instead of stacks labs( x = &quot;Political Ideology&quot;, y = &quot;Number of Students&quot;, title = &quot;Religious Students Tend to Be More Conservative&quot;, subtitle = &quot;Grouped bars make religiosity patterns clearer&quot;, fill = &quot;Religiosity&quot; ) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) The position = \"dodge\" argument tells ggplot to place bars side-by-side rather than stacking them. This makes it easier to compare the height of specific religious groups across political ideologies. 11.5.8 Fun with Color Palettes While ggplot’s default colors work well, sometimes you want something different. There are many color palette packages available: # Load the Taylor Swift color palette package (yes, this exists!) library(tayloRswift) # Apply Taylor Swift&#39;s 1989 album colors to our plot ggplot(data = roommate) + geom_bar(aes(x = b_polit_views_named, fill = b_polit_views_named)) + scale_fill_taylor(palette = &quot;taylor1989&quot;) + # Custom color palette labs( x = &quot;Political Ideology&quot;, y = &quot;Number of Students&quot;, title = &quot;Political Views of College Students&quot;, fill = &quot;Ideology&quot; ) + theme_minimal() + theme(legend.position = &quot;none&quot;) # Hide legend since colors match x-axis There are also more serious color palettes designed for clarity and accessibility: scale_fill_brewer(): ColorBrewer palettes (great for publications) scale_fill_viridis_d(): Colorblind-friendly palettes scale_fill_grey(): Grayscale for black-and-white printing 11.6 Histograms for Continuous Data When your variable has many possible values - like test scores, income, or approval ratings - bar plots become impractical. You’d need hundreds of tiny bars! Histograms solve this by grouping continuous data into bins. 11.6.1 Understanding the ANES Data Let’s switch to a different dataset to explore continuous variables. The American National Election Study (ANES) is one of the most important data sources in political science. Running since 1948, it surveys Americans about their political attitudes, behaviors, and demographics. We’ll use 2020 data, collected during a pivotal election year marked by the COVID-19 pandemic, racial justice protests, and deep political polarization. # Load the 2020 ANES data anes2020 &lt;- read.csv(&quot;anes2020.csv&quot;) # Explore the structure dim(anes2020) # How many respondents and variables? ## [1] 8280 12 # Look at the data a bit glimpse(anes2020) ## Rows: 8,280 ## Columns: 12 ## $ rid &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, … ## $ age &lt;int&gt; 46, 37, 40, 41, 72, 71, 37, 45, 70, 43, 37, 55, 30, 38, 41, 66, 54, 55, 62, 80,… ## $ race &lt;int&gt; 3, 4, 1, 4, 5, 1, 1, 1, 1, 3, 3, 1, 1, 4, 2, 3, 1, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1… ## $ latino &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2… ## $ vote12 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1… ## $ vote16 &lt;int&gt; 1, 1, 1, 1, 1, 2, 1, 2, NA, NA, NA, 1, 2, NA, 2, NA, 1, NA, NA, 1, NA, NA, 1, N… ## $ media &lt;int&gt; 2, 3, 1, 3, 2, 2, 3, 4, 3, 2, 4, 3, 1, 1, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2… ## $ partyid &lt;int&gt; 2, 5, 3, 2, 3, 3, 2, 2, 3, 1, 1, 1, 2, 1, 3, 3, 3, 1, 1, 1, 2, 1, 3, 2, 1, 1, 3… ## $ income &lt;int&gt; 21, 13, 17, 7, 22, 3, 4, 3, 10, 11, 9, 18, 1, 21, 3, 7, 22, 20, NA, 5, 12, 6, 2… ## $ obama.th &lt;int&gt; 0, 50, 90, 85, 10, 60, 15, 50, 60, 100, 100, 85, 0, 100, 85, 0, 85, 100, 100, 1… ## $ trump.th &lt;int&gt; 100, 0, 0, 15, 85, 0, 75, 100, 0, 0, 0, 0, 100, 0, 40, 100, 0, 0, 15, 15, 95, 0… ## $ biden.th &lt;int&gt; 0, 0, 65, 70, 15, 85, 50, 50, 85, 85, 100, 60, 0, 85, 60, 0, 60, 85, 85, 70, 0,… For our examples, we’ll focus on: biden.th: “Feeling thermometer” for Joe Biden (0-100 scale) 0 = Very cold/negative feelings 50 = Neutral 100 = Very warm/positive feelings latino: Whether respondent identifies as Hispanic/Latino Feeling thermometers are a clever survey tool. Instead of asking “Do you like Biden?” (which forces yes/no), they let respondents express degrees of warmth or coolness, capturing more nuance. Let’s prepare the data: # Convert latino to a factor with meaningful labels anes2020$latino &lt;- as.factor(anes2020$latino) anes2020 &lt;- anes2020 %&gt;% mutate( latino = dplyr::recode(latino, &quot;1&quot; = &quot;Hispanic&quot;, &quot;2&quot; = &quot;Non-Hispanic&quot; ) ) # Remove missing values anes2020 &lt;- anes2020 %&gt;% filter(!is.na(latino) &amp; !is.na(biden.th)) # Check the distribution of the Biden thermometer summary(anes2020$biden.th) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 15.00 55.00 49.26 85.00 100.00 11.6.2 Creating Basic Histograms Let’s start with base R’s hist() function to understand what histograms do: # Basic histogram hist(anes2020$biden.th) This shows the distribution is polarized - many people rate Biden very low (near 0) or very high (near 100), with fewer in the middle. This U-shaped pattern reflects America’s political polarization. We can improve this basic histogram: # Customized histogram hist(anes2020$biden.th, breaks = 20, # More bins for finer detail col = &quot;lightblue&quot;, # Add color xlab = &quot;Biden Feeling Thermometer (0-100)&quot;, ylab = &quot;Number of Respondents&quot;, main = &quot;Americans Are Polarized About Joe Biden&quot;, sub = &quot;Note clustering at 0 and 100&quot;) 11.6.3 Building Histograms with ggplot2 Now let’s use ggplot2 for more control and better aesthetics: # Basic ggplot histogram ggplot(data = anes2020) + geom_histogram(aes(x = biden.th)) That’s quite ugly! The default of 30 bins creates a spiky appearance because respondents tend to choose round numbers (50, 60, 70) rather than precise values (57, 63, 71). Let’s fix this: # Better histogram with fewer bins ggplot(data = anes2020) + geom_histogram(aes(x = biden.th), bins = 10, # Fewer bins smooth out spikes fill = &quot;darkblue&quot;, color = &quot;white&quot;, # White borders between bins alpha = 0.7) + # Some transparency labs( x = &quot;Feeling Thermometer (0 = Very Cold, 100 = Very Warm)&quot;, y = &quot;Number of Respondents&quot;, title = &quot;Americans Are Deeply Polarized About President Biden&quot;, subtitle = &quot;Many rate him 0 or 100, with fewer neutral ratings&quot;, caption = &quot;Source: 2020 American National Election Study&quot; ) + theme_minimal() The key parameters for histograms: bins: Number of bins (default 30) binwidth: Width of each bin (alternative to bins) color: Border color of bins fill: Interior color of bins alpha: Transparency (0 = invisible, 1 = solid) 11.6.4 Comparing Groups with Histograms How do Hispanic and non-Hispanic Americans differ in their views of Biden? Let’s find out: # Stacked histogram ggplot(data = anes2020) + geom_histogram(aes(x = biden.th, fill = latino), bins = 10, alpha = 0.7) + labs( x = &quot;Biden Feeling Thermometer&quot;, y = &quot;Count&quot;, title = &quot;Hispanic Americans View Biden More Favorably&quot;, subtitle = &quot;Distribution of Biden thermometer ratings by ethnicity&quot;, fill = &quot;Ethnicity&quot; ) + theme_minimal() The stacking makes it hard to compare distributions. We can see there are more non-Hispanic respondents (the bars are taller), but the shape comparison is difficult. Let’s try overlapping histograms instead: # Overlapping histograms ggplot(data = anes2020) + geom_histogram(aes(x = biden.th, fill = latino), bins = 10, alpha = 0.5, # Transparency to see overlap position = &quot;identity&quot;) + # Overlap instead of stack labs( x = &quot;Biden Feeling Thermometer&quot;, y = &quot;Count&quot;, title = &quot;Hispanic Americans View Biden More Favorably&quot;, subtitle = &quot;Hispanic distribution shifted right (more positive)&quot;, fill = &quot;Ethnicity&quot; ) + theme_minimal() + scale_fill_manual(values = c(&quot;Hispanic&quot; = &quot;darkred&quot;, &quot;Non-Hispanic&quot; = &quot;darkblue&quot;)) The transparency (alpha = 0.5) lets us see where the distributions overlap. The Hispanic distribution (red) is shifted right compared to non-Hispanic (blue), indicating more positive views. 11.7 Density Plots for Smooth Distributions Histograms have a fundamental limitation: the appearance depends heavily on bin choices. Density plots solve this by creating smooth curves that estimate the underlying distribution. 11.7.1 Understanding Density Plots Think of a density plot as a smoothed histogram. Instead of counting observations in bins, it estimates the probability density at each point. The total area under the curve always equals 1. # Basic density plot ggplot(data = anes2020) + geom_density(aes(x = biden.th)) + labs( x = &quot;Biden Feeling Thermometer&quot;, y = &quot;Density&quot;, title = &quot;Density Plot Shows Polarization More Clearly Than Histogram&quot; ) + theme_minimal() The peaks are even more obvious here - Americans cluster at the extremes with fewer moderate opinions. 11.7.2 Comparing Distributions with Density Plots Density plots excel at comparing groups because overlapping curves are easier to read than overlapping bars: # Comparing ethnicities with density plots ggplot(data = anes2020) + geom_density(aes(x = biden.th, fill = latino), alpha = 0.4) + # Transparency for overlap labs( x = &quot;Biden Feeling Thermometer&quot;, y = &quot;Density&quot;, title = &quot;Hispanic Americans Show More Positive Views of Biden&quot;, subtitle = &quot;Notice the Hispanic peak around 80-90 vs non-Hispanic peak at 0&quot;, fill = &quot;Ethnicity&quot;, caption = &quot;Area under each curve = 1&quot; ) + theme_minimal() + scale_fill_manual(values = c(&quot;Hispanic&quot; = &quot;#e41a1c&quot;, &quot;Non-Hispanic&quot; = &quot;#377eb8&quot;)) This clearly shows: Non-Hispanic respondents have a huge peak at 0 (very negative views) Hispanic respondents peak around 80-90 (very positive views) Both groups show polarization, but in opposite directions 11.7.3 Box Plots for Quick Comparisons When comparing many groups, even density plots become cluttered. Box plots summarize distributions using five key numbers: # Box plot comparison ggplot(data = anes2020) + geom_boxplot(aes(x = latino, y = biden.th, fill = latino)) + labs( x = &quot;Ethnicity&quot;, y = &quot;Biden Feeling Thermometer&quot;, title = &quot;Box Plots Summarize Key Distribution Features&quot;, subtitle = &quot;Hispanic median ~60, Non-Hispanic median ~30&quot; ) + theme_minimal() + theme(legend.position = &quot;none&quot;) # Hide redundant legend Box plots show: Thick line: Median (50th percentile) Box edges: 25th and 75th percentiles Whiskers: Extend to most extreme non-outlier points Dots: Individual outliers While less detailed than histograms or density plots, box plots excel when comparing many groups simultaneously. 11.8 Putting It All Together: Best Practices 11.8.1 Choosing the Right Visualization The choice depends on your data and message: Bar plots are best when: You have categorical data with relatively few levels You want to show counts or proportions The exact values matter Example: Votes by political party Histograms work well when: You have continuous data You want to see the shape of the distribution You’re exploring data for the first time Example: Distribution of campaign donations Density plots excel when: You need to compare multiple continuous distributions Smooth shapes matter more than exact counts You have enough data for reliable smoothing Example: Comparing income distributions across states Box plots are ideal when: You need to compare many groups You want a quick summary of key statistics You’re checking for outliers Example: Test scores across 50 schools 11.9 Summary Effective data visualization combines technical skills with design thinking. We’ve covered: Technical skills: Creating bar plots with geom_bar() Making histograms with geom_histogram() Drawing density plots with geom_density() Making boxplots with geom_boxplot() Customizing with themes, colors, and labels Using aesthetic mappings to encode information Design principles: Declutter, emphasize, and clarify Choose appropriate chart types Use color strategically Write informative titles and labels The ggplot2 advantage: Layer-by-layer construction Consistent grammar across all plot types Infinite customization possibilities Reproducible and shareable code Remember: The best visualization is one that communicates your message clearly to your specific audience. Always test your visualizations on others - if they’re confused, iterate and improve. 11.10 Study Questions 11.10.1 Understanding Visualization Principles What visual elements do viewers notice first when looking at a graph? List them in order and explain why this matters for design. Explain the three core principles of effective data visualization (declutter, emphasize, clarify) and give a specific example of each. Why are pie charts problematic for displaying data with many categories? What would you use instead? (Hint: It’s a barplot) 11.10.2 Understanding ggplot2 Structure In ggplot2, what is a “layer”? Give three examples of different layers you might add to a plot. Explain the difference between these two code snippets: geom_bar(aes(x = category), fill = \"blue\") geom_bar(aes(x = category, fill = \"blue\")) What does the + symbol do in ggplot2? Why is this approach useful? 11.10.3 Creating Bar Plots When should you use a bar plot instead of a histogram? In the roommate example, why did we convert b_polit_views to a factor before plotting? What does position = \"dodge\" do in a bar plot? When would you use it instead of the default stacking? 11.10.4 Working with Histograms What does the bins parameter control in a histogram? What happens if you set it too high or too low? The reading mentions that survey respondents tend to pick round numbers (like 70 or 80). How does this affect histograms and how can you address it? 11.10.5 Understanding Density Plots What’s the main advantage of density plots over histograms? When comparing two groups, why might overlapping density curves be easier to read than overlapping histograms? 11.10.6 Data Preparation Why is it important to remove missing values before creating visualizations? What’s the purpose of adding labels when converting numeric codes to factors? 11.10.7 Practical Application You have data on student exam scores (0-100). Would you use a bar plot, histogram, or density plot? Explain your reasoning. You want to show how many students are in each major (15 different majors). Which visualization would you choose and why? You’re comparing income distributions across 5 different countries. What type of plot would work best and why? 11.10.8 Code Writing Write the complete ggplot2 code to create a blue histogram of a variable called income from a dataset called survey, with 30 bins and appropriate labels. Write code to create a bar plot with different colors for each bar (not using default colors). 11.10.9 Interpretation Looking at the political views bar chart, what does the height of each bar represent? In a box plot, what percentage of the data falls within the box itself? "],["visualizing-relations-between-variables.html", "12 Visualizing Relations between Variables 12.1 Learning Objectives 12.2 What Is This For? 12.3 Comparing Groups with Boxplots 12.4 Scatterplots for Continuous Relationships 12.5 Using Facets to Reveal Patterns 12.6 Choosing the Right Visualization 12.7 Summary 12.8 Study Questions", " 12 Visualizing Relations between Variables 12.1 Learning Objectives Create and interpret boxplots to compare distributions across groups Master scatterplots for examining relationships between continuous variables Use facet_wrap() to create small multiples for comparative analysis Combine multiple visual elements (color, shape, faceting) to display 3-4 variables simultaneously Choose the appropriate visualization for different types of variable relationships 12.2 What Is This For? In the previous session, we learned how to visualize single variables - the distribution of ages, the frequency of categories, the spread of continuous data. But in real research, we rarely care about variables in isolation. We want to know: Do wealthy countries have higher life expectancies? Does education affect political participation? Are movie critics harsher than general audiences? These questions require us to visualize relationships between variables. Today, we’ll learn three powerful techniques: Boxplots for group comparisons - When you have a continuous variable and want to compare it across categories Scatterplots for continuous relationships - When both variables are continuous and you want to see patterns Faceting for deeper insights - When you need to examine relationships separately for different subgroups By the end of this session, you’ll be able to create visualizations that reveal complex patterns in your data - the kind of insights that tables of numbers simply can’t provide. 12.3 Comparing Groups with Boxplots 12.3.1 When to Use Boxplots Boxplots excel when you want to compare a continuous variable across different categories. They’re particularly useful because they show: The median (center line) The interquartile range (the box) The full spread (whiskers) Outliers (individual points) All in a compact, easy-to-compare format. 12.3.2 Congressional Age Example Let’s examine age patterns in the U.S. Congress using the FiveThirtyEight dataset. This data contains information about every member of Congress from 1947 to 2013. # Load required packages library(fivethirtyeight) library(ggplot2) library(dplyr) # Load the congressional age data data(&quot;congress_age&quot;) # Examine the structure glimpse(congress_age) ## Rows: 18,635 ## Columns: 13 ## $ congress &lt;int&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 8… ## $ chamber &lt;chr&gt; &quot;house&quot;, &quot;house&quot;, &quot;house&quot;, &quot;house&quot;, &quot;house&quot;, &quot;house&quot;, &quot;house&quot;, &quot;house&quot;, &quot;hous… ## $ bioguide &lt;chr&gt; &quot;M000112&quot;, &quot;D000448&quot;, &quot;S000001&quot;, &quot;E000023&quot;, &quot;L000296&quot;, &quot;G000017&quot;, &quot;W000265&quot;, … ## $ firstname &lt;chr&gt; &quot;Joseph&quot;, &quot;Robert&quot;, &quot;Adolph&quot;, &quot;Charles&quot;, &quot;William&quot;, &quot;James&quot;, &quot;Richard&quot;, &quot;Sol&quot;… ## $ middlename &lt;chr&gt; &quot;Jefferson&quot;, &quot;Lee&quot;, &quot;Joachim&quot;, &quot;Aubrey&quot;, NA, &quot;A.&quot;, &quot;Joseph&quot;, NA, NA, &quot;Laceill… ## $ lastname &lt;chr&gt; &quot;Mansfield&quot;, &quot;Doughton&quot;, &quot;Sabath&quot;, &quot;Eaton&quot;, &quot;Lewis&quot;, &quot;Gallagher&quot;, &quot;Welch&quot;, &quot;B… ## $ suffix &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ birthday &lt;date&gt; 1861-02-09, 1863-11-07, 1866-04-04, 1868-03-29, 1868-09-22, 1869-01-16, 1869… ## $ state &lt;chr&gt; &quot;TX&quot;, &quot;NC&quot;, &quot;IL&quot;, &quot;NJ&quot;, &quot;KY&quot;, &quot;PA&quot;, &quot;CA&quot;, &quot;NY&quot;, &quot;WI&quot;, &quot;MA&quot;, &quot;VA&quot;, &quot;KY&quot;, &quot;IN&quot;,… ## $ party &lt;chr&gt; &quot;D&quot;, &quot;D&quot;, &quot;D&quot;, &quot;R&quot;, &quot;R&quot;, &quot;R&quot;, &quot;R&quot;, &quot;D&quot;, &quot;R&quot;, &quot;R&quot;, &quot;D&quot;, &quot;R&quot;, &quot;D&quot;, &quot;D&quot;, &quot;D&quot;, &quot;D… ## $ incumbent &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU… ## $ termstart &lt;date&gt; 1947-01-03, 1947-01-03, 1947-01-03, 1947-01-03, 1947-01-03, 1947-01-03, 1947… ## $ age &lt;dbl&gt; 85.9, 83.2, 80.7, 78.8, 78.3, 78.0, 77.9, 76.8, 76.0, 75.8, 74.7, 74.0, 73.5,… First, let’s clean up the chamber variable to have proper capitalization: # Recode chamber names for better display congress_age &lt;- congress_age %&gt;% mutate(chamber = dplyr::recode(chamber, &quot;house&quot; = &quot;House&quot;, &quot;senate&quot; = &quot;Senate&quot; )) # Check our recoding table(congress_age$chamber) ## ## House Senate ## 15083 3552 12.3.3 Creating a Basic Boxplot Let’s compare ages between the House and Senate: # Basic boxplot ggplot(data = congress_age) + geom_boxplot(aes(x = chamber, y = age)) + labs( x = &quot;Chamber&quot;, y = &quot;Age&quot;, title = &quot;Senators Tend to Be Older Than Representatives&quot;, subtitle = &quot;Distribution of ages in the 80th-113th Congresses&quot; ) + theme_minimal() Let’s break down this code: geom_boxplot() creates the boxplot geometry Inside aes() (aesthetic mapping), we specify: x = chamber: The categorical variable for groups (House vs Senate) y = age: The continuous variable we’re comparing The aes() function is crucial - it tells ggplot which variables in your data should control which visual properties Notice how the boxplot immediately reveals several insights: The Senate median age is consistently higher (center line in each box) Both chambers have similar spreads (box heights are comparable) The House has more young outliers (individual points below the whisker) Very old members (80+) appear in both chambers as outliers 12.3.4 Adding a Third Variable with Color Now let’s add incumbency status to see if the age difference is driven by new members or veterans: # Boxplot with color for incumbency ggplot(data = congress_age) + geom_boxplot(aes(x = chamber, y = age, fill = incumbent)) + labs( x = &quot;Chamber&quot;, y = &quot;Age&quot;, title = &quot;Incumbents Are Older in Both Chambers&quot;, subtitle = &quot;Age distribution by chamber and incumbency status&quot;, fill = &quot;Incumbent?&quot; ) + theme_minimal() + scale_fill_manual(values = c(&quot;TRUE&quot; = &quot;#2E86AB&quot;, &quot;FALSE&quot; = &quot;#A23B72&quot;)) Here’s what changed: We added fill = incumbent inside aes() This creates a mapping between the incumbent variable and the fill color ggplot automatically assigns different colors to TRUE/FALSE Because it’s inside aes(), ggplot knows to create separate boxes for each value scale_fill_manual() lets us customize the colors: We specify exact colors for TRUE (blue) and FALSE (pink) Without this, ggplot would use default colors Key concept: When you put an argument inside aes(), you’re creating a mapping between data and visual properties. When it’s outside aes(), you’re setting a fixed property for all data points. The side-by-side boxes reveal that: Incumbents are older than non-incumbents in both chambers The age gap between chambers exists for both groups Non-incumbent senators are still older than non-incumbent representatives 12.3.5 Using Facets Instead of Colors Sometimes, faceting provides clearer comparisons than colors: # Using facets for chamber comparison ggplot(data = congress_age) + geom_boxplot(aes(x = incumbent, y = age)) + facet_wrap(~chamber) + labs( x = &quot;Incumbent?&quot;, y = &quot;Age&quot;, title = &quot;Age Patterns Are Similar Across Chambers&quot;, subtitle = &quot;Incumbency creates a larger age gap than chamber membership&quot; ) + theme_minimal() Notice the changes: We switched what’s on the x-axis: now x = incumbent We removed the fill aesthetic - back to single-colored boxes facet_wrap(~chamber) creates separate panels: The ~ (tilde) is R’s formula notation It means “create facets by the chamber variable” Each unique value of chamber gets its own panel Faceting makes it easier to focus on the incumbency comparison within each chamber. You can directly compare the TRUE vs FALSE boxes without the visual interference of different colors. 12.3.6 Visualizing Change Over Time Let’s push our visualization further by examining how these patterns have changed over the decades: # Convert congress number to factor for discrete colors congress_age$congress &lt;- as.factor(congress_age$congress) # Create a complex visualization ggplot(data = congress_age) + geom_boxplot(aes(x = incumbent, y = age, fill = congress)) + facet_wrap(~chamber, ncol = 1) + labs( x = &quot;Incumbent?&quot;, y = &quot;Age&quot;, fill = &quot;Congress&quot;, title = &quot;Congress Has Grown Older Over Time&quot;, subtitle = &quot;Especially among House incumbents (80th Congress = 1947-49, 113th = 2013-15)&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) + guides(fill = guide_legend(nrow = 2)) This code combines everything we’ve learned: fill = congress inside aes() assigns a different color to each Congress facet_wrap(~chamber, ncol = 1) creates one column of facets: ncol = 1 stacks the panels vertically Without this, ggplot would arrange them side-by-side theme(legend.position = \"bottom\") moves the legend below the plot guides(fill = guide_legend(nrow = 2)) arranges the legend in 2 rows This prevents the legend from being too wide Important: We converted congress to a factor first. This tells ggplot to treat each Congress as a discrete category rather than a continuous number, giving us distinct colors instead of a gradient. This complex visualization reveals: A clear trend toward older members over time The aging is most pronounced among House incumbents The Senate has maintained relatively stable age patterns 12.4 Scatterplots for Continuous Relationships 12.4.1 When Two Variables Are Both Continuous Boxplots work great when one variable is categorical. But what if both variables are continuous? Enter the scatterplot - the workhorse of data exploration. 12.4.2 Movie Ratings Example Let’s explore how professional critics and general audiences rate movies differently. We’ll use a dataset of 651 randomly sampled movies: # Load the movies data MovieData &lt;- read.csv(&quot;https://www.dropbox.com/scl/fi/v2ldpanyvhy2nb18vy20r/movies.csv?rlkey=g0tfkjuv3y4tbgsgvfx9c95sk&amp;dl=1&quot;) # Examine the structure str(MovieData) ## &#39;data.frame&#39;: 651 obs. of 32 variables: ## $ title : chr &quot;Filly Brown&quot; &quot;The Dish&quot; &quot;Waiting for Guffman&quot; &quot;The Age of Innocence&quot; ... ## $ title_type : chr &quot;Feature Film&quot; &quot;Feature Film&quot; &quot;Feature Film&quot; &quot;Feature Film&quot; ... ## $ genre : chr &quot;Drama&quot; &quot;Drama&quot; &quot;Comedy&quot; &quot;Drama&quot; ... ## $ runtime : int 80 101 84 139 90 78 142 93 88 119 ... ## $ mpaa_rating : chr &quot;R&quot; &quot;PG-13&quot; &quot;R&quot; &quot;PG&quot; ... ## $ studio : chr &quot;Indomina Media Inc.&quot; &quot;Warner Bros. Pictures&quot; &quot;Sony Pictures Classics&quot; &quot;Columbia Pictures&quot; ... ## $ thtr_rel_year : int 2013 2001 1996 1993 2004 2009 1986 1996 2012 2012 ... ## $ thtr_rel_month : int 4 3 8 10 9 1 1 11 9 3 ... ## $ thtr_rel_day : int 19 14 21 1 10 15 1 8 7 2 ... ## $ dvd_rel_year : int 2013 2001 2001 2001 2005 2010 2003 2004 2013 2012 ... ## $ dvd_rel_month : int 7 8 8 11 4 4 2 3 1 8 ... ## $ dvd_rel_day : int 30 28 21 6 19 20 18 2 21 14 ... ## $ imdb_rating : num 5.5 7.3 7.6 7.2 5.1 7.8 7.2 5.5 7.5 6.6 ... ## $ imdb_num_votes : int 899 12285 22381 35096 2386 333 5016 2272 880 12496 ... ## $ critics_rating : chr &quot;Rotten&quot; &quot;Certified Fresh&quot; &quot;Certified Fresh&quot; &quot;Certified Fresh&quot; ... ## $ critics_score : int 45 96 91 80 33 91 57 17 90 83 ... ## $ audience_rating : chr &quot;Upright&quot; &quot;Upright&quot; &quot;Upright&quot; &quot;Upright&quot; ... ## $ audience_score : int 73 81 91 76 27 86 76 47 89 66 ... ## $ best_pic_nom : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ best_pic_win : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ best_actor_win : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... ## $ best_actress_win: chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ best_dir_win : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... ## $ top200_box : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ director : chr &quot;Michael D. Olmos&quot; &quot;Rob Sitch&quot; &quot;Christopher Guest&quot; &quot;Martin Scorsese&quot; ... ## $ actor1 : chr &quot;Gina Rodriguez&quot; &quot;Sam Neill&quot; &quot;Christopher Guest&quot; &quot;Daniel Day-Lewis&quot; ... ## $ actor2 : chr &quot;Jenni Rivera&quot; &quot;Kevin Harrington&quot; &quot;Catherine O&#39;Hara&quot; &quot;Michelle Pfeiffer&quot; ... ## $ actor3 : chr &quot;Lou Diamond Phillips&quot; &quot;Patrick Warburton&quot; &quot;Parker Posey&quot; &quot;Winona Ryder&quot; ... ## $ actor4 : chr &quot;Emilio Rivera&quot; &quot;Tom Long&quot; &quot;Eugene Levy&quot; &quot;Richard E. Grant&quot; ... ## $ actor5 : chr &quot;Joseph Julian Soria&quot; &quot;Genevieve Mooy&quot; &quot;Bob Balaban&quot; &quot;Alec McCowen&quot; ... ## $ imdb_url : chr &quot;http://www.imdb.com/title/tt1869425/&quot; &quot;http://www.imdb.com/title/tt0205873/&quot; &quot;http://www.imdb.com/title/tt0118111/&quot; &quot;http://www.imdb.com/title/tt0106226/&quot; ... ## $ rt_url : chr &quot;//www.rottentomatoes.com/m/filly_brown_2012/&quot; &quot;//www.rottentomatoes.com/m/dish/&quot; &quot;//www.rottentomatoes.com/m/waiting_for_guffman/&quot; &quot;//www.rottentomatoes.com/m/age_of_innocence/&quot; ... # Focus on key variables MovieData %&gt;% select(title, critics_score, audience_score, genre, mpaa_rating) %&gt;% head() ## title critics_score audience_score genre mpaa_rating ## 1 Filly Brown 45 73 Drama R ## 2 The Dish 96 81 Drama PG-13 ## 3 Waiting for Guffman 91 91 Comedy R ## 4 The Age of Innocence 80 76 Drama PG ## 5 Malevolence 33 27 Horror R ## 6 Old Partner 91 86 Documentary Unrated 12.4.3 Creating a Basic Scatterplot # Basic scatterplot ggplot(data = MovieData) + geom_point(aes(x = critics_score, y = audience_score)) + labs( x = &quot;Critics Score&quot;, y = &quot;Audience Score&quot;, title = &quot;Critics and Audiences Sometimes Disagree&quot;, subtitle = &quot;Each point represents one movie&quot; ) + theme_minimal() Here’s how scatterplots differ from boxplots: geom_point() creates points instead of boxes Both x and y in aes() are now continuous variables Each observation gets its own point (unlike boxplots which summarize groups) The pattern shows: A general positive relationship (critics and audiences often agree) Considerable scatter (they don’t always agree) Some movies loved by audiences but not critics (upper left) Some movies loved by critics but not audiences (lower right) 12.4.4 Dealing with Overplotting When many points overlap, we lose information. Two solutions: Solution 1: Jittering # Add small random noise to separate overlapping points ggplot(data = MovieData) + geom_point(aes(x = critics_score, y = audience_score), position = &quot;jitter&quot;) + labs( x = &quot;Critics Score&quot;, y = &quot;Audience Score&quot;, title = &quot;Jittering Reveals Hidden Points&quot; ) + theme_minimal() Note that position = \"jitter\" is outside the aes() function: This is a fixed setting that applies to all points It adds small random noise to x and y coordinates The amount of jitter is automatically determined by ggplot Solution 2: Transparency # Make points semi-transparent ggplot(data = MovieData) + geom_point(aes(x = critics_score, y = audience_score), position = &quot;jitter&quot;, alpha = 0.5) + # 50% transparency labs( x = &quot;Critics Score&quot;, y = &quot;Audience Score&quot;, title = &quot;Transparency Shows Point Density&quot; ) + theme_minimal() Here we added alpha = 0.5: Also outside aes() because we want all points to have the same transparency Values range from 0 (invisible) to 1 (solid) Where points overlap, the color appears darker This helps reveal density patterns 12.4.5 Adding a Reference Line Sometimes it helps to add a reference line. Here’s a diagonal line representing perfect agreement: # Add diagonal line where critics = audience ggplot(data = MovieData) + geom_point(aes(x = critics_score, y = audience_score), position = &quot;jitter&quot;, alpha = 0.5) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + labs( x = &quot;Critics Score&quot;, y = &quot;Audience Score&quot;, title = &quot;Most Movies Fall Below the Line of Perfect Agreement&quot;, subtitle = &quot;Audiences tend to rate movies higher than critics&quot; ) + theme_minimal() New elements here: geom_abline() adds a straight line defined by intercept and slope intercept = 0, slope = 1 creates the line y = x This represents perfect agreement (critics score = audience score) Notice that color and linetype are outside any aes(): We want one red, dashed line Not a line that varies by data values Points above the line: audiences liked more than critics Points below the line: critics liked more than audiences Here’s the markdown code to add to the original worksheet where the reference line section is. You would replace the “Adding a Reference Line” section with this expanded version: Now let’s add a trend line to see the actual relationship in our data: # Add both reference line and smoothed trend line ggplot(data = MovieData) + geom_point(aes(x = critics_score, y = audience_score), position = &quot;jitter&quot;, alpha = 0.5) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + geom_smooth(aes(x = critics_score, y = audience_score), method = &quot;lm&quot;, se = FALSE, color = &quot;blue&quot;) + labs( x = &quot;Critics Score&quot;, y = &quot;Audience Score&quot;, title = &quot;Audiences Are Generally More Generous Than Critics&quot;, subtitle = &quot;Blue trend line shows actual relationship; red dashed line shows perfect agreement&quot; ) + theme_minimal() ## `geom_smooth()` using formula = &#39;y ~ x&#39; The geom_smooth() function adds a smoothed line through the data: method = \"lm\" fits a straight line (linear model) se = FALSE removes the confidence interval shading The blue line shows the actual average relationship Notice it has a gentler slope than the red line, confirming audiences compress their ratings less than critics 12.5 Using Facets to Reveal Patterns 12.5.1 Examining Patterns by Genre Do critics and audiences disagree more for certain types of movies? Let’s use faceting to find out: # Convert genre to factor MovieData$genre &lt;- as.factor(MovieData$genre) # Create faceted scatterplot ggplot(data = MovieData) + geom_point(aes(x = critics_score, y = audience_score), position = &quot;jitter&quot;, alpha = 0.5) + facet_wrap(~genre, ncol = 2) + labs( x = &quot;Critics Score&quot;, y = &quot;Audience Score&quot;, title = &quot;Critics and Audiences Disagree Most on Art House &amp; Musical Films&quot;, subtitle = &quot;Each panel shows one movie genre&quot; ) + theme_minimal() The facets reveal genre-specific patterns: Drama &amp; Documentary: Strong agreement (tight clusters along diagonal) Art House &amp; International: Critics often like more than audiences Action &amp; Adventure: Audiences more forgiving than critics Horror: Generally low scores from both groups 12.5.2 Adding Color for a Fourth Variable Let’s add MPAA rating as color to see if movie ratings affect the critic-audience relationship: # Convert MPAA rating to factor MovieData$mpaa_rating &lt;- as.factor(MovieData$mpaa_rating) # Faceted plot with color ggplot(data = MovieData) + geom_point(aes(x = critics_score, y = audience_score, color = mpaa_rating), position = &quot;jitter&quot;, alpha = 0.6) + facet_wrap(~genre, ncol = 2) + labs( x = &quot;Critics Score&quot;, y = &quot;Audience Score&quot;, color = &quot;MPAA Rating&quot;, title = &quot;R-Rated Movies Show More Disagreement&quot;, subtitle = &quot;Comparing critic and audience scores by genre and rating&quot; ) + theme_minimal() + scale_color_brewer(palette = &quot;Set1&quot;) We’re now encoding four variables: x position: Critics score y position: Audience score color: MPAA rating (inside aes() to map data to colors) facets: Genre New elements: color = mpaa_rating inside aes() maps each rating to a different color scale_color_brewer(palette = \"Set1\") uses a ColorBrewer palette These palettes are designed for clarity and colorblind-friendliness Notice how the legend title matches our color = \"MPAA Rating\" label 12.6 Choosing the Right Visualization 12.6.1 Decision Framework When visualizing relationships between variables, consider: Use boxplots when: One variable is categorical, one is continuous You want to compare distributions You need to show outliers Groups have unequal sizes Use scatterplots when: Both variables are continuous You want to see the form of the relationship Individual observations matter You’re looking for patterns or clusters Use faceting when: You need to compare patterns across groups The relationship might differ by subgroup You have a moderating third variable One plot would be too cluttered 12.6.2 Best Practices for Multi-Variable Plots Start simple, add complexity gradually First create the basic relationship Add color only if it reveals something new Use facets when groups obscure each other Maintain visual hierarchy Your main comparison should be most prominent Secondary variables should enhance, not distract Use muted colors for less important distinctions Consider your audience More variables = more cognitive load Include clear titles that state the finding Make legends and labels crystal clear Test different approaches Try both color and faceting Experiment with transparency and jittering Ask: “What story is clearest?” 12.7 Summary Visualizing relationships requires choosing the right tool for your data: Boxplots excel at comparing continuous distributions across categories Scatterplots reveal relationships between continuous variables Faceting allows detailed comparisons across subgroups Color and shape can encode additional variables, but use judiciously The key is to build visualizations layer by layer: Start with the core relationship Add visual elements that enhance understanding Use facets when single plots become cluttered Always prioritize clarity over complexity Remember: The goal isn’t to impress with complicated graphics, but to reveal patterns that lead to understanding. 12.8 Study Questions 12.8.1 Understanding Plot Types What are the five key pieces of information shown in a boxplot? How do you interpret each one? Why might overlapping points be a problem in a scatterplot? What are two ways to address this issue? You have data on voter turnout (continuous, 0-100%) and education level (categorical: “High School”, “College”, “Graduate”). Which type of plot would you use and why? Write the ggplot2 code to create a boxplot showing income (y-axis) by political party (x-axis), with different colors for gender. 12.8.2 Using Facets What does facet_wrap(~variable) do? What does the tilde (~) symbol mean in this context? When would you use facet_wrap() instead of just using color to distinguish groups? 12.8.3 Interpreting Visualizations In the Congress age boxplot showing both chambers and incumbency, what are three key insights you can draw? Looking at the movie scatterplot, what does it mean when a point falls far above the diagonal line? Far below? 12.8.4 Practical Applications You want to explore whether the relationship between campaign spending and vote share differs by type of election (primary vs. general). Describe the visualization you would create. 12.8.5 Code Challenges Modify this scatterplot code to add 30% transparency and use faceting by mpaa_rating: ggplot(data = MovieData) + geom_point(aes(x = critics_score, y = audience_score)) "],["from-samples-to-inference-estimates-estimators-and-sampling-distributions.html", "13 From Samples to Inference: Estimates, Estimators, and Sampling Distributions 13.1 Learning Objectives 13.2 What Is This For? 13.3 Learning note 13.4 The Fundamental Problem: Parameters We Can’t See 13.5 Estimators: Rules for Making Guesses 13.6 Properties of Good Estimators 13.7 The Magic of Sampling Distributions 13.8 The Central Limit Theorem: Nature’s Gift to Statistics 13.9 From Theory to Practice: Estimating the Standard Error 13.10 Notation Summary 13.11 Summary: The Big Picture 13.12 Study Questions", " 13 From Samples to Inference: Estimates, Estimators, and Sampling Distributions 13.1 Learning Objectives Understand what estimates and estimators are and how they differ from population parameters Learn the three key properties of good estimators: unbiasedness, consistency, and efficiency Understand what a sampling distribution is and how it differs from sample and population distributions Grasp the Central Limit Theorem and its implications for statistical inference Master the notation and terminology for parameters, estimators, and estimates 13.2 What Is This For? In political science, we rarely have data on entire populations. We can’t survey every American voter, interview every member of Congress throughout history, or measure democracy levels in every country at every point in time. Instead, we work with samples - subsets of the population we’re interested in. But how do we move from what we observe in our sample to claims about the broader population? How confident can we be that the average Biden approval rating in our survey reflects the true sentiment of all Americans? This session introduces the theoretical foundation that makes this leap possible. We’ll start with a fundamental question: When we calculate a statistic from our sample (like a mean), how close is it likely to be to the true population value? The answer involves understanding estimators, their properties, and the remarkable patterns that emerge when we think about all possible samples we could have drawn. By the end, you’ll understand why statistical inference works and be ready to quantify uncertainty in your estimates. 13.3 Learning note Below, we include R code that we are using to illustrate our points. Some of this is fairly advanced - too advanced for this class. This is there to show you that the plots we are showing you can be generated using the simple tools of random sampling. But don’t worry too much about the code for now. Focus on the concepts we are trying to teach in the text. We’ll return to (some of) the R issues later in the class in more details. 13.4 The Fundamental Problem: Parameters We Can’t See Let’s start with a concrete question that matters for understanding American politics: What percentage of Americans approve of the the democratic nominee for President? Since we will be working from data from 2020, the specific question is: What percentage of Americans approve of Joe Biden? This seems like a simple question, but it highlights a fundamental challenge in statistics. The true population parameter - let’s call it μ (the Greek letter “mu”) - represents the actual average Biden feeling thermometer rating among all American adults. This is a fixed number that exists in reality. But we can’t observe it directly because we can’t survey all 300+ million American adults. Instead, we have a sample. Let’s load the 2020 American National Election Study (ANES) data and explore: # Load necessary packages library(tidyverse) # Load the ANES 2020 data anes2020 &lt;- read.csv(&quot;anes2020.csv&quot;) # Look at the Biden feeling thermometer summary(anes2020$biden.th) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 15.00 55.00 49.24 85.00 100.00 220 # How many respondents do we have? n_respondents &lt;- sum(!is.na(anes2020$biden.th)) cat(&quot;We have Biden ratings from&quot;, n_respondents, &quot;respondents\\n&quot;) ## We have Biden ratings from 8060 respondents # Calculate our sample mean sample_mean &lt;- mean(anes2020$biden.th, na.rm = TRUE) cat(&quot;Sample mean Biden rating:&quot;, round(sample_mean, 2)) ## Sample mean Biden rating: 49.24 Now here’s the key insight: Our sample mean of about 49.4 is not the same as the true population parameter μ. It’s our best guess, but it’s just one possible value we could have gotten. If we had surveyed a different 8,000 Americans, we would have gotten a different number. This gap between what we can calculate (sample statistics) and what we want to know (population parameters) is the fundamental problem of statistical inference. Let’s introduce some notation to keep things clear: μ (mu) = true population mean (unknown) x̄ (x-bar) = sample mean (what we calculated) σ (sigma) = true population standard deviation (unknown) s = sample standard deviation (what we can calculate) 13.5 Estimators: Rules for Making Guesses An estimator is simply a rule or formula for calculating an estimate from data. Think of it as a recipe: given some data, an estimator tells you how to combine those ingredients to produce an estimate. The sample mean is one estimator, but it’s not the only one. Let’s explore different ways we could estimate the population’s average Biden rating: # Different estimators for central tendency biden_clean &lt;- na.omit(anes2020$biden.th) # Estimator 1: Sample mean (uses all data) est1_sample_mean &lt;- mean(biden_clean) # Estimator 2: Sample median (uses middle value) est2_sample_median &lt;- median(biden_clean) # Estimator 3: First observation only est3_first_obs &lt;- biden_clean[1] # Estimator 4: Largest value in sample est4_maximum &lt;- max(biden_clean) # Estimator 5: Mean of first 10% of observations n_10pct &lt;- floor(length(biden_clean) * 0.1) est5_partial_mean &lt;- mean(biden_clean[1:n_10pct]) # Compare our estimates cat(&quot;Estimator 1 (Sample mean):&quot;, round(est1_sample_mean, 2), &quot;\\n&quot;) ## Estimator 1 (Sample mean): 49.24 cat(&quot;Estimator 2 (Sample median):&quot;, round(est2_sample_median, 2), &quot;\\n&quot;) ## Estimator 2 (Sample median): 55 cat(&quot;Estimator 3 (First observation):&quot;, est3_first_obs, &quot;\\n&quot;) ## Estimator 3 (First observation): 0 cat(&quot;Estimator 4 (Maximum):&quot;, est4_maximum, &quot;\\n&quot;) ## Estimator 4 (Maximum): 100 cat(&quot;Estimator 5 (10% mean):&quot;, round(est5_partial_mean, 2), &quot;\\n&quot;) ## Estimator 5 (10% mean): 49.17 These estimators give us very different estimates! Some seem reasonable, others ridiculous. But they’re all valid estimators in the technical sense - they’re all rules for producing a number from data. The crucial insight: An estimator is the rule; an estimate is the number you get when you apply that rule to your specific sample. 13.6 Properties of Good Estimators What makes some estimators better than others? We evaluate estimators based on three key properties. 13.6.1 Unbiasedness: Right on Average An estimator is unbiased if it produces the correct answer on average across all possible samples. This doesn’t mean it’s right in any particular sample, just that it’s not systematically too high or too low. To illustrate this concept, let’s do something we normally can’t do in real research: pretend our ANES data is the entire population, then repeatedly sample from it. Then we can see how well each estimator does on average. # Treat our ANES data as the &quot;population&quot; population &lt;- biden_clean true_mean &lt;- mean(population) cat(&quot;&#39;True&#39; population mean:&quot;, round(true_mean, 2), &quot;\\n\\n&quot;) ## &#39;True&#39; population mean: 49.24 # Take many samples and apply different estimators n_simulations &lt;- 1000 sample_size &lt;- 500 # Storage for results results_mean &lt;- numeric(n_simulations) results_median &lt;- numeric(n_simulations) results_max &lt;- numeric(n_simulations) results_first &lt;- numeric(n_simulations) # Simulation loop set.seed(123) # For reproducibility for (i in 1:n_simulations) { # Draw a random sample sample_i &lt;- sample(population, size = sample_size, replace = FALSE) # Apply different estimators results_mean[i] &lt;- mean(sample_i) results_median[i] &lt;- median(sample_i) results_max[i] &lt;- max(sample_i) results_first[i] &lt;- sample_i[1] } What the code above does is sample 500 observations from the larger survey (where we have 8,000) observations and then repeats that process 1,000 times. So now we have 1,000 estimates to look at using four different “recipes:” the sample mean, the median, the maximum observation, and the first observation. In essence, we are imagining conducting 1,000 different surveys (each with a sample size of 500). We can then look at how each “recipe” performs. For bias, we are particularly interesting at how accurate each estimate is on average. Specifically, we want to know how the mean estimate approximates the “true” parameter 49.24. # Compare average performance cat(&quot;Average across all samples:\\n&quot;) ## Average across all samples: cat(&quot;Sample mean estimator:&quot;, round(mean(results_mean), 2), &quot;(Bias:&quot;, round(mean(results_mean) - true_mean, 2), &quot;)\\n&quot;) ## Sample mean estimator: 49.24 (Bias: 0 ) cat(&quot;Sample median estimator:&quot;, round(mean(results_median), 2), &quot;(Bias:&quot;, round(mean(results_median) - true_mean, 2), &quot;)\\n&quot;) ## Sample median estimator: 54.85 (Bias: 5.61 ) cat(&quot;Maximum estimator:&quot;, round(mean(results_max), 2), &quot;(Bias:&quot;, round(mean(results_max) - true_mean, 2), &quot;)\\n&quot;) ## Maximum estimator: 100 (Bias: 50.76 ) cat(&quot;First observation estimator:&quot;, round(mean(results_first), 2), &quot;(Bias:&quot;, round(mean(results_first) - true_mean, 2), &quot;)\\n&quot;) ## First observation estimator: 49.06 (Bias: -0.18 ) Here we see that the arithmetic mean (x-bar) and the strategy of just using the first person in our sample are both highly accurate on average – they are unbiased. The other choices, however, are biased. The median is a bit too high on average and the maximum is way off. We can visualize this by plotting a histogram of the estiamtes from each of our 1,000 samples. par(mfrow = c(2, 2)) hist(results_mean, breaks = 30, main = &quot;Sample Mean (Unbiased)&quot;, xlab = &quot;Estimate&quot;, xlim = c(0, 100)) abline(v = true_mean, col = &quot;red&quot;, lwd = 2) hist(results_first, breaks = 30, main = &quot;First Observation (Unbiased but noisy)&quot;, xlab = &quot;Estimate&quot;, xlim = c(0, 100)) abline(v = true_mean, col = &quot;red&quot;, lwd = 2) hist(results_median, breaks = 30, main = &quot;Sample Median (Nearly Unbiased)&quot;, xlab = &quot;Estimate&quot;, xlim = c(0, 100)) abline(v = true_mean, col = &quot;red&quot;, lwd = 2) hist(results_max, breaks = 30, main = &quot;Maximum (Biased)&quot;, xlab = &quot;Estimate&quot;, xlim = c(0, 100)) abline(v = true_mean, col = &quot;red&quot;, lwd = 2) Notice that the sample mean and first observation estimators are centered on the true value (unbiased), while the median and the maximum estimators are systematically too high (biased). 13.6.2 Efficiency: Less Variable We also have a preference for estimators that are less noisy. In the language of statistics, we want estimators that ahave less variance. This is efficiency. Estimators with lower variance are called more efficient. Even though both the sample mean and the “first observation” estimator are unbiased, look at how differently they perform in terms of efficiency. # Compare variance of unbiased estimators cat(&quot;Standard deviation of estimates (n = 500):\\n&quot;) ## Standard deviation of estimates (n = 500): cat(&quot;Sample mean:&quot;, round(sd(results_mean), 2), &quot;\\n&quot;) ## Sample mean: 1.5 cat(&quot;First observation:&quot;, round(sd(results_first), 2), &quot;\\n&quot;) ## First observation: 34.45 cat(&quot;\\nThe sample mean is about&quot;, round(sd(results_first)/sd(results_mean), 1), &quot;times more efficient!\\n&quot;) ## ## The sample mean is about 22.9 times more efficient! Think about this intuitively: the sample mean uses all the information in your data, while the first observation throws away 99.8% of it. More information → less uncertainty → more efficiency. 13.6.3 Consistency: Better with More Data An estimator is consistent if it gets closer to the true value as sample size increases. When the sample size approaches infinity, its accuracy is guaranteed. For unbiased estimators, if they become more efficient as the sample size increases they are also consistent. In our example, only the mean (x-bar) and median are consistent, although we tend to prefer the mean because it is also unbiased. 13.7 The Magic of Sampling Distributions Now we come to one of the most important concepts in statistics: the sampling distribution. Understanding this concept is crucial because it bridges the gap between the single sample we have and the claims we want to make about the population. 13.7.1 What Is a Sampling Distribution? A sampling distribution is the distribution of values we would get if we calculated our statistic (like a sample mean) from many possible samples of a given size from the population. Here’s the key idea: Imagine you could take thousands of different samples from the American public, each with 500 people. Each sample would give you a slightly different average Biden approval rating. If you collected all these different sample means and made a histogram, that would be the sampling distribution of the sample mean. Think of it this way. When you flip a coin once, you get heads or tails. But if you flip a coin 10 times and count the heads, you might get 3, 4, 5, 6, or 7 heads. If you repeat this “flip 10 times and count” process thousands of times, the distribution of those counts is like a sampling distribution. In our Biden approval example one sample of 500 people might give a mean rating of 48.2. Another sample of 500 different people might give 50.7. Yet another might give 49.1. The sampling distribution describes the pattern of all these possible sample means. It tells us: (i) what values are most likely (usually clustered around the true population mean); (ii) How much variability to expect (how spread out the sample means are); (iii) the shape of this variability (remarkably, it’s usually bell-shaped!). 13.7.2 Why This Matters The sampling distribution is mostly theoretical - in real research, we only get one sample. So why do we care about all these hypothetical other samples? Because the sampling distribution allows us to answer the critical questions about the world using only the sample we have. Even though we only have one sample mean, if we understand the sampling distribution, we can say things like: “If the true population mean were 45, it would be very unlikely to get a sample mean as high as 52”. In later classes, sampling distibutions are what we use to create key quantities like confidence intervals and p-values. Indeed, this is the foundation of every confidence interval and hypothesis test you’ll ever see. The sampling distribution transforms a single number (our sample statistic) into a statement about uncertainty and probability. 13.7.3 Distinguishing Three Types of Distributions This is where many students get confused, so let’s be crystal clear about the three different distributions we’re dealing with: 13.7.3.1 1. Population Distribution What it is: The distribution of the variable across every single individual in the entire population. In our example: If we could measure the Biden feeling thermometer for all 300+ million American adults and make a histogram, that would be the population distribution. Key features: This is what we’re trying to learn about, but we almost never see this distribution (because we can’t measure everyone). But it also has fixed parameter we are interested in: a true mean (μ) and true standard deviation (σ). In addition, its shape could be anything - normal, skewed, bimodal, etc. 13.7.3.2 2. Sample Distribution What it is: The distribution of the variable in your actual data - the one sample you collected. In our example: The histogram of Biden ratings from the 8,000 people in our ANES survey. Key features: This is what we actually observe, but it’s an imperfect reflection of the population distribution. It also has statistics we can calculate: sample mean (x̄) and sample standard deviatio (s). If the sample is big enough, its shape is usually simiilar to the population, but with random variation. 13.7.3.3 3. Sampling Distribution What it is: The distribution of a sample statistic (like the mean) across all possible samples of a given size. In our example: If we took millions of different samples of 500 Americans each and calculated the mean Biden rating for each sample, the histogram of all those means would be the sampling distribution. Key features: This is a theoretical construct (we imagine it, we don’t observe it). Each “data point” is a sample statistic, not an individual observation and it it describes how much our estimate varies from sample to sample. Thanks to the central limit theorem (discussed next), its shape is approximately normal for large samples. It also has a mean equal to the population parameter and a standard deviation called the standard error. 13.7.3.4 A Concrete Analogy Think of it like studying student heights at WashU: Population distribution: A histogram showing the height of every single WashU student. Sample distribution: A histogram showing the heights of the 200 students you actually measured Sampling distribution: A histogram showing the average heights from thousands of different groups of 200 students The critical insight: The population and sample distributions are about individual measurements. The sampling distribution is about the behavior of our estimator across many hypothetical samples. 13.8 The Central Limit Theorem: Nature’s Gift to Statistics Here’s where things get almost magical. The Central Limit Theorem (CLT) tells us that: For large samples, the sampling distribution of the sample mean is approximately normal, regardless of the shape of the population distribution. This is remarkable! Even though Biden ratings are bimodal (people tend to love or hate him), the sampling distribution of the mean is bell-shaped. Let’s demonstrate this with different population distributions: # Demonstrate CLT with different population shapes set.seed(126) # Create different population distributions # Uniform distribution pop_uniform &lt;- runif(10000, min = 0, max = 100) # Exponential (very skewed) distribution pop_skewed &lt;- rexp(10000, rate = 1/30) pop_skewed &lt;- pmin(pop_skewed, 100) # Cap at 100 # Bimodal distribution (like our Biden data) pop_bimodal &lt;- c(rnorm(5000, mean = 20, sd = 10), rnorm(5000, mean = 80, sd = 10)) pop_bimodal &lt;- pmax(pmin(pop_bimodal, 100), 0) # Keep between 0-100 # Function to demonstrate CLT demonstrate_clt &lt;- function(population, pop_name, n = 500, n_samples = 1000) { sample_means &lt;- replicate(n_samples, mean(sample(population, n))) par(mfrow = c(1, 2)) # Population distribution hist(population, breaks = 50, main = paste(pop_name, &quot;Population&quot;), xlab = &quot;Value&quot;, probability = TRUE) # Sampling distribution hist(sample_means, breaks = 30, main = paste(&quot;Sampling Distribution of Mean&quot;), xlab = &quot;Sample Mean&quot;, probability = TRUE) # Add normal curve curve(dnorm(x, mean = mean(sample_means), sd = sd(sample_means)), add = TRUE, col = &quot;red&quot;, lwd = 2) } demonstrate_clt(pop_uniform, &quot;Uniform&quot;) demonstrate_clt(pop_skewed, &quot;Skewed&quot;) demonstrate_clt(pop_bimodal, &quot;Bimodal&quot;) The CLT tells us specifically that: \\[\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\] Or in words: the sample mean follows a normal distribution with: Center: The true population mean (μ) Spread: The population variance divided by sample size (σ²/n) The standard deviation of the sampling distribution has a special name: the standard error. \\[SE(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}\\] 13.9 From Theory to Practice: Estimating the Standard Error There’s one catch: we don’t know σ (the population standard deviation). But we can estimate it with the sample standard deviation (s). That is, we jsut “plug in” our sample statistics and assume they are good enough. This is the foundation of confidence intervals, which we’ll cover soon! 13.10 Notation Summary Let’s consolidate all the notation we’ve introduced: Concept Population Sample Sampling Distribution Mean μ (unknown) x̄ (calculated) μ (center of sampling dist.) Standard deviation σ (unknown) s (calculated) σ/√n (standard error) Variance σ² (unknown) s² (calculated) σ²/n Key notation principles: Greek letters (μ, σ) = population parameters (usually unknown) Roman letters (x̄, s) = sample statistics (what we calculate) Hats (μ̂, σ̂) = estimates of population parameters 13.11 Summary: The Big Picture We started with a simple question: How can we learn about a population from just a sample? The answer involves several key ideas: Estimators are recipes - They tell us how to calculate estimates from data Good estimators have nice properties - Unbiased (right on average), consistent (better with more data), efficient (low variance) Sampling distributions describe uncertainty - They show how our estimate would vary across different samples The CLT makes inference possible - It tells us the sampling distribution is approximately normal, centered on the true population mean, and with variation we can approximate. Standard errors quantify precision - They measure how much our estimate typically varies These concepts form the foundation for all statistical inference. In the next class, we’ll use these ideas to construct confidence intervals and test hypotheses. 13.12 Study Questions 13.12.1 Conceptual Understanding Explain the difference between a parameter, an estimator, and an estimate using the Biden approval example. You calculate the mean age from a random sample of 100 WashU students and get 20.3 years. Is 20.3 a parameter, an estimator, or an estimate? What about the rule “sum all ages and divide by n”? Why can the sample maximum never be an unbiased estimator of the population mean? Explain in your own words what it means for an estimator to be consistent. Give an example of an estimator that is unbiased but not consistent. Two estimators are both unbiased and consistent. How would you choose between them? 13.12.2 Sampling Distributions What is the difference between a sample distribution and a sampling distribution? If you increase sample size from 100 to 400, by what factor does the standard error decrease? Explain why. The CLT says the sampling distribution of x̄ is approximately normal. What three things do we need to know to fully describe this normal distribution? 13.12.3 Application You survey 200 voters and find 55% support a candidate. Your friend surveys a different 200 voters and finds 51% support. Should you be surprised by this difference? (Hint: think about the standard error) A researcher studying county-level poverty rates decides to estimate the national rate using only the poorest county’s rate. What properties will this estimator have? Is it useful? "],["taking-random-samples-in-r.html", "14 Taking Random Samples in R 14.1 Learning Objectives 14.2 What Is This For? 14.3 Basic Sampling from Vectors 14.4 Controlling Randomness with set.seed() 14.5 Sampling from Data Frames 14.6 Sampling Without Replacement vs. With Replacement 14.7 Quick Reference 14.8 Summary 14.9 Study Questions", " 14 Taking Random Samples in R 14.1 Learning Objectives Take random samples from vectors and data frames Control randomness with set.seed() Use sample() for simple random sampling Use dplyr functions for sampling data frames Take samples with and without replacement 14.2 What Is This For? When working with large datasets, you often need to take random samples for analysis, testing code, or creating training/test sets. R provides several ways to randomly select observations. This tutorial covers the essential functions you’ll need for sampling. 14.3 Basic Sampling from Vectors 14.3.1 The sample() Function The workhorse function for random sampling in R is sample(). Let’s start with a simple vector: # Create a vector of numbers numbers &lt;- 1:10 print(numbers) ## [1] 1 2 3 4 5 6 7 8 9 10 # Take a random sample of 5 numbers sample(numbers, size = 5) ## [1] 9 1 3 6 5 # Run it again - you&#39;ll get different results! sample(numbers, size = 5) ## [1] 3 8 2 5 7 14.3.2 Key Arguments for sample() x: The vector to sample from size: How many elements to sample replace: Whether to sample with replacement (default is FALSE) prob: Optional weights for each element Note: If x is numeric, and has length of 1, then 1:x will be used # Sample all elements in random order (shuffle) sample(numbers) ## [1] 8 6 3 4 10 9 7 2 1 5 # Sample with replacement (elements can appear multiple times) sample(numbers, size = 15, replace = TRUE) ## [1] 6 3 2 4 5 7 5 9 3 9 8 6 5 6 9 14.4 Controlling Randomness with set.seed() Random samples are different each time by design. But for reproducible research, you need consistent results. Use set.seed() to fix the random number generator: Without set.seed - different results each time sample(1:10, size = 3) ## [1] 6 2 1 sample(1:10, size = 3) ## [1] 3 5 10 With set.seed - same results if you use the same seed set.seed(123) sample(1:10, size = 3) ## [1] 3 10 2 set.seed(123) # Same seed = same results sample(1:10, size = 3) ## [1] 3 10 2 Important: The seed number doesn’t matter (123, 42, 8675309, etc.). What matters is using the same seed for reproducibility. 14.5 Sampling from Data Frames 14.5.1 Method 1: Using Base R # Create a small example data frame students &lt;- data.frame( id = 1:20, name = paste0(&quot;Student&quot;, 1:20), score = round(rnorm(20, mean = 75, sd = 10)), year = sample(c(&quot;Freshman&quot;, &quot;Sophomore&quot;, &quot;Junior&quot;, &quot;Senior&quot;), 20, replace = TRUE) ) # View first few rows head(students) ## id name score year ## 1 1 Student1 76 Freshman ## 2 2 Student2 76 Sophomore ## 3 3 Student3 92 Junior ## 4 4 Student4 80 Junior ## 5 5 Student5 62 Senior ## 6 6 Student6 68 Freshman To sample rows from a data frame using base R, we need a little trick: randomly sample row indices and subset the data frame with it. # Sample 5 random rows set.seed(42) row_indices &lt;- sample(1:nrow(students), size = 5) students_sample &lt;- students[row_indices, ] print(students_sample) ## id name score year ## 17 17 Student17 70 Senior ## 5 5 Student5 62 Senior ## 1 1 Student1 76 Freshman ## 10 10 Student10 79 Senior ## 4 4 Student4 80 Junior 14.5.2 Method 2: Using dplyr (Recommended) The dplyr package provides cleaner functions for sampling. Similar to the sample() function, sample_n() and sample_frac() function accepts these arguments: tbl (data.frame / tibble): A population data size (integer): The size of a sample for sample_n, and the fraction of rows for sample_frac replace (Boolean): Generate sample with replacement? (default = FALSE) weight: Optional weights for each element library(dplyr) # sample_n: Sample a specific number of rows set.seed(42) students %&gt;% sample_n(size = 5) ## id name score year ## 1 17 Student17 70 Senior ## 2 5 Student5 62 Senior ## 3 1 Student1 76 Freshman ## 4 10 Student10 79 Senior ## 5 4 Student4 80 Junior # sample_frac: Sample a fraction of rows set.seed(42) students %&gt;% sample_frac(size = 0.25) # 25% of rows ## id name score year ## 1 17 Student17 70 Senior ## 2 5 Student5 62 Senior ## 3 1 Student1 76 Freshman ## 4 10 Student10 79 Senior ## 5 4 Student4 80 Junior 14.6 Sampling Without Replacement vs. With Replacement 14.6.1 Without Replacement (Default) Each observation can only be selected once (draw a ball from the urn, and do not put it back): # Try to sample more than available - ERROR! # sample(1:5, size = 10) # This would cause an error # Maximum sample size = population size sample(1:5, size = 5) # This works - it&#39;s a permutation ## [1] 2 5 1 4 3 14.6.2 With Replacement Observations can be selected multiple times (draw a ball and put it back into the urn): # Sample 10 times from 5 options set.seed(100) sample(1:5, size = 10, replace = TRUE) ## [1] 2 3 1 2 4 4 2 3 2 5 # Notice some numbers appear multiple times table(sample(1:5, size = 10, replace = TRUE)) ## ## 1 2 3 4 ## 1 2 3 4 14.7 Quick Reference Function Purpose Example sample(x, n) Sample n items from vector x sample(1:100, 10) sample(x) Random permutation of x sample(letters) set.seed(n) Fix random number generator set.seed(123) sample_n(df, n) Sample n rows from data frame df %&gt;% sample_n(50) sample_frac(df, p) Sample fraction p of rows df %&gt;% sample_frac(0.1) 14.8 Summary Use sample() for vectors, sample_n()/sample_frac() for data frames Always use set.seed() for reproducible results Specify replace = TRUE to allow repeated selections Be careful with edge cases (empty vectors, size = 1) This foundation will help you work with sampling distributions, bootstrap methods, and cross-validation in more advanced analyses. 14.9 Study Questions 14.9.1 Understanding the Basics What does the sample() function do? What are its main arguments? If you have a vector x &lt;- 1:20 and run sample(x, size = 5) twice, will you get the same results? Why or why not? What’s the difference between sample(1:10, size = 5) and sample(1:10)? Explain what set.seed() does and why it’s important for reproducible research. 14.9.2 Sampling with and without Replacement What does replace = FALSE mean in the context of sampling? What about replace = TRUE? If you have a vector with 10 elements, what’s the maximum size you can sample without replacement? With replacement? Write R code to simulate rolling a six-sided die 20 times. 14.9.3 Working with Data Frames What’s the difference between sample_n() and sample_frac()? When would you use each? You have a data frame called patients with 1000 rows. Write code to: Take a random sample of 50 patients Take a random 10% sample of patients If you run this code, what happens and why? r df &lt;- data.frame(x = 1:10, y = letters[1:10]) sample(df, size = 5) 14.9.4 Practical Applications You’re analyzing survey data and want to test your code on a smaller subset first. Write code to create a random sample of 100 rows from a data frame called survey_data. Explain why this code might give unexpected results: r my_sample1 &lt;- sample(1:100, 20) mean(my_sample1) my_sample2 &lt;- sample(1:100, 20) mean(my_sample2) How would you fix it to ensure both samples are identical? What will sample(10) return? Is this the same as sample(c(10))? Explain. "],["probability-and-probability-distributions.html", "15 Probability and Probability Distributions 15.1 Learning Objectives 15.2 What Is This For? 15.3 Part I: Foundations of Probability 15.4 Part II: The Normal Distribution 15.5 Part III: Working with Probability in R 15.6 Part IV: Looking Ahead 15.7 Summary 15.8 Study Questions for Probability and Probability Distributions", " 15 Probability and Probability Distributions 15.1 Learning Objectives Understand fundamental concepts of probability and how they apply to data analysis Distinguish between discrete and continuous probability distributions Master the normal distribution and its properties Calculate and interpret z-scores and their relationship to standard deviations Use R functions (pnorm, qnorm, dnorm) to calculate probabilities and quantiles Apply the empirical rule to normal distributions 15.2 What Is This For? So far in this course, we’ve learned to describe data - calculating means, creating visualizations, measuring spread. But description is only the beginning. The real power of statistics comes from making inferences: using what we observe in our sample to make claims about the broader population. To make that leap from description to inference, we need to understand uncertainty. When a poll reports that 52% of voters support a candidate “with a margin of error of ±3%,” that margin comes from probability theory. When researchers claim their results are “statistically significant,” they’re making a probability statement. When a forecaster says a candidate has a 70% chance of winning, they’re using probability distributions. This session introduces the foundation that makes all of this possible – probability. We’ll start with simple probability, build up to the normal distribution (the most important distribution in statistics), and learn how to calculate probabilities in R. This only touches on the subject, and I will focus on concepts over formal definitions or mathematics. But having some intuition about probability will help you understand concepts later in the class. And you will certainly need to know how to calculate probabilities in R. 15.3 Part I: Foundations of Probability 15.3.1 What is Probability? At its core, probability is the relative frequency of occurrence for some outcome if a process is repeated many times under similar conditions. It’s a number between 0 (impossible) and 1 (certain) that quantifies how likely something is to happen. And if you consider all of the possibe events that could happen, the total probability should sum to 1. Let’s start with a concrete example that builds intuition: rolling two dice. 15.3.1.1 Example: Two Dice When you roll two six-sided dice, there are 36 possible outcomes: Die 1 → 1 2 3 4 5 6 1 (1,1) (1,2) (1,3) (1,4) (1,5) (1,6) 2 (2,1) (2,2) (2,3) (2,4) (2,5) (2,6) 3 (3,1) (3,2) (3,3) (3,4) (3,5) (3,6) 4 (4,1) (4,2) (4,3) (4,4) (4,5) (4,6) 5 (5,1) (5,2) (5,3) (5,4) (5,5) (5,6) 6 (6,1) (6,2) (6,3) (6,4) (6,5) (6,6) To find the probability of any sum, we count how many ways we can get that sum and divide by 36. For example, there is only one way to end up with two dice that sum to two. So the probability of two is 1/36. Meanwhile, there are 6 ways to end up with a seven, so the probability of seven is 6/36. We can do that for all possible numbers. This bar chart shows a discrete probability distribution - each possible outcome has a specific probability, and all probabilities sum to 1. The outcome (the sum of two die) is on the x-axis. The probability that even twill occur is on the y-axis. 15.3.2 Discrete vs. Continuous Distributions Discrete distributions deal with countable outcomes: Number of yes votes in the Senate (0, 1, 2, …, 100) Number of supreme court justices appointed by a president Survey responses on a 1-5 scale Continuous distributions deal with measurements that can take any value in a range: Voter approval ratings (any percentage from 0% to 100%) Campaign spending (any dollar amount) Time until election results are called With continuous distributions, we don’t ask “What’s the probability of exactly 51.7%?”. Since there are infinitely many outcomes, the probability of any specific number is actually 0! Instead, we ask about ranges: “What’s the probability of getting between 51% and 52%?” 15.3.3 Formal Properties of Probability Distributions Now that we have intuition about probability, let’s formalize these concepts. Understanding these properties helps us verify that our calculations are correct and builds the foundation for statistical inference. 15.3.3.1 Properties of Discrete Probability Distributions For discrete random variables (like the sum of two dice or number of Senate votes), we use a probability mass function (PMF) to assign probabilities to each possible outcome. Formal Definition: If X is a discrete random variable that can take values x₁, x₂, x₃, …, then P(X = xᵢ) gives the probability of each outcome. Two fundamental properties must always hold: Non-negativity: Every probability must be between 0 and 1 \\[0 \\leq P(X = x_i) \\leq 1 \\text{ for all } i\\] Probabilities sum to 1: The total probability across all possible outcomes equals 1 \\[\\sum_{i} P(X = x_i) = 1\\] Let’s verify these properties with our dice example: # Verify the formal properties for our dice distribution dice_probs &lt;- data.frame( sum = 2:12, ways = c(1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1), probability = c(1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1) / 36 ) # Property 1: All probabilities between 0 and 1? all(dice_probs$probability &gt;= 0 &amp; dice_probs$probability &lt;= 1) ## [1] TRUE # Property 2: Do probabilities sum to 1? sum(dice_probs$probability) ## [1] 1 15.3.3.2 Properties of Continuous Probability Distributions For continuous random variables (like approval ratings or voter turnout), we use a probability density function (PDF) denoted as f(x). Key insight: For continuous distributions, the probability of any exact value is 0. Instead, probabilities are “areas under the curve.” Formal Definition: If X is a continuous random variable with PDF f(x), then: - f(x) gives the density at point x (height of the curve) - P(a &lt; X &lt; b) = area under f(x) between a and b Two fundamental properties must always hold: Non-negativity: The density function must be non-negative everywhere \\[f(x) \\geq 0 \\text{ for all } x\\] Total area equals 1: The total area under the curve equals 1 \\[\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\] If that integration symbol seems intimidating, don’t worry about it. We will never be doing these calculations by hand. Instead, we will let R handle these calculations for us. The key points to undersand are that: Probability for any range of values is the “area under the curve” The total probability “under the curve” for all possible values must sum to 1. 15.3.4 Parameters of Distributions Every probability distribution has characteristics that summarize its behavior. The two most important are: Mean (μ, “mu”): The expected value or long-run average Variance (σ², “sigma squared”): How spread out the values are Critical Distinction: Parameters vs. Statistics This is one of the most important conceptual distinctions in statistics, and it’s easy to confuse. Let’s break it down: Population Parameters (Greek letters) These are the TRUE values for the entire population We almost never know these values They are fixed, unchanging numbers We use Greek letters to denote them: μ (mu) = true population mean σ² (sigma squared) = true population variance σ (sigma) = true population standard deviation Sample Statistics (Roman letters) These are what we CALCULATE from our data They vary from sample to sample They are our best guesses at the population parameters We use Roman letters with special notation: x̄ (x-bar) = sample mean s² = sample variance s = sample standard deviation Why This Matters: An Example Imagine we want to know the average age of all U.S. voters (about 240 million people): μ = the TRUE average age of all 240 million voters (maybe 47.3 years) This is unknown unless we survey everyone (census) It’s a fixed number that exists in reality x̄ = the average age from our sample of 1,000 voters (maybe 46.8 years) This is what we actually calculate If we took a different sample, we’d get a different x̄ (maybe 47.9) We hope it’s close to μ, but it’s not exactly μ The entire field of statistical inference is about using what we can calculate (statistics like x̄) to make educated guesses about what we want to know (parameters like μ). This is why we need probability theory - to understand how much our sample statistics might vary from the true population parameters. 15.4 Part II: The Normal Distribution 15.4.1 Properties and Importance The normal distribution (also called the Gaussian distribution or bell curve) is the most important probability distribution in statistics. It appears everywhere because of the Central Limit Theorem, which says that sample averages (means) tend to follow a normal distribution regardless of the original data’s shape. Key properties of the normal distribution: Bell-shaped and perfectly symmetric around the mean Mean = Median = Mode (all at the center) Completely determined by two parameters: μ (mean) and σ (standard deviation) Total area under the curve = 1 (like all probability distributions) Tails technically extend infinitely but get very close to zero for values far from the mean 15.4.2 The Standard Normal and Z-Scores The standard normal distribution is a special case where μ = 0 and σ = 1. Any normal distribution can be converted to standard normal using: \\[Z = \\frac{X - \\mu}{\\sigma}\\] This transformation is called standardization, and the result is a z-score. 15.4.2.1 What Z-Scores Tell Us A z-score tells you how many standard deviations a value is from the mean: Z = 0: exactly at the mean Z = 1: one standard deviation above the mean Z = -2: two standard deviations below the mean 15.4.3 The Empirical Rule (68-95-99.7) For any normal distribution: ~68% of values fall within ±1σ of the mean ~95% of values fall within ±2σ of the mean ~99.7% of values fall within ±3σ of the mean This rule provides quick probability estimates without calculation. For example, if test scores are normally distributed with μ = 75 and σ = 10: About 68% score between 65 and 85 About 95% score between 55 and 95 Scores below 45 or above 105 are very rare (~0.3%) 15.5 Part III: Working with Probability in R 15.5.1 Key R Functions for the Normal Distribution R provides four essential functions for working with normal distributions: Function Purpose What It Returns Example dnorm(x) Density Height of the curve at point x dnorm(0) = 0.399 pnorm(x) Probability Area to the left of x (cumulative probability) pnorm(0) = 0.5 qnorm(p) Quantile The x value with p area to its left qnorm(0.5) = 0 rnorm(n) Random n random values from the distribution rnorm(100) generates 100 values Think of these as answering different questions: dnorm: “How tall is the curve here?” pnorm: “What’s the probability of getting less than x?” qnorm: “What value of x has probability p below it?” rnorm: “Give me random data from this distribution” 15.5.2 Calculating Probabilities with pnorm() The pnorm() function calculates cumulative probabilities - the area under the curve to the left of a value. Think of it as answering: “What percentage of the population falls below this value?” It is what is called the cumulative density function for the normal distribution. Intuitive Analogy: Imagine lining up everyone in order by height. pnorm() tells you what percentage of people you’d have to walk past before reaching someone of a specific height. # Example: Test scores in a class are normally distributed # The mean score is 45 with a standard deviation of 5 mu &lt;- 45 # Population mean atest score sigma &lt;- 5 # Population standard deviation threshold &lt;- 50 # We want to know: what % of students scored 50 or less # pnorm calculates the cumulative probability (area to the left) # It answers: &quot;What proportion of students scored below 40?&quot; prob &lt;- pnorm(threshold, mean = mu, sd = sigma) print(prob) ## [1] 0.8413447 This means about 84.1% of students scored below 50%, only 15.9% scored above 50%. Key Points about pnorm(): Always gives a number between 0 and 1 pnorm(mean) always equals 0.5 (half below, half above) Larger values → probabilities closer to 1 Smaller values → probabilities closer to 0 For probabilities to the right, use lower.tail = FALSE: # Sometimes we want P(X &gt; some value) instead of P(X &lt; some value) # Use lower.tail = FALSE to get the area to the RIGHT # What&#39;s the probability someone scores MORE than 50%? prob_right &lt;- pnorm(50, mean = 45, sd = 5, lower.tail = FALSE) print(prob_right) ## [1] 0.1586553 # Notice: prob + prob_right = 1.000 (they&#39;re complements!) We can also calculate the probability of an interval. We do this by subtracting. For example, what if we wanted to know what percent of people scored between 40 and 50. We can calculate the first the probability that people scored below 50%. Then we calculate the probability that they scored below 40. When we subtract these two, we get the probability that they fall in the middle. # For intervals, we need to subtract two probabilities # Example: What % of people have approval between 40% and 50%? prob_below_50 &lt;- pnorm(50, mean = 45, sd = 5) # P(X &lt; 50) prob_below_40 &lt;- pnorm(40, mean = 45, sd = 5) # P(X &lt; 40) prob_interval &lt;- prob_below_50 - prob_below_40 # P(40 &lt; X &lt; 50) print(round(prob_interval, 3)) ## [1] 0.683 The probability is about 68.3%. Visualization: To see this visually, think about what pnorm calculates when we put in 50 and what it looks like when we put in 40. When we subtract one from the other, we get the interval in between. 15.5.3 Finding Quantiles with qnorm() - The Inverse Problem The qnorm() function is the inverse of pnorm(). It is something called a quantile function for the normal distribution. This confuses many students, so let’s break it down: pnorm() asks: “Given a value, what’s the probability below it?” qnorm() asks: “Given a probability, what value has that much below it?” Think of it like this: pnorm() is like asking: “If someone is 6 feet tall, what percentile are they?” qnorm() is like asking: “What height represents the 90th percentile?” Another way to think about it: pnorm(): Value → Probability (forward direction) qnorm(): Probability → Value (reverse direction) Let’s see how pnorm and qnorm are inverses of each other mu &lt;- 45 # Test scores with mean 45 sigma &lt;- 5 # and standard deviation 5 # Start with a specific test value original_value &lt;- 48 # Someone with score of 48 # Step 1: Use pnorm to find what percentile this person is at # &quot;What percentage of people have score of 48 or below?&quot; prob &lt;- pnorm(original_value, mean = mu, sd = sigma) print(prob) ## [1] 0.7257469 We can see that about 72.6% of students scored below 48. We use qnorm to convert back from percentile to score. We ask, “What score corresponds to the 72.6th percentile?” back_to_value &lt;- qnorm(prob, mean = mu, sd = sigma) print(back_to_value) ## [1] 48 They’re inverses - we get back where we started! Common qnorm() Applications: There are three common ways we use qnorm. We use it to find specific percentils? We use it to find important “cutoffs.” We use it to find important middle ranges. Finding specific quantiles Let’s go back to our test example. What is the 75th percentile for this test? That is, what score would you need to have to score aboe 75% of other students? percentile_75 &lt;- qnorm(0.75, mean = mu, sd = sigma) print(percentile_75) ## [1] 48.37245 Finding “cutoffs” I can use this same idea to find important cutoffs at both the top and the bottom of the distribution. For instance, let’s say that I want to find all of the students in the class in the top 10%. I just need to find the “cutoff” that divides the top 10% from the bottom 90%. Since pnorm by default calculates probabilities below a quantile we can just do: top_10_cutoff &lt;- qnorm(0.90, mean = mu, sd = sigma) print(round(top_10_cutoff, 2)) ## [1] 51.41 Now I would just look for any student who scored higher than this threshold! Finding symmetric intervals Sometimes we might want to find symmetric intervals around the mean that include a certain percentage of the distribution. For instance, we might ask, “What range contains the middle 90% of scores?” Using the same ideas as before, we just calculate the following. # Middle 90% means 5% in each tail, so 5th and 95th percentiles lower_5 &lt;- qnorm(0.05, mean = mu, sd = sigma) # 5th percentile upper_95 &lt;- qnorm(0.95, mean = mu, sd = sigma) # 95th percentile print(paste0(&quot;Middle 90% of scores: &quot;, round(lower_5, 1), &quot; to &quot;, round(upper_95, 1))) ## [1] &quot;Middle 90% of scores: 36.8 to 53.2&quot; 15.5.4 Understanding dnorm(): The Height of the Curve While pnorm() gives us probabilities (areas), dnorm() is the density function - the height of the normal curve at any point. This doesn’t directly give us a probability, but it’s useful for: Drawing normal distributions Understanding where the curve is tallest (at the mean) # Simple example: Plot a normal distribution with mean=45, sd=5 mu &lt;- 45 sigma &lt;- 5 # Step 1: Create a sequence of x values # We need points along the x-axis where we&#39;ll calculate the height x &lt;- seq(from = mu - 4*sigma, # Start at 4 SDs below mean (40) to = mu + 4*sigma, # End at 4 SDs above mean (160) length.out = 100) # Use 100 points for smooth curve # Step 2: Calculate the height (density) at each x value using dnorm() y &lt;- dnorm(x, mean = mu, sd = sigma) # Step 3: Create a data frame for ggplot df &lt;- data.frame(x = x, y = y) # Step 4: Plot the normal distribution ggplot(df, aes(x = x, y = y)) + geom_line(color = &quot;blue&quot;, size = 1.5) + # Draw the curve geom_vline(xintercept = mu, # Add vertical line at mean color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) + annotate(&quot;text&quot;, # Add text annotation x = mu + 4, y = 0.025, label = &quot;Mean = 45\\n(peak of curve)&quot;, color = &quot;red&quot;, size = 3) + labs( title = &quot;Normal Distribution using dnorm()&quot;, subtitle = &quot;N(45, 5)&quot;, x = &quot;Value&quot;, y = &quot;Density (Height)&quot; ) + theme_minimal() + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(size = 11, color = &quot;gray50&quot;) ) The dnorm() function calculates the probability density function (PDF) of the normal distribution: \\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\] Where: \\(x\\) is the value where you want the height \\(\\mu\\) is the mean \\(\\sigma\\) is the standard deviation \\(e \\approx 2.718\\) is Euler’s number \\(\\pi \\approx 3.14159\\) This formula gives the height of the bell curve at any point \\(x\\). But you will never have to use this formula, because R will do it for you (or I will just give you the R output you need). 15.5.5 Generating Random Data with rnorm() The rnorm() function generates random values from a normal distribution. This is incredibly useful for: Simulations Testing code before you have real data Understanding sampling variability Bootstrap methods (advanced topic) # rnorm() generates random values from a normal distribution # Syntax: rnorm(n, mean, sd) where n is how many values you want # IMPORTANT: Set seed for reproducibility # Without set.seed(), you get different random numbers each time set.seed(123) # Any number works - this ensures we all get the same &quot;random&quot; results # Generate 1000 random test scores from N(45, 5) n_people &lt;- 1000 mu&lt;-45 # True population mean sigma&lt;-5 # True population standard deviation # Generate the random sample random_scores &lt;- rnorm(n = n_people, mean = mu, sd = sigma) print(random_scores[1:20]) # print first 20 randomly selected values ## [1] 42.19762 43.84911 52.79354 45.35254 45.64644 53.57532 47.30458 38.67469 41.56574 42.77169 ## [11] 51.12041 46.79907 47.00386 45.55341 42.22079 53.93457 47.48925 35.16691 48.50678 42.63604 15.6 Part IV: Looking Ahead 15.6.1 The t-Distribution: When Samples Are Small While the normal distribution is fundamental, real-world data analysis often involves small samples where we don’t know the population standard deviation. Enter the t-distribution: Key points about the t-distribution: Thicker tails = more probability for extreme values Degrees of freedom (df) = n - 1 for most applications As df increases, t-distribution → normal distribution Used for hypothesis testing with small samples (coming soon!) 15.7 Summary We’ve built the mathematical foundation for statistical inference: Probability quantifies uncertainty using numbers from 0 to 1 Discrete distributions assign probabilities to countable outcomes Continuous distributions describe probabilities over ranges The normal distribution is central to statistics - symmetric, bell-shaped, and determined by μ and σ Z-scores standardize values to “standard deviations from mean” The empirical rule gives quick probability estimates (68-95-99.7) R functions (pnorm, qnorm, dnorm) make probability calculations straightforward and are the cumulative distribution function (CDF), quantile function, and probability density function (PDF) for the normal distribution. This foundation enables us to move from describing what we see in our data to making inferences about populations, testing hypotheses, and quantifying uncertainty. When we combine this what what we have already learned about sampling distributions, we are now ready to start doing statistics. 15.8 Study Questions for Probability and Probability Distributions 15.8.1 Understanding Probability Concepts What is probability? What values can a probability take, and what do those values mean? When rolling two dice, why is 7 the most likely sum? How many ways can you get a sum of 7 versus a sum of 2? Explain the difference between discrete and continuous probability distributions. Why is the probability of getting exactly 51.7% in a continuous distribution technically 0? What question should we ask instead? 15.8.2 Parameters vs. Statistics What’s the difference between μ and x̄? Between σ and s? Why do we use different symbols? A pollster surveys 1,000 voters and finds that 52% support a candidate. Is 52% a parameter or a statistic? What about the true support level among all voters? 15.8.3 The Normal Distribution List three key properties of the normal distribution. What two parameters completely determine a normal distribution? How do changes in each parameter affect the shape? TRUE or FALSE: In a normal distribution, the mean, median, and mode are all equal. Explain why. 15.8.4 Z-Scores and Standardization If someone’s test score has a z-score of -1.5, what does this tell you about their performance? Convert these values to z-scores: X = 85, μ = 75, σ = 10 X = 60, μ = 75, σ = 10 If approval ratings are N(45, 8), what z-score corresponds to an approval rating of 53%? 15.8.5 The Empirical Rule State the empirical rule (68-95-99.7 rule). What assumption must be true for this rule to apply? If test scores follow N(70, 10), use the empirical rule to find approximately: What percentage of students score between 60 and 80? What percentage score above 90? What range contains the middle 95% of scores? 15.8.6 Working with R Functions Match each R function with what it calculates: pnorm() qnorm() dnorm() rnorm() A. Generates random values B. Finds the height of the curve C. Finds the value given a probability D. Finds the probability given a value What does pnorm(100, mean = 100, sd = 15) return? Explain why without running the code. If pnorm(30, mean = 25, sd = 5) returns 0.841, what would qnorm(0.841, mean = 25, sd = 5) return? Explain. 15.8.7 Calculating Probabilities in R Voter turnout in a district is normally distributed with mean 55% and standard deviation 8%. Write R code to find: The probability that turnout is less than 50% The probability that turnout is greater than 65% The probability that turnout is between 50% and 60% For the same distribution (N(55, 8)), write R code to find: The 90th percentile of turnout The turnout level that only 5% of elections exceed The range that contains the middle 80% of turnout values A student runs this code: qnorm(0.5, mean = 100, sd = 15) Without running it, what value will it return and why? 15.8.8 Challenge Problems (advanced/optional/not on test) Challenge 1. Write R code to verify the empirical rule. Generate 10,000 values from N(100, 15), then calculate what percentage fall within 1, 2, and 3 standard deviations of the mean. Challenge 2. Two districts have voter turnout distributions: District A: N(60, 5) District B: N(55, 10) Which district is more likely to have turnout above 65%? Below 50%? Show your work using R code. "],["final-poster-project-guide.html", "16 Final Poster Project Guide 16.1 Learning Objectives 16.2 What Is This For? 16.3 Project Deliverables and Timeline 16.4 Finding Your Research Question 16.5 Theory and Hypotheses 16.6 What You Cannot Do 16.7 What You Should Do 16.8 Understanding the Grading Rubric 16.9 Learning from Example Posters 16.10 Tips for Success 16.11 Technical Requirements 16.12 Frequently Asked Questions 16.13 Final Reminders", " 16 Final Poster Project Guide knitr::opts_chunk$set(echo = TRUE) 16.1 Learning Objectives By the end of this guide, you will understand how to design and execute a complete research project, including developing a falsifiable hypothesis, collecting or finding appropriate data, conducting statistical analysis, and presenting your findings in a professional poster format. You’ll also learn what I’m looking for when grading these projects and how to avoid the most common mistakes students make. 16.2 What Is This For? The poster project is one of the most important part of this course, worth 20% of your final grade. This is your opportunity to take everything you’ve learned about research design, data analysis, statistical reasoning, data management, and visualization and apply it to a question you genuinely care about. Unlike problem sets where I give you the data and tell you what to analyze, here you’ll make all the decisions: What question should I ask? How should I collect data? What statistical methods are appropriate? How should I present my findings? I grade these posters personally because I consider them the culmination of your learning this semester. The best projects don’t just demonstrate technical competence—they show genuine curiosity about the world and a willingness to take intellectual risks. I greatly prefer teams who pursue exciting, ambitious questions and find null results to teams that play it safe with uninteresting hypotheses. Science works by being wrong productively, and you’re new at this, so embrace the possibility that your hypothesis might not be supported. That said, null results are only acceptable when your research design is sound, your analysis is correct, and you interpret those results thoughtfully. This guide will help you understand what constitutes good research design and how to present your work professionally, regardless of whether your hypothesis is supported. 16.3 Project Deliverables and Timeline You’ll submit your project as a single ZIP file containing three components: a PDF of your poster, an R script with all your analysis code, and all the data files needed to run that code. The submission is due by 5pm on the date listed on the syllabus. During your final lab session, I will attend and hear your poster presentation. You’ll explain your project, present your results, and answer questions from me and other students. Your R script must be completely replicable. The only thing I should need to change is the working directory. Otherwise, when I run your code, it should execute without any errors and produce results that exactly match what’s on your poster. Any discrepancies or errors will result in point deductions. Your code should also include informative comments throughout (using the # symbol) explaining what you’re doing. Code without comments will be penalized. Generally, I don’t need to see all the code you used to collect, organize, or clean your data—but I do want the code that takes your final dataset, conducts the statistical analyses, and generates the tables and figures that appear on your poster. To get you started, I will ask each group to provide me with a one page description of your project by November 1. The format of this document is quite flexible, and is not a firm commitment. But it is an opportunity to work with your group to put a plan down on paper (even if you may deviate later). The document shouldch outline the goals of the project, a sketch of your approach, individual responsibilities of the project members, and a tentative schedule. I recommended you include milestones and deliverables in your project plan, which helps to keep your group accountable. 16.4 Finding Your Research Question The most important decision you’ll make is choosing your research question. You should have a strong and substantive motivation for why you chose it. This means more than just “I thought it would be interesting”—you need to explain why this question matters, what gap in our understanding it addresses, or what practical or theoretical implications the answer might have. Your motivation section should include a few sentences of background context. More fundamentally, though, you need to care about the question itself. What topics interest you? What phenomenon are you dying to explain? If you don’t care about the question, this project will be miserable to complete. I’ve seen teams produce brilliant work on topics as diverse as recycling behavior, mental health services on campus, election fraud, and partisan polarization—what they all had in common was genuine intellectual curiosity. Your research question should be succinct and easily understood. Most importantly, it needs to be falsifiable. This is not the same as being “provable”—it means you need to be able to tell me what kind of statistical evidence would prove you wrong. If you can’t explain that, you don’t really have a scientific theory (or a null hypothesis for that matter). For example, “What makes college kids vote?” is too vague to be falsifiable. But “College students in states with stricter voter ID laws vote at rates 5-10 percentage points lower than students in states without such laws” is falsifiable—you could test this and potentially reject it. Your question also needs to be specific enough to be manageable but interesting enough to matter. Broad questions like “Why do Americans vote?” or “What causes peace?” are too big. You need something you can actually test with available data and time. Better questions might be “Do college students vote more when registration is easier?” or “Does information about campus mental health wait times affect support for new campus construction?” These are specific, testable, and potentially interesting. 16.5 Theory and Hypotheses Once you have a question, you need to develop a theory. This doesn’t have to be elaborate, but you need to clearly state what you expected when you started gathering data and why. What’s your theory of how the world works? For example, if you’re studying whether “please recycle” signs increase recycling behavior, your theory might be that visible prompts activate social norms and reduce cognitive load, making pro-environmental behavior more likely. This gives your hypothesis a foundation beyond simple speculation. Your hypothesis needs to be precisely stated with both null and alternative versions. The null hypothesis typically states there is no relationship or effect. The alternative hypothesis states what you expect to find. Be specific about the direction and magnitude when possible. “Recycling signs increase recycling” is vague. “Recycling bins with visible signs will have more recyclable materials than bins without signs” is better—it’s specific, measurable, and falsifiable. 16.6 What You Cannot Do While you have considerable freedom in choosing your topic, there are some important boundaries. First, you cannot conduct time-series studies where you’re studying one unit over time. Questions like “What drives presidential approval over time?” are out of bounds because time-series analysis requires additional statistical skills not covered in this course. Second, you cannot do purely exploratory projects. If you ask “What factors determine attitudes toward abortion?” without specifying which factors and why, that’s exploration, not hypothesis testing. You need to tell me what factors you think drive attitudes toward abortion, explain your reasoning, and then test that theory. Third, you cannot collect sensitive data, study risky behaviors, or work with at-risk populations. This means no surveys about dating habits, drug use, or alcohol consumption. No surveys of minors or homeless populations. These restrictions exist both for ethical reasons and because proper study of these topics requires IRB approval and methodological training you haven’t yet received. Fourth, you cannot sample on the dependent variable. This is a crucial methodological point that many students miss. If you want to study the causes of election fraud, you cannot only look at cases where fraud occurred—you also need cases where fraud did not occur for comparison. Similarly, if you want to study causes of police shootings, you can’t only examine shooting incidents—you need cases where police encounters did not result in shootings. Without variation in your dependent variable, you cannot identify what causes that variation. Finally, you cannot sample on the independent variable. If you want to study the effect of church attendance on political beliefs, you can’t only survey people at church. To identify the effect of church attendance, you need to compare church-goers to non-church-goers. This seems obvious when stated plainly, but students often overlook this when designing surveys. 16.7 What You Should Do While there are restrictions, there are also exciting possibilities. You’re encouraged (though not required) to conduct your own experiment. Do “please recycle” signs cause people to recycle more? Does information about wait times affect perceptions of campus services? Do different question wordings produce different survey responses? Experiments let you establish causal relationships in ways that observational data cannot. If you plan to collect your own data, please see me before executing your plan—I may be able to provide incentives like drawings for gift cards. The university pays for all students to have access to Qualtrics, a powerful survey software. If you want to survey Washington University students about topics they care about—campus politics, student life, attitudes toward university policies—this can produce engaging and relevant research. Just remember to follow all the sampling rules discussed above. If you don’t plan to collect your own data, many excellent datasets are freely available. Harvard’s Institute for Quantitative Social Sciences maintains Dataverse, a massive repository of social science data. The American National Election Study provides rich data on American political behavior and attitudes. The ICPSR Data Archives at the University of Michigan offer thousands of datasets on topics ranging from crime to health to education. The Washington University library also provides data services and can help you locate data on your topic. And of course, if only you knew a faculty member with wide-ranging interests in quantitative analysis who is experienced in helping undergraduates get the data they need to do good projects—perhaps you could ask her or him for help. 16.8 Understanding the Grading Rubric Your project is graded on eight components, each worth five points, for a total of 40 points. Let me walk you through each section and explain what distinguishes excellent work from mediocre work. library(knitr) include_graphics(&quot;Images/GradingRubric.jpeg&quot;) 16.8.1 Introduction and Theory An excellent introduction precisely identifies both null and alternative hypotheses and provides substantive and theoretical motivation for the research project. This means more than saying “I wanted to know about X”—it means explaining why X matters theoretically or practically, what gap in knowledge your project addresses, and what theory predicts about the relationship you’re studying. A merely good introduction identifies the hypotheses and provides motivation, but perhaps less precisely or thoroughly. A mediocre introduction describes hypotheses without precisely specifying them or lacks clear substantive motivation. A weak introduction states theory incorrectly or vaguely and lacks appropriate theoretical grounding. 16.8.2 Methods An excellent methods section specifies all important aspects of how the study was conducted in detailed and replicable fashion. This means explaining both where your data came from and how you tested your hypothesis. If you collected data yourself, describe your survey questions, sampling approach, and any important methodological choices. If you used pre-existing data, explain the source and briefly describe its methodology. This applies to covariates as well—if you’re controlling for income or education, explain where those variables came from. Crucially, you must justify your analytical choices. Why did you use regression instead of a t-test? Why did you include certain control variables? Why did you transform variables in particular ways? Note that the example posters provided are from a more advanced version of this class, so you won’t be familiar with all the models they use. You are not expected to use advanced models to get full credit—but whatever modeling choices you make, you must defend them. A merely good methods section covers most important aspects relatively clearly and addresses possible design choices. A mediocre section specifies some important aspects but doesn’t always explain methods well or misses key design choices. A weak section fails to provide most important information about study conduct or methodology. 16.8.3 Results Remember: This is a poster, not a paper. An excellent results section uses figures and tables to illustrate findings in an informative and easy-to-understand way. The graphics tell a clear story with minimal text needed. All figures must be created in R—Excel figures make me angry. Tables may be created in Microsoft Office (or other tools), although we will show you how to make attractive tables for your poster later in the class. Your figures should be thorough, detailed, and properly labeled with clear axes and legends. You should let your graphics do most of the work. Don’t write long paragraphs explaining every detail—use short comments that direct attention to key findings. Look at the example posters to see how successful teams present results visually. A merely good results section has clear figures and tables with reasonably clear explanations. A mediocre section has unappealing or poorly prepared visualizations that aren’t fully integrated with text discussion. A weak section has sloppy or hard-to-understand graphics with weak integration of hypotheses. 16.8.4 Limitations and Conclusions An excellent limitations and conclusions section provides thorough discussion of the limitations of your findings, potential design flaws, and interpretation of results. Every research project has limitations—sample size, measurement issues, confounding variables you couldn’t control, generalizability concerns. Acknowledge these forthrightly and thoughtfully. Don’t just provide “pro forma” limitations (the kind where you say “well, we could have had a bigger sample” without really thinking about implications). Think seriously about what could affect your results and why. Your conclusion should discuss potential explanations for your findings. Were results statistically significant? What does this tell you about your research question? What inferences can we draw? What are ideas for future research? You must state whether you rejected your null hypothesis. And remember: it is not required that you find statistically significant results. Most teams won’t. This is okay if your research design was sound and you interpret null results thoughtfully. A merely good section has clear and thoughtful discussion with substantive conclusions but perhaps less depth. A mediocre section provides some useful discussion but with incomplete or less sound conclusions. A weak section is vague, incomplete, or inconsistent. 16.8.5 Statistical Analysis (Poster) An excellent statistical analysis uses innovative or at minimum correct statistical methods appropriate for your research question with no errors. A merely good analysis uses correct methods with no or few errors. A mediocre analysis potentially uses inappropriate methods with some errors. A weak analysis has flawed methods with significant errors. Note that this is a grade for the choices themselves, not the writing. The score here is about whether or not you used the appropriate methdos for your question. The “methods” score is about whether they are clearly described and justified. 16.8.6 Statistical Analysis (R Script) An excellent R script replicates poster findings exactly from correctly annotated code with clear comments explaining your analytical choices. A merely good script largely replicates analysis with mostly clear code. A mediocre script provides partial replication or has faulty code with incomplete comments. A weak script doesn’t replicate results, lacks comments, or has significant errors. This is critical: I must be able to run your script (changing only the working directory) and get your exact poster results. Any discrepancies equal point deductions. 16.8.7 Graphical Design An excellent poster is exceptionally and appropriately laid out with no formatting problems. The layout flows easily and is aesthetically pleasing. It’s not unreasonably colored and doesn’t have weird fonts, formatting issues, text that leaks across columns, etc. Most groups use Microsoft PowerPoint, but other tools are fine. Whatever you use, make it clean and readable. A merely good poster has attractive design with few formatting problems. A mediocre poster is somewhat attractive but has some formatting issues. A weak poster is difficult to read or poorly designed with many formatting problems. 16.8.8 Writing Quality The single most common way students lose points is by making posters that are too wordy.You should be able to succinctly explain each step of your research without writing a page on it. In fact, I’d be happy with no paragraphs. Use full sentences, but don’t cram paragraph after paragraph of text. An excellent poster is exceptionally well-written and articulate with precise, clear, mistake-free prose. It’s concise and elegant, not too long. A merely good poster is very well-written with few or no typos and appropriate length. A mediocre poster is moderately well-written but has some typos and is wordy or vague. A weak poster is unclear and awkward with numerous typos and length problems. 16.9 Learning from Example Posters Below are posters that did very well in past years. However, none were perfect. I’ll point out strengths and weaknesses so you can do even better. PDF versions of all example posters are available on Canvas in the ‘ExamplePosters’ directory. Higher resolution versions of all of these posters are provided on Canvas. 16.9.1 Example 1: Hating Habif This poster examined how information about wait times affects student perceptions of campus mental health services. include_graphics(&quot;Images/Habifposter.png&quot;) Strengths: Strong motivations for research and hypothesis about something the team genuinely cared about. They ran their own experiment and created visually attractive, clearly presented results. Weaknesses: The sample size could have been bigger. The conclusion section was a bit wordy. The limitations were somewhat pro forma rather than deeply engaged with potential problems. 16.9.2 Example 2: Should I Switch Where I’m Registered? This poster examined whether students are more likely to switch their registration to states with more competitive elections include_graphics(&quot;Images/poster1.png&quot;) Strengths: This team had strong motivations for their research and developed a clear hypothesis about something they genuinely cared about. They ran their own survey and analyzed two different datasets! Their limitations and conclusion sections were exhaustive with substantive interpretation of results. This showed real intellectual engagement with their findings. Weaknesses: Despite these strengths, this project was way too wordy. The design included too many text descriptions when it should have relied more on graphs and tables to speak for themselves. While the results were well done, the organization and structure made it hard to follow. The poster would also have been stronger with explicit prospects for future research. 16.9.3 Example 3: Voter Registration This poster explored whether changes in early-voting laws in specific North Carolina counties affected voter turnout. include_graphics(&quot;Images/poster3.png&quot;) 16.9.4 Example 4: Money Trouble This poster explores how student attitudes about adding more buildings to campus changes when students learn about budget difficulties in other campus operations. include_graphics(&quot;Images/poster2.png&quot;) Strengths: This poster provided substantive reasons for research with a clear hypothesis. The methods section described both data collection techniques and hypothesis testing techniques—this is what you should aim for. Their limitations and conclusions sections were substantive and perceptive. Most importantly, their results section allowed graphics to speak for themselves, making it easy to understand findings without paragraphs of information. Weaknesses: The poster should have included more specific grounds for future research beyond general suggestions. There were some small spelling and grammatical errors. And yes, it was still somewhat wordy in parts. 16.10 Tips for Success 16.10.1 Start Early This advice applies to all group projects, but it’s especially important here. Groups that start after Thanksgiving Break don’t do as well as groups that begin closer to the beginning of November. This project is worth 20% of your grade—don’t take it lightly. Starting early gives you time to work out problems, get feedback, revise your approach, and produce polished work. 16.10.2 Come to Office Hours Early and Often While you’re note required to come speak with me once about your project, I strongly encourage multiple check-ins. I grade these projects, so only I know what I’m looking for. Teams that meet with me early and often consistently do well. The only teams that have received objectively bad grades made drastic changes to their research project at the last minute without consulting me. Don’t be that team. The TAs are excellent resources for specific questions and help with R, but I should be your first and last point of contact for big questions about topic selection, research design, theoretical development, and interpretation. Don’t hesitate to reach out. 16.10.3 Pay Close Attention to the Rubric Take a close look at the markers for getting high marks in each grading section. I will follow these requirements for fives very closely. Use the rubric as a checklist before submitting. Have you addressed every component? Have you met every requirement for a five in each section? The rubric isn’t arbitrary—it represents what I genuinely believe constitutes excellent research at your level. 16.11 Technical Requirements 16.11.1 Creating Your Poster Most students use Microsoft PowerPoint to create posters, though you’re welcome to use other tools like Adobe Illustrator, LaTeX beamerposter, or Inkscape. Whatever tool you choose, your poster should be readable from 3-4 feet away. Use large fonts: at least 72 points for the title, 48 points for headers, and 28-32 points for body text. Standard poster size is around 36 inches by 48 inches. You must export your poster to PDF before submission. In PowerPoint, go to File, then Save As, choose PDF from the file type dropdown, and select “High Quality” or “Best for Printing” in options. 16.11.2 Printing Your Poster You don’t need to print your poster. This class is just to big to organize a real poster session! But if you really want an amazing docration for your dorm/appartment, there are multilpe places on campus where you can print your poster. 16.11.3 R Code Best Practices Your R script should be organized and well-commented. Start with a header identifying your team and members. Load all necessary libraries at the beginning. Set your working directory early—this should be the only line I need to change. Organize your code into clear sections: data loading, data preparation, descriptive statistics, hypothesis testing, and figure generation. Comment extensively. Explain what each section does, why you made certain choices, and what results mean. Don’t just describe what the code does line by line—explain your reasoning. For example, rather than commenting “creates a new variable,” write “creates binary variable for high education (BA or higher) to test hypothesis that education increases voting.” Before submitting, close R completely, open only your submitted script, change only the working directory, run the entire script, and verify all results match your poster. This is what I will do, so test it yourself first. 16.12 Frequently Asked Questions What if my results aren’t statistically significant? This is completely fine and very common in real research. You will not be penalized for null results. What matters is that your research design was sound, your analysis was correct, you interpreted null results properly, you discussed why you might have gotten null results, and you suggested future research directions. Remember: I prefer ambitious projects with null results to safe projects with weak findings. Can I use Excel for figures? No. Excel figures make me angry. All figures must be created in R. However, tables may be created using Microsoft Office tools. How long should each section be? Shorter is better. This is a poster, not a paper. The most common issue is being too wordy. As a rough guide: introduction should be 2-3 short paragraphs, methods should be one to three short paragraphs, results should let figures do the talking with two to three sentences per figure, conclusion should be one paragraph, and limitations can be bullet points or one paragraph. When in doubt, cut text and add visual elements. What if I can’t find the right data? Come talk to me. I have extensive experience helping students find data. Options include using different data for a similar question, collecting your own data through surveys or experiments, modifying your question to fit available data, or asking a faculty member who knows about quantitative data. (Hint: that faculty member is me.) Can I change my topic mid-project? You can, but be careful. The only teams that have gotten objectively bad grades made drastic changes at the last minute without consulting me. If you need to change topics, come talk to me immediately, explain why, get approval for your new topic, and understand that you must still meet all deadlines. Can I work alone? This is designed as a group project, but if you have special circumstances, come discuss them with me. 16.13 Final Reminders Start early. Come to office hours multiple times. Be concise—fewer words is better. Null results are okay—I prefer ambitious failures to safe projects. Study the rubric—I follow it closely. Test your R code—it must run perfectly. And ask for help—I want you to succeed. This project is challenging but rewarding. Follow these guidelines, start early, come to office hours, and you’ll create something you’re proud of. Good luck! "],["confidence-intervals.html", "17 Confidence Intervals 17.1 Introduction: Why Point Estimates Aren’t Enough 17.2 Review: From Samples to Populations 17.3 Understanding Confidence Intervals 17.4 The Structure of a Confidence Interval 17.5 Building Confidence Intervals: Three Scenarios 17.6 Comparing Confidence Levels 17.7 A Complete Example: From Start to Finish 17.8 Decision Guide: Which Method Should You Use? 17.9 Things to Look Out For 17.10 Review Questions: Confidence Intervals", " 17 Confidence Intervals knitr::opts_chunk$set(echo = TRUE) 17.1 Introduction: Why Point Estimates Aren’t Enough Imagine you’re working on a story for the student newspaper about sleep habits on campus. You survey 50 randomly selected students and find that they sleep an average of 6.8 hours per night. You want to report this finding, but you know this is just one sample from the larger population of all students at your university. How can you report this number so that it reflects your uncertainty about the true average for all students? This is the fundamental question that confidence intervals help us answer. While a point estimate like 6.8 hours is useful, it’s incomplete. It doesn’t tell us anything about the uncertainty in our estimate. A confidence interval gives us a range of plausible values for the true population parameter. And we know that this range of values has certain properties derived from probability theory. By the end of this lesson, you’ll be able to construct confidence intervals for population means under different scenarios, interpret what those intervals mean, and understand the tradeoffs involved in choosing different confidence levels. 17.2 Review: From Samples to Populations Before we dive into confidence intervals, let’s review some key concepts about statistical inference. Remember that we use sample statistics to make inferences about population parameters. The notation is important here, so let’s be clear about what represents what. When we talk about samples versus populations, we use different symbols to keep track of which is which. The sample mean is denoted \\(\\bar{x}\\) (or sometimes \\(\\bar{y}\\)), while the population mean is denoted \\(\\mu\\). Similarly, the sample standard deviation is \\(s\\), while the population standard deviation is \\(\\sigma\\). This distinction matters because we almost never know the population parameters—if we did, we wouldn’t need to do statistical inference! English term Sample Statistic Population parameter Mean \\(\\bar{x}\\) \\(\\mu\\) Standard deviation \\(s\\) \\(\\sigma\\) Variance \\(s^2\\) \\(\\sigma^2\\) The Central Limit Theorem tells us something remarkable: even if the population isn’t normally distributed, the distribution of sample means becomes approximately normal as the sample size increases. This is why we can use the normal distribution to construct confidence intervals for means, at least when our sample size is reasonably large. 17.3 Understanding Confidence Intervals A confidence interval is an interval estimate of a population parameter. Unlike a point estimate, which is just a single number, an interval estimate gives us a range of plausible values. But what does it mean when we say we have a “95% confidence interval”? 17.3.1 The Correct Interpretation Here’s where things get a bit subtle, and it’s important to understand this correctly. A 95% confidence interval does not mean there is a 95% probability that the true parameter falls within this particular interval. Once we’ve calculated a specific interval, the true parameter either is or isn’t in that interval—we just don’t know which. Instead, the 95% refers to the procedure we used to construct the interval. If we were to take many, many random samples from the population and calculate a 95% confidence interval for each sample, then about 95% of those intervals would contain the true population parameter. It’s a statement about the long-run behavior of the method, not about any particular interval. Think of it this way: imagine you surveyed 100 different random samples of students about their sleep habits, and you calculated a 95% confidence interval for each sample. You’d expect that about 95 of those intervals would capture the true average sleep time for all students, while about 5 of them would miss it. Before you collect any particular sample, you can be 95% confident that the interval you calculate will be one of the ones that captures the true value. We can see this by simulating random samples in R. In this simulation, we take 20 different random samples and calculated a 95% confidence interval for each one. Notice that most intervals (shown in green) contain the true population mean (the blue dashed line), but one (shown in red) misses it. This is exactly what we expect—about 95% capture the truth. This interpretation is admittedly a bit confusing at first, but it’s the technically correct one. In practice, when we report a 95% confidence interval, we often say something like “we are 95% confident that the true population mean falls between X and Y.” This is a reasonable shorthand, but just remember that it’s not really correct. Confidence is really a statement about the reliability of our method, not a probability statement about any particular interval. 17.4 The Structure of a Confidence Interval Every confidence interval has the same basic structure. It starts with a point estimate (our best single guess at the parameter) and then adds and subtracts a margin of error. The general form is: \\[\\text{point estimate} \\pm \\text{margin of error}\\] For means, the point estimate is the sample mean \\(\\bar{x}\\). The margin of error depends on two things: how much variability there is in the sampling distribution (measured by the standard error), and how confident we want to be (captured by a critical value from either the normal or t-distribution). \\[\\text{Confidence Interval} = \\bar{x} \\pm (\\text{critical value}) \\times (\\text{standard error})\\] The margin of error is calculated as the critical value multiplied by the standard error. The critical value depends on the confidence level we choose. For a 95% confidence level using the normal distribution, the critical value is approximately 1.96. For a 90% confidence level, it’s about 1.645. For a 99% confidence level, it’s about 2.576. ## Warning in attr(x, &quot;align&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) ## Warning in attr(x, &quot;format&quot;): &#39;xfun::attr()&#39; is deprecated. ## Use &#39;xfun::attr2()&#39; instead. ## See help(&quot;Deprecated&quot;) Confidence Level Alpha Alpha/2 Z Critical Value 90% 0.10 0.05 1.645 95% 0.05 0.025 1.960 99% 0.01 0.005 2.576 Notice a pattern here? As we want to be more confident that we’ve captured the true parameter, our critical value gets larger, which makes our margin of error larger, which makes our confidence interval wider. This is the fundamental tradeoff: we can be more confident by casting a wider net, but that wider net gives us a less precise estimate. 17.5 Building Confidence Intervals: Three Scenarios The exact formula we use to construct a confidence interval depends on what we know about the population and how large our sample is. There are three main scenarios we need to consider, each requiring a slightly different approach. 17.5.1 Scenario A: When We Know the Population Standard Deviation Sometimes—though not often in real research—we know the population standard deviation \\(\\sigma\\). In this case, we can calculate the exact standard error of the sampling distribution using the formula: \\[\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\] This tells us how much variability we expect in sample means if we were to take many samples of size \\(n\\) from the population. The confidence interval is then: \\[\\bar{x} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}\\] where \\(z\\) is the critical value from the standard normal distribution corresponding to our desired confidence level. 17.5.1.1 Example: Sleep Study with Known σ Let’s return to our sleep study example. Suppose we surveyed 50 students and found a sample mean of 6.8 hours of sleep per night. Let’s also suppose (unrealistically) that we somehow know the population standard deviation is exactly 1.5 hours. We want to construct a 95% confidence interval. First, we calculate the standard error: n &lt;- 50 sigma &lt;- 1.5 x_bar &lt;- 6.8 se_known &lt;- sigma / sqrt(n) print(&quot;Standard Error&quot;) ## [1] &quot;Standard Error&quot; round(se_known, 3) ## [1] 0.212 The standard error is about 0.21 hours. Now we need the critical value for a 95% confidence level. For the normal distribution, this is 1.96, but we can get R to calculate it precisely using the qnorm() function. For a 95% confidence interval, we want the z-score that leaves 2.5% in each tail (since 100% - 95% = 5%, and we split that equally between the two tails). z_critical &lt;- qnorm(0.975) print(&quot;Critical Value&quot;) ## [1] &quot;Critical Value&quot; round(z_critical, 3) ## [1] 1.96 Now we can calculate the margin of error and construct the confidence interval: margin_of_error &lt;- z_critical * se_known lower_bound &lt;- x_bar - margin_of_error upper_bound &lt;- x_bar + margin_of_error cat(&quot;Margin of Error:&quot;, round(margin_of_error, 3), &quot;hours\\n&quot;) ## Margin of Error: 0.416 hours cat(&quot;95% Confidence Interval: [&quot;, round(lower_bound, 2), &quot;,&quot;, round(upper_bound, 2), &quot;] hours\\n&quot;) ## 95% Confidence Interval: [ 6.38 , 7.22 ] hours So our 95% confidence interval is approximately [6.38, 7.22] hours. We interpret this by saying that if we used this procedure many times with different samples, about 95% of the intervals we construct would contain the true average sleep time for all students at the university. Notice that we used qnorm(0.975) rather than qnorm(0.95). This is because the normal distribution is symmetric, and we want 95% in the middle, which means 2.5% in each tail. The qnorm() function with 0.975 gives us the z-score that has 97.5% of the distribution to the left of it, which is equivalent to leaving 2.5% in the right tail. 17.5.2 Scenario B: Unknown Population Standard Deviation, Large Sample In reality, we almost never know the population standard deviation. This is much more common—we have to estimate \\(\\sigma\\) using our sample standard deviation \\(s\\). When we do this, our estimate of the standard error becomes: \\[\\hat{\\sigma}_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\] The hat notation (\\(\\hat{\\sigma}\\)) indicates that this is an estimate rather than a known value. Now here’s a key question: should we still use the normal distribution for our critical values? If our sample size is large (generally, \\(n \\geq 30\\)), the Central Limit Theorem tells us that the sampling distribution of the mean is approximately normal, even if the population isn’t normal. Moreover, when we have a large sample, our estimate \\(s\\) is usually pretty close to the true \\(\\sigma\\), so using the normal distribution for critical values is reasonable. This is why, for large samples with unknown \\(\\sigma\\), we still use z-scores from the normal distribution. 17.5.2.1 Example: Sleep Study with Estimated σ Let’s continue with our sleep study example, but now assume we don’t know the population standard deviation. Instead, we calculated a sample standard deviation of \\(s = 1.6\\) hours from our 50 students. s &lt;- 1.6 se_estimated &lt;- s / sqrt(n) round(se_estimated, 3) ## [1] 0.226 Notice that our estimated standard error (0.226) is close to but not exactly the same as our known standard error from before (0.212). Now we construct the confidence interval the same way as before: margin_of_error_large &lt;- z_critical * se_estimated lower_bound_large &lt;- x_bar - margin_of_error_large upper_bound_large &lt;- x_bar + margin_of_error_large cat(&quot;95% Confidence Interval: [&quot;, round(lower_bound_large, 2), &quot;,&quot;, round(upper_bound_large, 2), &quot;] hours\\n&quot;) ## 95% Confidence Interval: [ 6.36 , 7.24 ] hours Our interval is [6.36, 7.24] hours, which is slightly wider than before because we had to estimate the standard deviation. This is the price we pay for not knowing the population parameter—a bit more uncertainty in our interval. 17.5.3 Scenario C: Unknown Population Standard Deviation, Small Sample Now we come to the trickiest scenario, and one that’s quite common in real research: we have a small sample (say, \\(n &lt; 30\\)) and we don’t know the population standard deviation. In this case, using the normal distribution isn’t quite right. Because we’re estimating the standard deviation from a small sample, there’s additional uncertainty that we need to account for. This is where the t-distribution comes in. You learned about the t-distribution in the previous lesson on probability distributions. Recall that the t-distribution looks similar to the normal distribution but has thicker tails, which reflects the extra uncertainty we have when working with small samples. As the sample size increases, the t-distribution gets closer and closer to the normal distribution. The key parameter for the t-distribution is the degrees of freedom, which for a single mean is \\(df = n - 1\\). With fewer degrees of freedom, the t-distribution has thicker tails (more probability in the extremes), which means larger critical values and wider confidence intervals. This is exactly what we want—when we have less data, we should be less certain, and our intervals should be wider to reflect that uncertainty. 17.5.3.1 Example: Sleep Study with Small Sample Let’s modify our sleep study example. Suppose instead of surveying 50 students, we only managed to survey 15 students, and we found a sample mean of 6.8 hours with a sample standard deviation of 1.6 hours. n_small &lt;- 15 s_small &lt;- 1.6 x_bar_small &lt;- 6.8 se_small &lt;- s_small / sqrt(n_small) se_small ## [1] 0.4131182 The standard error is now larger (0.413) because we’re dividing by the square root of a smaller number. Now for the critical value, we use the qt() function instead of qnorm(), and we need to specify the degrees of freedom: df &lt;- n_small - 1 t_critical &lt;- qt(0.975, df = df) t_critical ## [1] 2.144787 Notice that the t-critical value (2.145) is larger than the z-critical value (1.96) we used before. This reflects the additional uncertainty in this setting. Now we construct the confidence interval: margin_of_error_small &lt;- t_critical * se_small lower_bound_small &lt;- x_bar_small - margin_of_error_small upper_bound_small &lt;- x_bar_small + margin_of_error_small cat(&quot;95% Confidence Interval: [&quot;, round(lower_bound_small, 2), &quot;,&quot;, round(upper_bound_small, 2), &quot;] hours\\n&quot;) ## 95% Confidence Interval: [ 5.91 , 7.69 ] hours Our 95% confidence interval is [5.91, 7.69] hours. This interval is much wider than what we got with 50 observations, reflecting both the larger standard error from having fewer observations and the larger critical value from using the t-distribution. This is exactly as it should be—with less data, we’re less certain about where the true population mean lies. The logic behind using qt(0.975, df = 14) is the same as before: we want 95% in the middle of the distribution, so 2.5% in each tail, which means we want the t-value with 97.5% of the distribution to the left of it. 17.6 Comparing Confidence Levels So far we’ve focused on 95% confidence intervals, but we can construct intervals at any confidence level. The choice of confidence level involves a tradeoff between confidence and precision. Let’s explore this using our large-sample sleep study example (n = 50, \\(\\bar{x}\\) = 6.8, s = 1.6). First, let’s construct a 90% confidence interval: z_90 &lt;- qnorm(0.95) se &lt;- s / sqrt(n) lower_90 &lt;- x_bar - z_90 * se upper_90 &lt;- x_bar + z_90 * se width_90 &lt;- upper_90 - lower_90 cat(&quot;90% Confidence Interval: [&quot;, round(lower_90, 2), &quot;,&quot;, round(upper_90, 2), &quot;]\\n&quot;) ## 90% Confidence Interval: [ 6.43 , 7.17 ] cat(&quot;Width:&quot;, round(width_90, 2), &quot;hours\\n\\n&quot;) ## Width: 0.74 hours Now a 95% confidence interval: z_95 &lt;- qnorm(0.975) lower_95 &lt;- x_bar - z_95 * se upper_95 &lt;- x_bar + z_95 * se width_95 &lt;- upper_95 - lower_95 cat(&quot;95% Confidence Interval: [&quot;, round(lower_95, 2), &quot;,&quot;, round(upper_95, 2), &quot;]\\n&quot;) ## 95% Confidence Interval: [ 6.36 , 7.24 ] cat(&quot;Width:&quot;, round(width_95, 2), &quot;hours\\n\\n&quot;) ## Width: 0.89 hours And finally a 99% confidence interval: z_99 &lt;- qnorm(0.995) lower_99 &lt;- x_bar - z_99 * se upper_99 &lt;- x_bar + z_99 * se width_99 &lt;- upper_99 - lower_99 cat(&quot;99% Confidence Interval: [&quot;, round(lower_99, 2), &quot;,&quot;, round(upper_99, 2), &quot;]\\n&quot;) ## 99% Confidence Interval: [ 6.22 , 7.38 ] cat(&quot;Width:&quot;, round(width_99, 2), &quot;hours\\n&quot;) ## Width: 1.17 hours Notice the pattern: as we increase our confidence level from 90% to 95% to 99%, our intervals get wider. This is the tradeoff: if you want to be more confident that you’ve captured the true parameter, you have to accept a less precise estimate. ## Warning: `geom_errorbarh()` was deprecated in ggplot2 4.0.0. ## ℹ Please use the `orientation` argument of `geom_errorbar()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## `height` was translated to `width`. In practice, 95% confidence intervals are the most common, as they represent a reasonable balance between confidence and precision. However, the choice depends on the context and the consequences of being wrong. In exploratory research, 90% intervals might be fine. In medical research where decisions have serious consequences, researchers might prefer 99% intervals. 17.7 A Complete Example: From Start to Finish Let’s work through a complete example from beginning to end using real data. We’ll use the congressional age data from FiveThirtyEight, which contains information about every member of the U.S. Congress from 1947 to 2013. Suppose we’re interested in estimating the average age of Congress members, but we can’t analyze the entire dataset. Instead, we’ll take a random sample of 25 Congress members and construct a 95% confidence interval for the true average age. First, let’s load the data and take our sample: library(fivethirtyeight) data(&quot;congress_age&quot;) # Set seed for reproducibility set.seed(42) # Take a random sample of 25 Congress members sample_congress &lt;- congress_age[sample(nrow(congress_age), 25), ] # Calculate sample statistics sample_mean &lt;- mean(sample_congress$age) sample_sd &lt;- sd(sample_congress$age) sample_n &lt;- nrow(sample_congress) sample_mean ## [1] 50.912 sample_sd ## [1] 9.691978 sample_n ## [1] 25 Now we need to decide which scenario applies. We don’t know the population standard deviation (we almost never do), and our sample size is 25, which is less than 30. This means we’re in Scenario C—we should use the t-distribution. Calculate the standard error: sample_se &lt;- sample_sd / sqrt(sample_n) sample_se ## [1] 1.938396 Find the critical value from the t-distribution: sample_df &lt;- sample_n - 1 alpha &lt;- 1 - 0.95 sample_t_critical &lt;- qt(1 - alpha/2, df = sample_df) sample_t_critical ## [1] 2.063899 Calculate the margin of error: sample_margin &lt;- sample_t_critical * sample_se sample_margin ## [1] 4.000652 Finally, construct the confidence interval: sample_lower &lt;- sample_mean - sample_margin sample_upper &lt;- sample_mean + sample_margin sample_lower ## [1] 46.91135 sample_upper ## [1] 54.91265 We can interpret this result as follows: based on our random sample of 25 Congress members, we are 95% confident that the true average age of all Congress members in our dataset is between these two values. If we repeated this sampling procedure many times and calculated a confidence interval each time, approximately 95% of those intervals would contain the true population mean. Let’s verify our interval against the actual population mean: # Calculate the true population mean true_mean &lt;- mean(congress_age$age) true_mean ## [1] 53.31373 # Did our interval capture it? sample_lower &lt;= true_mean &amp; true_mean &lt;= sample_upper ## [1] TRUE In this case, our confidence interval successfully captured the true population mean! Of course, in real research we wouldn’t know the true population mean—that’s why we’re constructing confidence intervals in the first place. But this exercise demonstrates how the method works. 17.8 Decision Guide: Which Method Should You Use? Here’s a systematic way to decide which approach to use when constructing a confidence interval for a mean: Step 1: Do you know the population standard deviation \\(\\sigma\\)? If yes (which is rare), use Scenario A with the normal distribution If no (which is almost always the case), go to Step 2 Step 2: Is your sample size large (\\(n \\geq 30\\))? If yes, use Scenario B with the normal distribution as an approximation If no (small sample), use Scenario C with the t-distribution The boundary of \\(n = 30\\) is somewhat arbitrary, but it’s a useful rule of thumb. For sample sizes between 20 and 30, it usually doesn’t matter much whether you use z or t critical values. For very small samples (say, \\(n &lt; 15\\)), you definitely want to use the t-distribution. One additional consideration: the Central Limit Theorem tells us that the sampling distribution of the mean is approximately normal for large samples, even if the population isn’t normal. But for small samples, if you have strong reason to believe the population is very non-normal (highly skewed, for example), be cautious about constructing confidence intervals without additional analysis. 17.9 Things to Look Out For Several common mistakes come up repeatedly when students first learn about confidence intervals. Being aware of these can help you avoid them. 17.9.1 Common Mistakes Mistake 1: Misinterpreting the confidence level. The most common mistake is saying “there is a 95% probability that the true mean is in this interval.” Once you’ve calculated a specific interval, the true mean either is or isn’t in it—there’s no probability involved for that particular interval. The 95% refers to the long-run success rate of the procedure, not the probability for any particular interval you’ve calculated. Mistake 2: Forgetting to divide alpha by 2. When using qnorm() or qt() to find critical values, remember that for a two-sided confidence interval, you need to split the alpha level between the two tails. For a 95% confidence interval, you use qnorm(0.975), not qnorm(0.95). Mistake 3: Using the wrong standard error formula. Make sure you’re dividing by the square root of n, not just by n. The standard error is \\(\\frac{s}{\\sqrt{n}}\\), not \\(\\frac{s}{n}\\). Mistake 4: Using z when you should use t. If you have a small sample and unknown population standard deviation, you should use the t-distribution. Using the normal distribution will make your confidence intervals too narrow, giving you false confidence in your estimate. Mistake 5: Forgetting about assumptions. Confidence intervals for means assume that your sample is a random sample from the population. If your sample is biased or not representative, your confidence interval may not capture the true population parameter, regardless of what confidence level you choose. 17.9.2 When Confidence Intervals Can Be Misleading Confidence intervals are powerful tools, but they can be misleading in certain situations. Here’s a real-world example that illustrates the importance of proper sampling. In the run-up to the 2016 US presidential election, many polls showed Hillary Clinton ahead of Donald Trump. These polls typically reported margins of error around ±3 percentage points, which corresponds to a 95% confidence interval. Yet Trump won the election. What went wrong? The issue wasn’t with the mathematics of confidence intervals—it was with the sampling. Many polls failed to adequately represent certain groups of voters, particularly those without college degrees in key swing states. No amount of statistical sophistication can fix a biased sample. The confidence intervals accurately reflected sampling variability, but they couldn’t account for the fact that the samples weren’t truly representative of the voting population. The lesson here is clear: a confidence interval is only as good as the data that goes into it. Before you trust a confidence interval, ask yourself whether the sample was truly random and representative of the population you care about. 17.10 Review Questions: Confidence Intervals 17.10.1 Conceptual Understanding Explain in your own words what a 95% confidence interval means. Why is it incorrect to say “there is a 95% probability that the true mean is in this interval”? What are the two components needed to calculate the margin of error? How does each component contribute to the width of the interval? Why do confidence intervals get wider as we increase the confidence level (e.g., from 90% to 95% to 99%)? What is the tradeoff? Explain the difference between precision and accuracy in the context of confidence intervals. Can you have a precise but inaccurate confidence interval? What is the Central Limit Theorem and why is it important for constructing confidence intervals? 17.10.2 Decision Making You have a sample of n = 305 with unknown population standard deviation. Which distribution should you use to construct a confidence interval: normal (z) or t? Explain your reasoning. You have a sample of n = 18 with unknown population standard deviation. Which distribution should you use? Why? Under what circumstances (if any) would you know the population standard deviation σ in real research? 17.10.3 Calculation Practice A researcher samples 40 voters and finds a mean trust rating in Congress of 32.5 (on a 0-100 scale) with a standard deviation of 15.3. What is the standard error? Calculate the 95% confidence interval. Interpret this interval in a complete sentence. A small pilot study surveys 10 students about sleep hours. The sample mean is 6.5 hours with a standard deviation of 1.8 hours. Why must you use the t-distribution for this problem? What are the degrees of freedom? Construct a 90% confidence interval (you’ll need to use R or a t-table). You construct two confidence intervals from the same data: [45.2, 54.8] and [43.1, 56.9]. Without doing any calculations, which interval has the higher confidence level? How do you know? 17.10.4 R Programming What’s wrong with this code for finding a 95% confidence interval critical value, and how would you fix it? r z_critical &lt;- qnorm(0.95) Write R code to construct a 99% confidence interval for a sample with mean = 78, sd = 12, and n = 50. Show all steps. 17.10.5 Interpretation A researcher reports: “The 95% confidence interval for average study time is [12.3, 18.7] hours per week.” Explain what this means to someone who hasn’t taken statistics. Two studies both construct 95% confidence intervals for the same population parameter where we know the population variance \\(\\sigam^2\\). Study A gets [40, 60] while Study B gets [48, 52]. Which study had the larger sample size? How do you know? Which interval is more useful? Why? 17.10.6 Critical Thinking A political poll reports a candidate’s support at 52% with a margin of error of ±3 percentage points. The candidate loses the election with 48% of the vote. Does this mean the poll was wrong? Explain your reasoning considering what you know about confidence intervals. Why do confidence intervals assume random sampling? What happens to the interpretation if your sample is biased, even if you calculate the interval correctly? A researcher collects data from n = 25 with \\(\\bar{x}\\) = 100 and s = 15. They want to report a confidence interval but are unsure whether to use 90%, 95%, or 99% confidence. What factors should influence this decision? "],["understanding-hypothesis-testing.html", "18 Understanding Hypothesis Testing 18.1 Learning Objectives 18.2 Why Hypothesis Testing Matters 18.3 The Basic Logic of Hypothesis Testing 18.4 The Research Context 18.5 The Five Parts of a Hypothesis Test 18.6 A Second Example: Stronger Evidence 18.7 Understanding What P-Values Really Mean 18.8 The Logic of Statistical Significance 18.9 Why This Matters for Research 18.10 Common Pitfalls and How to Avoid Them 18.11 The Bigger Picture 18.12 Study Questions", " 18 Understanding Hypothesis Testing knitr::opts_chunk$set(echo = TRUE) 18.1 Learning Objectives Understand why we use hypothesis tests in research Grasp the logic behind hypothesis testing Learn the five essential components of any hypothesis test Distinguish between null and alternative hypotheses Understand p-values and statistical significance 18.2 Why Hypothesis Testing Matters Imagine you’re having dinner with friends, and someone claims that WashU students are more conservative than the national average. Everyone you know seems to be a Biden voter, so you’re skeptical. How would you settle this argument? You could: Argue really loudly (whoever is most obnoxious wins) Tell stories about your liberal classmates (anecdotal evidence) Actually collect and analyze data (the scientific approach!) This is what hypothesis testing is all about: using data to test competing claims in a systematic, rigorous way. It’s how we move from opinion and anecdote to evidence-based conclusions. 18.3 The Basic Logic of Hypothesis Testing Hypothesis testing might seem complex at first, but the underlying logic is straightforward. Here’s the basic idea: Start as a skeptic. We begin by assuming the claim we’re testing is false. This assumption is called the null hypothesis. Collect evidence. We gather data through random sampling to see what’s actually happening. Ask a key question. If the skeptic’s position were correct, how likely would we be to see data like ours (or even more extreme)? Draw a conclusion. If our data would be very unlikely under the null hypothesis, we have good reason to reject it. If not, we can’t rule it out. This approach protects us from jumping to conclusions. We only reject the skeptic’s position when we have strong evidence against it. 18.4 The Research Context Let’s make this concrete. According to a 2019 national survey, 17.8% of college freshmen identified as conservative. Now suppose you take a random sample of 200 WashU freshmen, and 39 of them (19.5%) identify as conservative. Your friend claims WashU students are more conservative than average. You’re skeptical. Can this difference (19.5% vs 17.8%) be explained by random chance, or does it suggest something real? This is exactly the kind of question hypothesis testing helps us answer. 18.5 The Five Parts of a Hypothesis Test Every hypothesis test has the same five components. Let’s walk through each one. 18.5.1 1. Assumptions Before we can perform a hypothesis test, we need to verify certain conditions: Randomness: The data must come from a random sample or have randomess from some other source that is like a random sample. This ensures our sample is representative of the population. Normal sampling distribution: With a large sample, we can assume the sampling distribution of the sample mean is approximately normal. This is thanks to the Central Limit Theorem, which gives us the mathematical leverage we need. Smaller sample sizes: With smaller sample sizes, we may need to make additional assumptions like the normality of the populiation. These assumptions aren’t just technical details—they’re the foundation that makes our statistical conclusions valid. 18.5.2 2. State the Hypotheses In any hypothesis test, we have two competing hypotheses: The Null Hypothesis (\\(H_0\\)): This represents the skeptic’s position—the claim we’re trying to disprove. In our example, the null hypothesis is that WashU students are no more conservative than the national average. Mathematically: \\(H_0: \\mu \\le 0.178\\) The Alternative Hypothesis (\\(H_a\\)): This is the research claim we’re testing—the theory that motivated our study. In our example: WashU students are more conservative than average. Mathematically: \\(H_a: \\mu &gt; 0.178\\) Notice something important: we’re testing whether we can reject the null hypothesis. We never “prove” the alternative hypothesis. We can only show that the data are inconsistent with the null hypothesis. In research papers, you’ll rarely see the null hypothesis spelled out explicitly. Instead, researchers state their hypothesis (e.g., “democracies are less likely to go to war than autocracies”), and the null hypothesis is implied (democracies are not less war-prone than autocracies). 18.5.3 3. Calculate the Test Statistic The test statistic measures how far our sample result falls from what the null hypothesis predicts, relative to the expected variability. For our example: Sample proportion: \\(\\hat{\\mu} = \\bar{x} = \\frac{39}{200} = 0.195\\) Null hypothesis value: \\(\\mu_0 = 0.178\\) Standard error: \\(se_{\\bar{x}} = \\frac{s}{\\sqrt{n}} \\approx 0.028\\) The test statistic is: \\[TS = \\frac{\\hat{\\mu} - \\mu_0}{se_{\\hat{\\mu}}} = \\frac{0.195 - 0.178}{0.028} = 0.607\\] What does this mean? Our observed proportion is 0.607 standard errors above what the null hypothesis predicts. The test statistic tells us how extreme our result is in standardized units. Think of it this way: if the null hypothesis is true, how surprising is our result? The test statistic quantifies that surprise. 18.5.4 4. Calculate the P-Value P-value: The probability of observing a sample statistic as extreme as ours (or more extreme), assuming the null hypothesis is true. This is the heart of hypothesis testing. The p-value tells us: “If the skeptic is right, and the true proportion really is 17.8% or less, what’s the probability we’d randomly sample a result as extreme as 19.5% or more?” In our example, the p-value is approximately 0.272. This means there’s a 27.2% chance of getting a sample like ours (or more extreme) if the null hypothesis is true. Important distinction: We’re not just asking about the probability of getting exactly 19.5%. We’re asking about the probability of getting 19.5% or anything even more extreme (like 20%, 25%, etc.). One-sided vs. Two-sided Tests: Our example uses a one-sided test because we only care if WashU students are MORE conservative (not less conservative). If we cared about differences in either direction, we’d use a two-sided test and multiply our p-value by 2. 18.5.5 5. Draw a Conclusion Now we interpret the p-value using a decision rule called the significance level (denoted \\(\\alpha\\), usually set at 0.05). The decision rule: If p-value ≤ 0.05: Reject the null hypothesis If p-value &gt; 0.05: Fail to reject the null hypothesis In our example, p ≈ 0.272, which is much larger than 0.05. Therefore: We made a research hypothesis (WashU students are more conservative than average) We set out the skeptic’s position (they’re not more conservative) The evidence did not refute the skeptic Conclusion: The test is inconclusive. We cannot reject the null hypothesis. What does “fail to reject” mean? It means we don’t have sufficient evidence to disprove the skeptic. It does not mean the skeptic is definitely right—just that our data don’t contradict their position strongly enough. 18.6 A Second Example: Stronger Evidence Let’s imagine our sample was different. Suppose 55 out of 200 students identified as conservative (27.5% instead of 19.5%). Now: Sample proportion: \\(\\bar{x} = 0.275\\) Standard error: \\(se \\approx 0.0317\\) Test statistic: \\(TS = \\frac{0.275 - 0.178}{0.0317} = 3.06\\) P-value: ≈ 0.001 With p ≈ 0.001, we would reject the null hypothesis. Our conclusion changes: We made a research hypothesis (WashU students are more conservative than average) We set out the skeptic’s position The evidence would be highly unlikely (only 0.1% chance) if the skeptic were right Conclusion: We reject the null hypothesis in favor of the research hypothesis This smaller p-value tells us that if the true proportion were really 17.8% or less, it would be extremely unlikely (only about 1 in 1,000 chance) to randomly sample 55 conservatives out of 200 students. 18.7 Understanding What P-Values Really Mean The p-value is probably the most misunderstood concept in statistics. Let’s be clear about what it does and doesn’t tell us: What a p-value IS: The probability of seeing data as extreme as ours (or more extreme) IF the null hypothesis is true A measure of how surprising our data would be under the null hypothesis A tool for decision-making about statistical hypotheses What a p-value is NOT: The probability that the null hypothesis is true The probability that our result happened by chance A measure of the size or importance of an effect Think of the p-value as answering this question: “If the skeptic is right, how weird is my data?” A small p-value means your data would be very weird under the null hypothesis, suggesting the null hypothesis is probably wrong. 18.8 The Logic of Statistical Significance Why do we use 0.05 as our cutoff? This threshold (\\(\\alpha\\) = 0.05) is a convention, not a law of nature. It means we’re willing to reject the null hypothesis when there’s only a 5% chance (or less) of seeing data this extreme if the null is true. Different fields sometimes use different thresholds: Medical research often uses \\(\\alpha\\) = 0.01 (more stringent) Some social sciences use \\(\\alpha\\) = 0.10 (less stringent) The key is to decide on α before looking at your data Type I and Type II Errors: Hypothesis testing involves uncertainty, so mistakes are possible: Type I Error: Rejecting the null hypothesis when it’s actually true (false positive). The significance level α is the probability of making this error. Type II Error: Failing to reject the null hypothesis when it’s actually false (false negative). We can’t eliminate both types of errors—reducing one typically increases the other. The significance level represents our tolerance for Type I errors. 18.9 Why This Matters for Research Hypothesis testing provides a systematic framework for: Testing theories: Does democracy reduce war? Do income levels affect voting behavior? Hypothesis testing helps us evaluate these claims with data. Avoiding confirmation bias: By starting from a skeptical position (the null hypothesis), we force ourselves to find strong evidence before accepting \\(\\alpha\\) claim. Communicating uncertainty: P-values and significance levels make explicit how confident we are in our conclusions. Replicability: Other researchers can replicate our tests and verify our conclusions because hypothesis testing follows a standardized procedure. 18.10 Common Pitfalls and How to Avoid Them Pitfall 1: Confusing “fail to reject” with “accept” Wrong: “We accept the null hypothesis” Right: “We fail to reject the null hypothesis” or “The test was inconclusive” Failing to find evidence against the null is not the same as proving it’s true. Pitfall 2: P-hacking Testing multiple hypotheses and only reporting significant results is unethical and leads to false discoveries. Pitfall 3: Ignoring practical significance A statistically significant result might not be practically important. With huge samples, tiny differences can be statistically significant but substantively meaningless. Pitfall 4: Misinterpreting the p-value Remember: the p-value is NOT the probability that the null hypothesis is true. 18.11 The Bigger Picture We commonly have two tasks in data analysis: Understanding a complex world: What patterns exist in our data? Understanding a probabilistic world: Are these patterns systematic or just random noise? Hypothesis testing addresses the second question. When we find a pattern in data, hypothesis testing helps us determine whether that pattern is likely to reflect something real (systematic) or just sampling variability (random). This is why hypothesis testing is fundamental to science. It provides a principled way to distinguish signal from noise, helping us build reliable knowledge about the world. 18.12 Study Questions 18.12.1 Understanding the Concepts What is a p-value? Explain what it means in plain English, without using technical jargon. What does it mean to “reject the null hypothesis”? What does it mean to “fail to reject the null hypothesis”? Why don’t we say “accept the null hypothesis”? What is a significance level (\\(\\alpha\\))? Why is \\(\\alpha\\) = 0.05 commonly used as the threshold for statistical significance? Explain the difference between a one-sided hypothesis test and a two-sided hypothesis test. When would you use each type? 18.12.2 Interpreting Results A researcher tests whether WashU students study more hours per week than the national average of 15 hours. She finds p = 0.03. Using \\(\\alpha\\) = 0.05, what should she conclude? What does this p-value mean? Another researcher tests whether campaign ads affect voter turnout and finds p = 0.18. Using \\(\\alpha\\) = 0.05, what should he conclude? Does this prove that campaign ads have no effect? Explain. 18.12.3 Setting Up Hypothesis Tests A researcher believes that students who attend office hours get higher grades than those who don’t. What is the null hypothesis? What is the alternative hypothesis? Should this be a one-sided or two-sided test? Why? You want to test whether Democratic and Republican voters differ in their support for climate policy (not specifying which direction). State the null hypothesis State the alternative hypothesis Should this be a one-sided or two-sided test? Why? 18.12.4 Understanding Errors What is a Type I error? What is the probability of making a Type I error if we use \\(\\alpha\\) = 0.05? What is a Type II error? Which is worse: Type I or Type II error? Does it depend on the situation? 18.12.5 Critical Thinking A friend tells you: “I found p = 0.02, which means there’s only a 2% chance the null hypothesis is true!” What’s wrong with this interpretation? What does p = 0.02 actually mean? 18.12.6 Application Questions You’re studying whether voter turnout is higher in competitive districts. Design a hypothesis test: What would you measure? What is your null hypothesis? What is your alternative hypothesis? What evidence would convince you to reject the null hypothesis? "],["hypothesis-testing-in-r-are-americans-worried-about-ai-taking-their-jobs.html", "19 Hypothesis Testing in R: Are Americans Worried About AI Taking Their Jobs? 19.1 Summary 19.2 Introduction: The AI Revolution and Job Anxiety 19.3 Loading and Understanding the Data 19.4 Data Cleaning: Real Data is Messy 19.5 The Five Steps of Hypothesis Testing 19.6 Refining Our Analysis: The t-Distribution 19.7 Using R’s Built-in Function 19.8 One-Sided vs Two-Sided Tests 19.9 Review Questions", " 19 Hypothesis Testing in R: Are Americans Worried About AI Taking Their Jobs? knitr::opts_chunk$set(echo = TRUE) 19.1 Summary In this module, you’ll learn how to conduct hypothesis tests in R using real data from the 2024 General Social Survey about AI job concerns. We’ll work through the complete five-step hypothesis testing framework, starting with simple approaches using the normal distribution, then refining our analysis with the t-distribution. By the end, you’ll be able to: Set up null and alternative hypotheses Calculate test statistics and p-values manually Use R’s built-in functions for efficiency Interpret statistical results in meaningful policy contexts Most importantly, you’ll see that hypothesis testing isn’t just abstract math—it’s a tool for answering real questions that matter for society. 19.2 Introduction: The AI Revolution and Job Anxiety It’s 2025, and AI is everywhere. ChatGPT writes essays, DALL-E creates art, and self-driving cars are becoming reality. Your parents ask if they should worry about their jobs. Politicians debate whether we need Universal Basic Income. Tech leaders alternately promise utopia and warn of disaster. But what do ordinary Americans actually think? Are people worried about AI taking their jobs, or is this just media hype? The 2024 General Social Survey—one of the most important social science datasets in America—asked respondents: “How worried are you that AI will take over many jobs done by humans?” Today, we’ll use hypothesis testing to determine whether Americans are significantly worried about AI job displacement. This isn’t just an academic exercise—the answer has real implications for policy debates about worker retraining, tech regulation, and the social safety net. 19.3 Loading and Understanding the Data Let’s start by loading the GSS data and exploring what we’re working with: # Load necessary packages library(haven) # For reading Stata files library(dplyr) # For data manipulation library(ggplot2) # For visualization # Load the GSS 2024 data # Note: In your assignment, adjust the file path as needed data &lt;- read_dta(&quot;GSS2024.dta&quot;) # Look at the structure of our variable of interest table(data$aiworry) ## ## 1 2 3 4 5 ## 431 612 256 168 69 Notice something important: We have a lot of missing values! Only about 1,536 of 3,309 respondents answered this question. This is common in the GSS—they use a “split ballot” design where different subsets get different questions to keep the survey manageable. Let’s look at the scale: 1 = Very Worried 2 = Somewhat Worried 3 = Neither Worried nor Unworried 4 = Not Very Worried 5 = Not At All Worried Critical insight: Lower numbers mean MORE worried! 19.4 Data Cleaning: Real Data is Messy Before we can test anything, we need to clean our data. This is where most real data analysis happens—not in fancy models, but in careful data preparation: # Remove NAs and invalid response codes gss_clean &lt;- data %&gt;% filter(!is.na(aiworry) &amp; aiworry %in% c(1, 2, 3, 4, 5)) # Check our cleaned data n &lt;- nrow(gss_clean) print(paste(&quot;Sample size after cleaning:&quot;, n)) ## [1] &quot;Sample size after cleaning: 1536&quot; # Calculate summary statistics summary(gss_clean$aiworry) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 1.00 2.00 2.24 3.00 5.00 # Visualize the distribution ggplot(gss_clean, aes(x = factor(aiworry))) + geom_bar(fill = &quot;steelblue&quot;) + scale_x_discrete(labels = c(&quot;1\\nVery\\nWorried&quot;, &quot;2\\nSomewhat\\nWorried&quot;, &quot;3\\nNeutral&quot;, &quot;4\\nNot Very\\nWorried&quot;, &quot;5\\nNot At All\\nWorried&quot;)) + labs(title = &quot;Distribution of AI Job Worry Among Americans&quot;, subtitle = &quot;GSS 2024 (n = 1,536)&quot;, x = &quot;Level of Worry&quot;, y = &quot;Number of Respondents&quot;) + theme_minimal() + theme(axis.text.x = element_text(size = 10)) Look at that distribution! More people fall on the “worried” side (1-2) than the “not worried” side (4-5). But is this pattern statistically significant, or could it be due to random sampling variation? 19.5 The Five Steps of Hypothesis Testing Now we’ll apply the five-step framework you learned in the previous module to test whether Americans are significantly worried about AI. 19.5.1 Step 1: Check Assumptions Before running any test, we verify our assumptions: This is a random sample, so we have systematic randomness where each data can be considered somewhat an indpendent draw from a commmon population. (In more advanced classes, we would discuss the use of “survey weights”, which are used to correct for imballance in the demographic coverage of the poll. But let’s keep things simple for today.) We n &gt; 1,000, which meanse the CLT theorem applies. With n = 1,536, we can confidently use the Central Limit Theorem. The sampling distribution of our sample mean will be approximately normal, even though the underlying data is discrete (1-5 scale). 19.5.2 Step 2: State the Hypotheses We need to translate our research question into mathematical hypotheses. Remember, 3 is the neutral point on our scale. Null Hypothesis (H₀): μ ≥ 3 Americans are neutral or unconcerned about AI taking jobs Alternative Hypothesis (Hₐ): μ &lt; 3 Americans ARE worried about AI taking jobs This is a one-sided test because we have a specific direction in mind—we’re testing whether people are MORE worried than neutral, not just different from neutral. 19.5.3 Step 3: Calculate the Test Statistic The test statistic measures how many standard errors our sample mean is from the hypothesized value: # Calculate sample statistics x_bar &lt;- mean(gss_clean$aiworry) s &lt;- sd(gss_clean$aiworry) n &lt;- nrow(gss_clean) print(paste(&quot;Sample mean:&quot;, round(x_bar, 3))) ## [1] &quot;Sample mean: 2.24&quot; print(paste(&quot;Sample standard deviation:&quot;, round(s, 3))) ## [1] &quot;Sample standard deviation: 1.11&quot; print(paste(&quot;Sample size:&quot;, n)) ## [1] &quot;Sample size: 1536&quot; # Calculate standard error se &lt;- s / sqrt(n) print(paste(&quot;Standard error:&quot;, round(se, 3))) ## [1] &quot;Standard error: 0.028&quot; # Calculate test statistic (using z for now with large sample) mu_0 &lt;- 3 # Our null hypothesis value (neutral) z_stat &lt;- (x_bar - mu_0) / se print(paste(&quot;Test statistic (z):&quot;, round(z_stat, 3))) ## [1] &quot;Test statistic (z): -26.845&quot; Our test statistic is -26.845. This means our sample mean is about 26.8 standard errors BELOW the neutral point of 3. That’s quite far! 19.5.4 Step 4: Calculate the P-Value The p-value tells us: “If Americans were truly neutral about AI (H₀ true), what’s the probability of getting a sample mean this low or lower?” # Calculate p-value using normal distribution p_value_normal &lt;- pnorm(z_stat) print(paste(&quot;P-value (using normal):&quot;, round(p_value_normal, 6))) ## [1] &quot;P-value (using normal): 0&quot; The p-value is essentially 0! This means if Americans were truly neutral about AI, there’s almost no chance we’d randomly sample 1,536 people and find them this worried. 19.5.5 Step 5: Draw Conclusions Statistical Conclusion: We reject the null hypothesis (p &lt; 0.001). There is overwhelming statistical evidence that Americans are worried about AI job displacement. Practical Interpretation: The average American rates their worry at 2.24 on our 5-point scale, significantly below the neutral point of 3. This falls between “Very Worried” (1) and “Somewhat Worried” (2), suggesting substantial concern about AI’s impact on employment. 19.6 Refining Our Analysis: The t-Distribution So far, we’ve used the normal distribution, which is fine for large samples. But technically, we should use the t-distribution because we don’t know the true population standard deviation—we estimated it from our sample. We often don’t worry about this because the t-distribution and the standard normal distribution are very similar with large samples. 19.6.1 When and Why to Use t The t-distribution accounts for the extra uncertainty that comes from estimating the population standard deviation. It has “fatter tails” than the normal distribution, making it slightly harder to reject the null hypothesis. # Recalculate using t-distribution t_stat &lt;- (x_bar - mu_0) / se # Same calculation as z_stat df &lt;- n - 1 # Degrees of freedom # P-value using t-distribution p_value_t &lt;- pt(t_stat, df = df) # Compare the two approaches comparison &lt;- data.frame( Method = c(&quot;Normal (z)&quot;, &quot;t-distribution&quot;), Test_Statistic = c(z_stat, t_stat), P_Value = c(p_value_normal, p_value_t), Difference = c(0, p_value_t - p_value_normal) ) print(comparison) ## Method Test_Statistic P_Value Difference ## 1 Normal (z) -26.84479 4.851555e-159 0.000000e+00 ## 2 t-distribution -26.84479 9.082071e-131 9.082071e-131 With n = 1,536, the normal and t distributions give virtually identical results. The difference in p-values is negligible. But it can show up with smaller samples. If you want to be careful, you can just default to using the t-distribution. 19.7 Using R’s Built-in Function Now that we understand what’s happening under the hood, let’s use R’s convenient t.test() function: # One-sample t-test result &lt;- t.test(gss_clean$aiworry, mu = 3, # Testing against neutral alternative = &quot;less&quot;) # We hypothesize mean &lt; 3 # Display results print(result) ## ## One Sample t-test ## ## data: gss_clean$aiworry ## t = -26.845, df = 1535, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is less than 3 ## 95 percent confidence interval: ## -Inf 2.286204 ## sample estimates: ## mean of x ## 2.239583 Let’s extract the key components: Here’s the test statistic result$statistic ## t ## -26.84479 Here’s the degrees of freedom result$parameter ## df ## 1535 Here’s the p-value result$p.value ## [1] 9.082071e-131 Here’s the sample mean result$estimate ## mean of x ## 2.239583 Notice the confidence interval is entirely below 3, which aligns with our rejection of \\(H_0\\). Note that the lower end of the confidence interval is not defined because we are doing a one sided test. 19.8 One-Sided vs Two-Sided Tests We used a one-sided test because we specifically hypothesized that Americans would be worried (mean &lt; 3). But what if we just wanted to test whether opinions differ from neutral in either direction? # Two-sided test result_two &lt;- t.test(gss_clean$aiworry, mu = 3, alternative = &quot;two.sided&quot;) print(result_two) ## ## One Sample t-test ## ## data: gss_clean$aiworry ## t = -26.845, df = 1535, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 3 ## 95 percent confidence interval: ## 2.184021 2.295146 ## sample estimates: ## mean of x ## 2.239583 In this case, the two-sided p-value is twice the one-sided p-value (when the result is in the predicted direction). Both lead to the same conclusion here because the evidence is so strong. When to use each: One-sided: You have a specific directional hypothesis based on theory Two-sided: You’re exploring whether there’s any difference Default: When in doubt, use two-sided (more conservative) 19.9 Review Questions 19.9.1 Conceptual Understanding Why did we test against 3 rather than another value like 2 or 4? Hint: Think about what 3 represents on our scale What would change if we used a two-sided test instead of one-sided? Consider both the p-value and the interpretation Why do pnorm() and pt() give nearly identical p-values with our sample? Think about sample size and distribution properties If only 20 people had answered the survey, what would change in our analysis? Consider assumptions, distribution choice, and confidence in results What does it mean that Americans average 2.24 on this 5-point scale? Interpret in plain English for a non-statistician 19.9.2 Practical Application What policy recommendations might follow from our finding? Connect statistics to real-world implications If we wanted 99% confidence instead of 95%, what would change? Think about Type I error, p-values, and conclusions 19.9.3 Technical Skills Write R code to test if the mean worry is significantly different from 2 (very worried). If the standard error were 0.5 instead of what we calculated, how would this affect our test? "],["two-sample-hypothesis-testing-comparing-means-between-groups.html", "20 Two-Sample Hypothesis Testing: Comparing Means Between Groups 20.1 Comparing Means: The Next Level 20.2 What Information Goes Into Comparing Means? 20.3 The Hypothesis Testing Framework 20.4 The Tale of Two Formulas 20.5 Putting It All Into Practice 20.6 A Real Example: Can Social Pressure Get Out the Vote? 20.7 Running Our First T-test 20.8 Understanding the t.test() Function 20.9 Comparing Specific Treatments 20.10 What’s Actually Happening Under the Hood? 20.11 Review Questions", " 20 Two-Sample Hypothesis Testing: Comparing Means Between Groups knitr::opts_chunk$set(echo = TRUE) 20.1 Comparing Means: The Next Level Up until now, we’ve been stuck in a one-sample world. We’ve learned how to test whether the mean of our sample is different from some hypothesized value. That’s useful, but let’s be honest—it’s not where the real action is in social science research. The questions that really keep us up at night aren’t about comparing our data to some theoretical benchmark. Instead, we want to know things like: Does studying actually help you get better grades? Do attack ads really make people less likely to vote for a candidate? Does drinking coffee make you more productive, or does it just make you think you’re more productive? These are all questions about comparing two groups. And that’s exactly what we’re going to learn how to do today. 20.2 What Information Goes Into Comparing Means? When we compare two groups, we need to know four basic things about our data: The mean for sample 1 (\\(\\bar{y}_1\\)) The mean for sample 2 (\\(\\bar{y}_2\\)) The standard deviation for sample 1 (\\(S_1\\)) The standard deviation for sample 2 (\\(S_2\\)) Oh, and we also need to know how many observations we have in each group (\\(n_1\\) and \\(n_2\\)). That’s going to matter a lot, as we’ll see in a minute. 20.3 The Hypothesis Testing Framework Here’s the wonderful thing: the basic logic of hypothesis testing doesn’t change just because we have two groups. We’re still using the same fundamental formula: \\[\\text{Test Statistic} = \\frac{(\\text{Estimate} - \\text{Null})}{\\text{Standard Error}}\\] What’s different is what goes into each piece. Our estimate is now the difference between the two sample means: \\((\\bar{y}_1 - \\bar{y}_2)\\). The null hypothesis is usually that this difference equals zero—in other words, that there’s no real difference between the groups. And the standard error… well, that’s where things get interesting. 20.4 The Tale of Two Formulas Here’s where sample size becomes crucial. If both your samples are reasonably large (let’s say n &gt; 30), life is good. The Central Limit Theorem has your back, and you can use this relatively simple formula for the standard error: \\[SE = \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\\] This is basically saying: the uncertainty in the difference comes from the uncertainty in each group’s mean, and we add them together (well, we add the variances and then take the square root, but you get the idea). But what if you have small samples? Now things get trickier. With small samples, we can’t count on the Central Limit Theorem to make everything normal. Instead, we will use the t-distribution (implicitly assuming that the data is normal), and we use something called the pooled variance: \\[\\hat{\\sigma}^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\\] This is essentially a weighted average of the two sample variances. We then use this to calculate our standard error: \\[SE = \\hat{\\sigma}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\] And instead of using the normal distribution, we use the t-distribution with \\(n_1 + n_2 - 2\\) degrees of freedom. 20.5 Putting It All Into Practice Let’s walk through the process step by step: Step 1: Assumptions First, figure out if you’re in the large sample or small sample world. If either group has fewer than 30 observations, you’re in small sample territory. Step 2: State Your Hypotheses The null hypothesis is almost always that the means are equal: \\(H_0: \\mu_1 - \\mu_2 = 0\\). The alternative is that they’re different: \\(H_a: \\mu_1 - \\mu_2 \\neq 0\\) (though sometimes you might have a specific direction in mind). Step 3: Calculate the Test Statistic Take the difference in sample means and divide by the standard error. Which standard error formula you use depends on your sample size. Step 4: Find the P-value For large samples, use pnorm(). For small samples, use pt() with the appropriate degrees of freedom. Don’t forget to multiply by 2 for a two-sided test! Step 5: Make Your Decision If the p-value is less than your significance level (usually 0.05), reject the null. Otherwise, fail to reject. 20.6 A Real Example: Can Social Pressure Get Out the Vote? Enough with the formulas—let’s see this in action with some real data! We’re going to analyze a fascinating experiment about voting and social pressure. 20.6.1 Gerber, Green, &amp; Larimer GOTV Data We are going to discuss an experiment on social pressure. (We will talk about this more when we talk about causality.) Here is a link to the data. In 2006, one of three mailers was sent out as part of a study to voters. There were three different kinds of messages: Civic Duty (It’s your duty to vote) Hawthorne (You’re being studied so go vote) Neighbors (Your neighbors will know if you voted) First let’s read in the data and look at it: # Load our tools library(dplyr) social &lt;- read.csv(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/social.csv&quot;) # Take a peek social %&gt;% glimpse() ## Rows: 305,866 ## Columns: 6 ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;fem… ## $ yearofbirth &lt;int&gt; 1941, 1947, 1951, 1950, 1982, 1981, 1959, 1956, 1968, 1967, 1941, 1945, 1949… ## $ primary2004 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ messages &lt;chr&gt; &quot;Civic Duty&quot;, &quot;Civic Duty&quot;, &quot;Hawthorne&quot;, &quot;Hawthorne&quot;, &quot;Hawthorne&quot;, &quot;Control&quot;… ## $ primary2006 &lt;int&gt; 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0… ## $ hhsize &lt;int&gt; 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 4, 4, 4, 4, 2, 2… Let’s see what messages were sent out: social %&gt;% count(messages) ## messages n ## 1 Civic Duty 38218 ## 2 Control 191243 ## 3 Hawthorne 38204 ## 4 Neighbors 38201 So we have a control group that got nothing, and three different treatment messages. That’s a lot to keep track of, so let’s simplify things by combining all the treatment groups together: # Create a simple treatment vs control variable social &lt;- social %&gt;% mutate(messages2 = case_when( messages %in% c(&quot;Hawthorne&quot;, &quot;Civic Duty&quot;, &quot;Neighbors&quot;) ~ &quot;Treatment&quot;, messages == &quot;Control&quot; ~ &quot;Control&quot; )) # Check our work social %&gt;% count(messages2) ## messages2 n ## 1 Control 191243 ## 2 Treatment 114623 20.7 Running Our First T-test Now for the moment of truth. Did these messages actually work? Let’s find out: t.test(primary2006 ~ messages2, alternative = &quot;two.sided&quot;, var.equal = FALSE, data = social) ## ## Welch Two Sample t-test ## ## data: primary2006 by messages2 ## t = -23.869, df = 234582, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group Control and group Treatment is not equal to 0 ## 95 percent confidence interval: ## -0.04506412 -0.03822505 ## sample estimates: ## mean in group Control mean in group Treatment ## 0.2966383 0.3382829 Whoa! Look at that t-statistic: -23.869. That’s huge! And the p-value is basically zero (2.2 × 10^-16 is scientific notation for “really, really, really small”). What does this tell us? The treatment groups had significantly different turnout than the control group. In fact, looking at the sample means, the control group had about 29.7% turnout while the treatment groups had about 31.9% turnout. That might not sound like much, but in the world of voter mobilization, a 2.2 percentage point increase is a big deal! 20.8 Understanding the t.test() Function The t.test() function in R is pretty flexible. Let me show you some options: For different hypotheses: You can change the alternative argument depending on what you’re testing. Use \"two.sided\" when you just want to know if the groups are different (this is usually what you want). Use \"greater\" or \"less\" if you have a specific direction in mind. For variance assumptions: The var.equal argument tells R whether to assume the two groups have equal variance. I recommend always using var.equal = FALSE unless you have a really good reason not to. It’s the safer choice. But note that the formula I taught you above is for when the variances are equal (just because the math is easier). 20.9 Comparing Specific Treatments What if we want to dig deeper and compare specific treatments to each other? Let’s see if the “Neighbors” treatment (the aggressive one) was more effective than the “Civic Duty” treatment (the gentle one): # Extract just the voters who got these two treatments neighbors_turnout &lt;- social %&gt;% filter(messages == &quot;Neighbors&quot;) %&gt;% pull(primary2006) civic_turnout &lt;- social %&gt;% filter(messages == &quot;Civic Duty&quot;) %&gt;% pull(primary2006) # Compare them t.test(x = neighbors_turnout, y = civic_turnout) ## ## Welch Two Sample t-test ## ## data: neighbors_turnout and civic_turnout ## t = 18.463, df = 76271, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.05667888 0.07014226 ## sample estimates: ## mean of x mean of y ## 0.3779482 0.3145377 Look at that! The “Neighbors” treatment had about 37.8% turnout compared to 31.5% for “Civic Duty”. The p-value is again essentially zero, so this difference is highly significant. Social pressure works! 20.10 What’s Actually Happening Under the Hood? Let’s peek behind the curtain and calculate the test statistic manually (now assuming a shared variance across groups). This will help you understand what R is actually doing: # Get summary statistics for each group results &lt;- social %&gt;% group_by(messages2) %&gt;% summarise( mean = mean(primary2006), sd = sd(primary2006), n = n() ) # Take a look results ## # A tibble: 2 × 4 ## messages2 mean sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Control 0.297 0.457 191243 ## 2 Treatment 0.338 0.473 114623 Now let’s do the calculation ourselves: # Extract the values we need mean_treatment &lt;- results$mean[results$messages2 == &quot;Treatment&quot;] mean_control &lt;- results$mean[results$messages2 == &quot;Control&quot;] sd_treatment &lt;- results$sd[results$messages2 == &quot;Treatment&quot;] sd_control &lt;- results$sd[results$messages2 == &quot;Control&quot;] n_treatment &lt;- results$n[results$messages2 == &quot;Treatment&quot;] n_control &lt;- results$n[results$messages2 == &quot;Control&quot;] # The difference in means diff_means &lt;- mean_treatment - mean_control cat(&quot;Difference in means:&quot;, diff_means, &quot;\\n&quot;) ## Difference in means: 0.04164458 # The standard error (using the large sample formula) se &lt;- sqrt(sd_treatment^2/n_treatment + sd_control^2/n_control) cat(&quot;Standard error:&quot;, se, &quot;\\n&quot;) ## Standard error: 0.001744682 # The test statistic t_stat &lt;- diff_means / se cat(&quot;Test statistic:&quot;, t_stat, &quot;\\n&quot;) ## Test statistic: 23.86944 # The p-value (using normal approximation since we have huge samples) p_value &lt;- 2 * pnorm(abs(t_stat), lower.tail = FALSE) cat(&quot;P-value:&quot;, p_value, &quot;\\n&quot;) ## P-value: 6.362432e-126 Look at that—we got essentially the same answer as t.test()! The tiny differences are just due to R using a slightly more sophisticated method for calculating degrees of freedom. 20.11 Review Questions 20.11.1 Conceptual Questions When would you use a two-sample t-test instead of a one-sample t-test? Give me a specific example from political science research where you’d need to compare two groups. What is the null hypothesis in a two-sample t-test? Write it out in both words and mathematical notation. Why do we usually assume the difference is zero? Why do we use different formulas for small samples (n &lt; 30) versus large samples? If the p-value from a t-test is 0.03 and your significance level is 0.05, what do you conclude? What if your significance level was 0.01? How would you explain this to someone who’s never taken statistics? 20.11.2 Calculation Practice By hand calculation: You’re comparing study methods. Group A (flashcards) has n = 40, mean = 85, sd = 8. Group B (re-reading) has n = 35, mean = 81, sd = 10. Calculate the test statistic. Is this a large or small sample case? Interpreting R output: Your colleague runs a t-test and gets: t = 3.45, df = 58, p-value = 0.001 95% CI: (2.3, 8.7) mean of x: 42.5 mean of y: 37.0 What’s the difference in means? Would you reject the null at α = 0.05? What does that confidence interval actually tell you? 20.11.3 R Programming Questions Diving deeper into the GOTV data: Using the social pressure data, test whether the “Neighbors” treatment had a different effect than the “Hawthorne” treatment. Write the code and tell me what you find. # Your code here "],["causality---understanding-cause-and-effect.html", "21 Causality - Understanding Cause and Effect 21.1 Learning Objectives 21.2 What Is This For? 21.3 Part 1: Prediction vs. Causation 21.4 Part 2: The Fundamental Problem of Causal Inference 21.5 Part 3: Average Treatment Effects 21.6 Part 4: Confounding and Causal Relationships 21.7 Part 5: Establishing Causation 21.8 Part 6: Two Paths to Causal Inference 21.9 Conclusion 21.10 Study Questions", " 21 Causality - Understanding Cause and Effect knitr::opts_chunk$set(echo = TRUE) 21.1 Learning Objectives Understand the fundamental difference between causation and prediction Master the concept of potential outcomes and counterfactuals Recognize common confounding relationships and why they matter Understand the fundamental problem of causal inference Grasp the concept of average treatment effects (ATE) Learn why randomization is the gold standard for causal inference 21.2 What Is This For? Every day, we’re bombarded with claims about cause and effect. Coffee causes cancer. No wait, coffee prevents cancer. Social media causes depression. Tax cuts create jobs. Campaign ads change minds. These aren’t just academic questions. Causal claims in empirical science shape personal decisions and public policy worth trillions of dollars. Unfortunately, many of these claims are wrong, or at least unproven. The problem isn’t that the researchers are incompetent (well, usually not). The problem is that figuring out what causes what is incredibly, fundamentally difficult. Today we’re going to understand why that is, and what we can do about it. Let me start with my favorite example of why this matters. Years ago, journalist Thomas Friedman noticed something fascinating: no two countries that both had McDonald’s restaurants had ever gone to war with each other. This led to what he called the “Golden Arches Theory of Conflict Prevention.” The pattern held for years – it was a real, observable correlation. So should we achieve world peace by opening more McDonald’s franchises? Should the UN budget include a line item for Big Macs? Of course not. The presence of McDonald’s doesn’t cause peace. Instead, countries wealthy and developed enough to have McDonald’s also tend to be economically interconnected in ways that make war costly. The McDonald’s is just along for the ride. This distinction between correlation and causation isn’t just academic. It’s the difference between policies that work and policies that waste resources. It’s the difference between medical treatments that heal and ones that just happen to be popular among people who were going to get better anyway. 21.3 Part 1: Prediction vs. Causation Let’s get precise about what we mean by prediction versus causation. These are fundamentally different concepts, and confusing them causes endless trouble. 21.3.1 What is Prediction? Prediction is actually the simpler concept. We say that variable X predicts variable Y if knowing the value of X helps us make a better guess about the value of Y. That’s it. No deep philosophical commitment required. For example, your credit score predicts whether you’ll file a car insurance claim. This might seem weird – what does paying your credit card bills on time have to do with your driving? But insurance companies have discovered that people with low credit scores file more claims on average. They don’t need to know why this relationship exists. They just need to know that it does exist, reliably enough to set premiums. Here’s what’s crucial: prediction requires zero causal interpretation. Ice cream sales predict drowning deaths (both go up in summer). The number of Nicolas Cage movies in a year used to predict swimming pool drownings (this is real data, and yes, it’s as ridiculous as it sounds). Your height predicts your salary. None of these relationships are causal, but they’re all real correlations in the data. 21.3.2 What is Causation? Causation is much more demanding. We say X causes Y if changing X would lead to a change in Y. Notice that key word: “would.” Causation is fundamentally about counterfactuals – things that didn’t happen but could have. Let’s make this concrete with the 2008 financial crisis. Congress passed a massive stimulus package to create jobs. Did it work? To answer that question causally, we need to know what would have happened if Congress hadn’t passed the stimulus. If unemployment would have hit 20% without the stimulus and only reached 10% with it, then the stimulus prevented mass unemployment. If unemployment would have stayed at 5% without the stimulus but jumped to 10% with it, then the stimulus was actively harmful. See the problem? We can’t observe both realities. We got the stimulus, unemployment hit 10%, and we’ll never know what the alternative timeline looked like. This is why economists still argue about the stimulus twenty years later. 21.3.3 Why the Distinction Matters The distinction between prediction and causation matters because they lead to completely different actions. If you want to predict which students will struggle in your class, you might look at their previous grades, their attendance, maybe even seemingly random things like whether they sit in the front row. All of these predict academic performance. But if you want to help struggling students improve, you need to know what causes poor performance. Moving a struggling student to the front row probably won’t magically improve their grades – sitting in front is more likely a signal of motivation than a cause of success. You need to identify actual causal factors: maybe they need tutoring, maybe they’re working too many hours at a job, maybe they have an undiagnosed learning difference. Insurance companies can use credit scores for prediction because they just want to estimate risk. But a government trying to reduce traffic accidents needs to understand causation. Banning people with bad credit from driving would be both unfair and ineffective. You need to address actual causes like drunk driving, distracted driving, poor road design. 21.4 Part 2: The Fundamental Problem of Causal Inference Now we’re ready to tackle the core challenge that makes causal inference so difficult. It’s called the fundamental problem of causal inference, and once you understand it, you’ll see why so many causal claims you encounter are probably wrong. 21.4.1 Potential Outcomes Framework To think clearly about causation, we need to introduce some notation. Don’t worry – the math is simple, but it helps us be precise about concepts that are easy to muddle in plain English. Let’s say we want to know whether a new tutoring program improves test scores. For each student, we’ll denote the treatment as T, where T = 1 means the student gets tutoring and T = 0 means they don’t. Now here’s the key insight: each student has two potential outcomes. We’ll write these as Y(1) and Y(0). The outcome Y(1) is the test score the student would get if they received tutoring. The outcome Y(0) is the test score that same student would get if they didn’t receive tutoring. For any individual student i, the causal effect of tutoring would be: \\[\\text{Individual Causal Effect} = Y_i(1) - Y_i(0)\\] This is just the difference between what happens with treatment and what happens without it. If Sarah would score 85 with tutoring and 75 without it, then the causal effect of tutoring for Sarah is 10 points. 21.4.2 The Core Problem Here’s where things get frustrating: we can never observe both Y(1) and Y(0) for the same person. Sarah either gets tutoring or she doesn’t. We see one potential outcome, and the other remains forever counterfactual – something that could have happened but didn’t. This isn’t a problem we can solve with better data or fancier statistics. It’s baked into the nature of reality. We don’t have access to parallel universes where we can run different versions of history and see what happens. The movie “It’s a Wonderful Life” provides a perfect illustration. George Bailey wishes he’d never been born, and an angel shows him what his town would look like if he’d never existed. He sees his brother dead (because George wasn’t there to save him from drowning), his wife unmarried, his town turned into a slum. The difference between the actual world and this counterfactual world represents George Bailey’s causal effect on his community. That’s a beautiful story, but we don’t have guardian angels to show us counterfactual worlds. When we give a student tutoring, we can’t see what their score would have been without it. When Congress passes a stimulus, we can’t see what unemployment would have been without it. 21.4.3 What We Actually Observe Let me show you what this looks like with some example data. Imagine we have six students, and we’re studying the effect of tutoring on test scores: # Create example data showing potential outcomes students &lt;- data.frame( Student = c(&quot;Amy&quot;, &quot;Bob&quot;, &quot;Carlos&quot;, &quot;Diana&quot;, &quot;Eve&quot;, &quot;Frank&quot;), Treated = c(1, 0, 1, 0, 1, 0), Y_1 = c(85, 78, 92, 88, 73, 81), # Potential outcome with treatment Y_0 = c(75, 72, 80, 85, 65, 79), # Potential outcome without treatment True_Effect = c(10, 6, 12, 3, 8, 2) ) # What we wish we could see print(&quot;What we wish we could see (both potential outcomes):&quot;) ## [1] &quot;What we wish we could see (both potential outcomes):&quot; students ## Student Treated Y_1 Y_0 True_Effect ## 1 Amy 1 85 75 10 ## 2 Bob 0 78 72 6 ## 3 Carlos 1 92 80 12 ## 4 Diana 0 88 85 3 ## 5 Eve 1 73 65 8 ## 6 Frank 0 81 79 2 In this magical world where we can see everything, we could calculate that tutoring helps Amy by 10 points, Bob by 6 points, Carlos by 12 points, and so on. But here’s what we actually get to see in the real world: # What we actually observe observed &lt;- students[,1:2] observed$Observed_Score &lt;- ifelse(students$Treated == 1, students$Y_1, students$Y_0) print(&quot;What we actually observe:&quot;) ## [1] &quot;What we actually observe:&quot; observed ## Student Treated Observed_Score ## 1 Amy 1 85 ## 2 Bob 0 72 ## 3 Carlos 1 92 ## 4 Diana 0 85 ## 5 Eve 1 73 ## 6 Frank 0 79 Amy got tutoring and scored 85. Would she have scored 75 without tutoring? We’ll never know. Bob didn’t get tutoring and scored 72. Would he have scored 78 with tutoring? Again, we’ll never know. This is the fundamental problem of causal inference: causal effects are defined in terms of counterfactuals we can’t observe. 21.5 Part 3: Average Treatment Effects Since we can’t measure individual causal effects, we need a different approach. This is where the concept of average treatment effects comes in, and it’s how most modern causal inference works. 21.5.1 From Individual to Average Effects While we can’t calculate the individual causal effect for Amy, we can try to estimate the average causal effect across all students. The Average Treatment Effect (ATE) is defined as: \\[\\text{ATE} = \\frac{1}{n} \\sum_{i=1}^{n} [Y_i(1) - Y_i(0)]\\] In plain English, this is the average of all the individual causal effects. If we could magically observe all potential outcomes, we’d calculate the treatment effect for each person and then take the average. Of course, we still can’t observe all potential outcomes, so we can’t calculate this directly. But – and this is the key insight – we can estimate it under certain conditions. Instead of the true ATE, we calculate: \\[\\widehat{\\text{ATE}} = \\frac{1}{n_1} \\sum_{\\{T_i=1\\}} Y_i(1) - \\frac{1}{n_0} \\sum_{\\{T_i=0\\}} Y_i(0)\\] This looks more complicated than it is. We’re just taking the average outcome for people who got treatment minus the average outcome for people who didn’t get treatment. Using our tutoring example: # Calculate the estimated ATE from observed data treated_scores &lt;- observed$Observed_Score[observed$Treated == 1] control_scores &lt;- observed$Observed_Score[observed$Treated == 0] mean_treated &lt;- mean(treated_scores) mean_control &lt;- mean(control_scores) estimated_ate &lt;- mean_treated - mean_control cat(&quot;Average score with tutoring:&quot;, mean_treated, &quot;\\n&quot;) ## Average score with tutoring: 83.33333 cat(&quot;Average score without tutoring:&quot;, mean_control, &quot;\\n&quot;) ## Average score without tutoring: 78.66667 cat(&quot;Estimated Average Treatment Effect:&quot;, estimated_ate, &quot;\\n&quot;) ## Estimated Average Treatment Effect: 4.666667 # What&#39;s the true ATE with our magical complete data? true_ate &lt;- mean(students$True_Effect) cat(&quot;True ATE (if we could see all potential outcomes):&quot;, true_ate, &quot;\\n&quot;) ## True ATE (if we could see all potential outcomes): 6.833333 If this all looks pretty familliar, that’s because it is. This is just the difference-in-means estimator we have already looked at in this class. To calculate the ATE, we just need to subtract the means for each group. But this won’t work all of the time. We will need some additional assumptions. 21.5.2 The Key Assumption Notice that our estimate is pretty close to the truth in this example. But that’s only because I generated the data in a special way. In the real world, this estimate is only valid if the treated and control groups are comparable. What does “comparable” mean? It means there are no systematic differences between the groups beyond the treatment itself that are related to the outcome. In our tutoring example, the estimate would be biased if, say, struggling students were more likely to seek tutoring. Then the tutoring group would have scored lower than the control group even without any tutoring, and we’d underestimate (or even reverse) the true effect. This is worth stating formally: the simple difference in means gives us a valid estimate of the causal effect only when assignment to treatment is independent of the potential outcomes. When this condition fails, we have what’s called confounding or selection bias. 21.6 Part 4: Confounding and Causal Relationships Now we need to talk about the various ways that correlations can exist without causation. Understanding these patterns is crucial for recognizing when a correlation might be causal and when it’s definitely not. 21.6.1 Common Causal Patterns The simplest case is direct causation, where X actually causes Y. We can represent this with an arrow: X → Y. Campaign donations might cause electoral success. Smoking causes lung cancer. Practice causes improvement. These are genuine causal relationships where changing X would change Y. But there are several other patterns that create correlations without direct causation. The most important is the spurious relationship, where some third factor Z causes both X and Y. We write this as: Z → X and Z → Y. Summer weather causes both ice cream sales and swimming, which causes both ice cream sales and drowning deaths. There’s a strong correlation between ice cream sales and drowning, but buying ice cream doesn’t make you drown, and drowning doesn’t make people buy ice cream. The correlation is real, but it’s not causal. Another pattern is chain relationships, where X causes Z which causes Y. We write this as X → Z → Y. Education might increase income, and income might improve health, creating a correlation between education and health. Education does affect health, but only indirectly through income. These different patterns matter enormously for policy and intervention. If ice cream caused drowning (direct causation), we should ban ice cream. If summer weather causes both (spurious relationship), banning ice cream would accomplish nothing. If education improves health through income (chain relationship), then education would only help health if it actually increased income – getting a degree without improved earnings wouldn’t help. 21.6.2 Real-World Example: The Google Memo A few years ago, an engineer at Google wrote a memo arguing that women were underrepresented in tech because of biological differences in personality and interests. Women, he argued, were more interested in people than things, preferred work-life balance, and were more neurotic. Since these differences could explain why fewer women worked in tech, he concluded that diversity efforts were misguided. The memo caused a huge controversy, and Google fired the engineer. Some people were outraged at Google – didn’t the engineer cite real psychological research? Wasn’t Google punishing him for following the science? Here’s the problem: the engineer had confused correlation for causation in the presence of an obvious confounder. Yes, researchers have found population-level average differences between men and women on various psychological measures. And yes, these differences correlate with representation in tech. But consider this alternative explanation: Gender norms in our society (let’s call this factor Z) might cause both psychological differences (X) and career outcomes (Y). Maybe growing up in a world that tells girls they’re not good at math makes them less interested in technical subjects. Maybe seeing few female role models in tech makes women prefer other careers. Maybe workplace cultures that assume long hours are possible because someone else is handling childcare create that “preference” for work-life balance. To make this absurdly clear, let me propose an equally “scientific” argument: Women have longer hair than men on average. Long hair gets in the way of looking at computer screens. Therefore, women are biologically less suited for programming. This is obviously ridiculous, but it follows the exact same logical structure as the original memo. We have a real correlation (hair length correlates with tech employment), but the causal interpretation is nonsense because we’re ignoring the confounding factor of social gender norms. 21.7 Part 5: Establishing Causation Given all these challenges, how do we ever establish causation? There are three things we need to demonstrate, and the third one is by far the hardest. 21.7.1 Three Requirements for Causal Claims First, we need to establish temporal order. For X to cause Y, X must come before Y in time. This might seem obvious, but it’s not always clear in observational data. Do democratic countries become wealthy, or do wealthy countries become democratic? Does depression cause social media use, or does social media use cause depression? Without clear temporal ordering, we can’t even begin to make causal claims. Second, we need to show an association. If X causes Y, then X and Y should be correlated in our data. This is what most of statistics is about – detecting patterns and relationships in data. We can calculate correlations, run regressions, perform t-tests. But remember association is an important signal for causation, but it’s not sufficient. Correlation does not imply causation, even when the correlation is highly statistically significant. Third, and most challenging, we need to eliminate alternative explanations. This is where most causal arguments fail. Even if X comes before Y and X correlates with Y, there might be some confounding factor Z that explains the relationship. Maybe people who do X are different from people who don’t in some other way that affects Y. Maybe some external factor causes both X and Y. Maybe the relationship runs in the opposite direction. 21.7.2 The Challenge of Confounders The burden of proof is on the researcher making the causal claim. You need to convince skeptics that you’ve thought of all the relevant confounders and dealt with them. This is incredibly difficult because the world is complex and interconnected. Consider a simple question: does getting a college degree increase earnings? College graduates earn more than non-graduates, so there’s definitely an association. College comes before career earnings, so temporal order is established. But what about alternative explanations? Maybe people who go to college are smarter, and they would have earned more anyway. Maybe they’re more motivated. Maybe they come from wealthier families with better connections. Maybe they’re healthier, or more confident, or better at standardized tests, or live in areas with better job markets, or any of a thousand other differences that could affect earnings. This is why the phrase “all else being equal” appears so often in social science. We want to know the effect of college on earnings, all else being equal. But in observational data, all else is never equal. 21.8 Part 6: Two Paths to Causal Inference Given these challenges, how do we ever make valid causal claims? There are two main approaches, and they both try to make the treatment and control groups comparable in different ways. 21.8.1 Path 1: Randomization The gold standard for causal inference is the randomized controlled trial (RCT), also known as an experiment. The beautiful thing about randomization is that it breaks the connection between treatment assignment and potential outcomes. Here’s how it works. Take your sample and randomly assign some units to treatment and others to control. Since assignment is random, there’s no systematic difference between the groups. Motivated students are equally likely to end up in treatment or control. Wealthy students are equally likely to be in each group. Every possible confounder, whether we’ve thought of it or not, is balanced across the groups on average. Let me demonstrate this with a simulation: # Simulate an experiment n &lt;- 1000 # Create potential confounders motivation &lt;- rnorm(n) ability &lt;- rnorm(n) family_income &lt;- rnorm(n) # Potential outcomes depend on confounders y0 &lt;- 50 + 5*motivation + 3*ability + 2*family_income + rnorm(n, 0, 5) y1 &lt;- y0 + 10 # True treatment effect is 10 # Random assignment treatment &lt;- sample(c(0, 1), n, replace = TRUE) # Observed outcome y_observed &lt;- ifelse(treatment == 1, y1, y0) # Compare treated and control mean_treated &lt;- mean(y_observed[treatment == 1]) mean_control &lt;- mean(y_observed[treatment == 0]) estimated_effect &lt;- mean_treated - mean_control cat(&quot;Estimated treatment effect from experiment:&quot;, round(estimated_effect, 2), &quot;\\n&quot;) ## Estimated treatment effect from experiment: 9.92 cat(&quot;True treatment effect: 10\\n&quot;) ## True treatment effect: 10 # Check balance on confounders cat(&quot;\\nChecking balance on confounders:\\n&quot;) ## ## Checking balance on confounders: cat(&quot;Motivation difference:&quot;, round(mean(motivation[treatment == 1]) - mean(motivation[treatment == 0]), 3), &quot;\\n&quot;) ## Motivation difference: 0.097 cat(&quot;Ability difference:&quot;, round(mean(ability[treatment == 1]) - mean(ability[treatment == 0]), 3), &quot;\\n&quot;) ## Ability difference: 0.012 cat(&quot;Income difference:&quot;, round(mean(family_income[treatment == 1]) - mean(family_income[treatment == 0]), 3), &quot;\\n&quot;) ## Income difference: -0.083 See how the randomization balanced all the confounders across treatment and control? The differences are tiny, just random noise. This is why experiments give us valid causal estimates even when we don’t know what all the confounders are. Sometimes we can’t randomize treatment ourselves, but nature or policy does it for us. These are called natural experiments or quasi-experiments. Maybe admission to a school is determined by a cutoff score, and students just above and below the cutoff are essentially similar. Maybe a new law is implemented in some states but not others for arbitrary political reasons. These situations can provide experiment-like conditions without actual randomization. 21.8.2 Path 2: Statistical Adjustment When we can’t randomize, we try to adjust for confounders statistically. This is what most empirical social science does. We measure potential confounders and include them in our statistical models, trying to compare “like with like.” The problem is that we can only adjust for confounders we’ve measured. If we’re studying the effect of college on earnings and we control for test scores, family income, and motivation, we’re still vulnerable to unmeasured confounders like health, personality, or social connections. With statistical adjustment (like regression, which we’ll introduce next), we could try to control for confoudners and get closer to the truth. But only if we’ve measured ability accurately and included it in our model. 21.9 Conclusion Causation is one of the most important concepts in data science, and also one of the most challenging. The fundamental problem of causal inference – that we can’t observe counterfactuals – means we can never directly measure causal effects. Instead, we have to be clever about study design and careful about our assumptions. The key takeaways are these: First, correlation is not causation, and the difference between prediction and causation matters enormously for decision-making. Second, randomization is the gold standard for causal inference because it balances all confounders, even ones we haven’t thought of. Third, when we can’t randomize, we need to think carefully about what might confound our comparisons and do our best to adjust for these factors. Most importantly, be skeptical of causal claims. When someone says X causes Y, ask yourself: How do they know? Could there be confounders they haven’t considered? Would the groups being compared be similar in the absence of treatment? The world is full of spurious correlations and confounded relationships. A little skepticism goes a long way. 21.10 Study Questions 21.10.1 Understanding Concepts What is the fundamental problem of causal inference? Explain why we can never directly observe the causal effect of a treatment on an individual. Explain the difference between prediction and causation. Give an example of something that would be useful for prediction but not for making policy decisions. What are potential outcomes? If we’re studying whether a job training program increases employment, define Y(0) and Y(1) for an individual participant. 21.10.2 Identifying Relationships A study finds that cities with more police officers have higher crime rates. Draw arrows showing at least two possible explanations for this correlation. Which explanation is direct causation, and which shows confounding? Students who eat breakfast tend to get better grades. List three possible explanations for this pattern: one involving direct causation, one involving reverse causation, and one involving a confounding variable. Draw the arrows for each. Researchers notice that countries with more chocolate consumption have more Nobel Prize winners per capita. What’s the most likely explanation for this correlation? What would be the confounder? 21.10.3 Applying Knowledge A researcher wants to study whether online classes lead to worse learning outcomes than in-person classes. What would be: The treatment variable (T)? The potential outcomes Y(1) and Y(0)? The average treatment effect (ATE)? Two potential confounders that could bias the comparison? Your friend says, “I started taking vitamin C and haven’t gotten sick since. Vitamin C prevents illness!” What’s wrong with this causal reasoning? How would you properly test whether vitamin C prevents illness? 21.10.4 Critical Thinking A city notices that streets with more bike lanes have fewer traffic accidents and concludes that bike lanes make streets safer. A skeptic argues that the city probably put bike lanes on streets that were already safer. How could you determine who’s right? What would an ideal experiment look like? During the COVID-19 pandemic, some argued that lockdowns didn’t work because places with strict lockdowns still had high case rates. Explain why this comparison might be confounded. What would you need to know to properly evaluate the causal effect of lockdowns? 21.10.5 Working with Data Suppose you have data on 1000 students. Half received free tutoring (randomly assigned) and half didn’t. The tutored group has an average test score of 78, and the control group has an average of 72. What is the estimated average treatment effect? Why can we interpret this difference as causal? If tutoring hadn’t been randomly assigned, what problems might arise? A company wants to know if their new website design increases sales. They switch to the new design on January 1st and notice sales are 20% higher in January than in December. Why can’t they conclude the new design caused the increase? Design a better test of the website’s effect. "],["correlation-and-bivariate-regression.html", "22 Correlation and Bivariate Regression 22.1 Learning Objectives 22.2 What Is This For? 22.3 Loading the Data: Economic Voting in Presidential Elections 22.4 From Groups to Relationships 22.5 Correlation Coefficients 22.6 Limitations of Correlation 22.7 From Correlation to Regression 22.8 Simple Linear Regression 22.9 Summary and Key Takeaways 22.10 Review/Study Questions", " 22 Correlation and Bivariate Regression knitr::opts_chunk$set(echo = TRUE) 22.1 Learning Objectives By the end of this module, you will understand: What correlation measures and how to interpret correlation coefficients. How correlation relates to linear regression and develop an intuition for how least squares regression works. Learn how to fit and interpret simple regression models in R and practice visualizing relationships between two continuous variables. 22.2 What Is This For? So far in this course, we have learned how to compare outcomes across groups using tools like the difference in means and t-tests. These methods work beautifully when one of our variables is binary, defining two distinct groups like treatment and control, or Democrat and Republican. But what happens when both of our variables can take on many different values? Consider one of the most fundamental questions in political science: do voters reward incumbents when the economy performs well? This theory of economic voting suggests that when incomes rise during a president’s term, voters should be more likely to support the incumbent party in the next election. But in this analysis, both of our key variables here are continuous. Income growth might range from negative values during recessions to several percent during boom times. Vote share can range anywhere from crushing defeats below 40% to landslide victories above 60%. Forcing either variable into crude categories like “good economy” versus “bad economy” would lose crucial information about the strength of this relationship. This reading introduces two closely related tools that let us analyze relationships between continuous variables without sacrificing information: correlation coefficients and linear regression. These methods (and especially regression) form the foundation for much of modern statistical analysis in political science. Once you understand these concepts, you will be able to explore questions about how economic conditions predict election outcomes, how campaign spending relates to vote share, or whether political knowledge increases with age. It will also prepare you for the rest of the class, which will focus exclusively on the regression modeling framework. 22.3 Loading the Data: Economic Voting in Presidential Elections Throughout this module, we will work with real data on economic voting in U.S. presidential elections. This dataset contains information about income growth during each presidential term and the subsequent vote share received by the incumbent party. Let us load the data and explore its structure: library(dplyr) library(ggplot2) # Load the economic voting data votes &lt;- read.csv(&quot;votes.csv&quot;) # Examine the structure glimpse(votes) ## Rows: 17 ## Columns: 7 ## $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17 ## $ year &lt;int&gt; 1948, 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 1984, 1988, 1992, 1996,… ## $ rdi4 &lt;dbl&gt; 3.450990, 1.470730, 2.978460, 0.574562, 5.575570, 3.460140, 3.655830, 2.93891… ## $ vote &lt;dbl&gt; 52.32560, 44.62310, 57.74650, 49.89920, 61.34540, 49.59350, 61.81260, 48.9297… ## $ demcand &lt;chr&gt; &quot;Truman&quot;, &quot;Stevenson&quot;, &quot;Stevenson&quot;, &quot;Kennedy&quot;, &quot;Johnson&quot;, &quot;Humphrey&quot;, &quot;McGove… ## $ repcand &lt;chr&gt; &quot;Dewey&quot;, &quot;Eisenhower&quot;, &quot;Eisenhower&quot;, &quot;Nixon&quot;, &quot;Goldwater&quot;, &quot;Nixon&quot;, &quot;Nixon&quot;, … ## $ reelection &lt;int&gt; 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1 Our key variables of interest are rdi4: average annual change in real disposable income over past 4 years vote: incumbent party vote share in the election The unit of observation in this dataset is a presidential election year. For example, the 2012 observation captures Barack Obama’s first term (2008-2012), showing that real disposable income grew at an average annual rate of about 2.17% during those four years, and his Democratic Party received about 51.4% of the two-party vote when he ran for reelection. This data allows us to test a central hypothesis in political science: when the economy performs well (measured by income growth), the incumbent party should perform better at the polls. But how strong is this relationship? Can we use economic performance to predict election outcomes? These are exactly the questions that correlation and regression help us answer. 22.4 From Groups to Relationships Let us briefly review where we are coming from. When we use a t-test or calculate a difference in means, we are comparing an outcome variable across two groups. One of our variables must be binary to define these groups. This works perfectly for many research questions in political science, such as comparing voter turnout between competitive and non-competitive districts. But our economic voting question involves two continuous variables. Income growth during a presidential term can take many values, from the negative growth of severe recessions to the rapid growth of economic booms. Similarly, incumbent vote share varies continuously from devastating defeats to landslide victories. We need analytical tools that preserve and utilize all this variation. That is exactly what correlation and regression provide. These methods allow us to examine whether and how strongly two continuous variables relate to each other, and to make specific predictions about one variable based on values of another. 22.5 Correlation Coefficients 22.5.1 The Basic Idea A correlation coefficient gives us a single number that summarizes the linear relationship between two variables. When we say income growth and incumbent vote share are correlated, we mean that knowing income growth gives us information about the likely vote share. Higher income growth tends to go with higher vote shares (positive correlation). Remember that correlation describes association, not causation. If income growth and vote share are positively correlated, it means that knowing the economy grew helps us predict the incumbent party will likely do well. It does not necessarily prove that economic growth causes electoral success. Perhaps both are influenced by other factors like war, scandals, or candidate quality. Correlation tells us two variables move together; it does not tell us why. 22.5.2 Visualizing the Relationship Before diving into the mathematics, let us visualize our data to get an intuitive sense of the relationship: # Create a basic scatterplot plot(x = votes$rdi4, y = votes$vote, main = &quot;Economic Voting in Presidential Elections&quot;, xlab = &quot;Average Annual Income Growth (%)&quot;, ylab = &quot;Incumbent Party Vote Share (%)&quot;, ylim = c(40, 70), pch = 19, col = &quot;darkblue&quot;) # Add a grid for easier reading grid() Looking at this scatterplot, you can already see a pattern. Elections with higher income growth (toward the right) tend to have higher incumbent vote shares (toward the top). Elections with low or negative income growth tend to see the incumbent party perform poorly. This visual pattern is exactly what correlation captures numerically. 22.5.3 Understanding the Correlation Coefficient The correlation coefficient, often denoted as r, always falls between -1 and 1. These bounds reflect the maximum possible linear relationship between two variables. A correlation of 1 means perfect positive correlation where all points fall exactly on an upward-sloping line. A correlation of -1 means perfect negative correlation where all points fall exactly on a downward-sloping line. A correlation of 0 indicates no linear relationship. In political science, you rarely see correlations near -1 or 1 because social phenomena are complex with many influencing factors. The correlation between income growth and incumbent vote share is substantial by political science standards but far from perfect, reflecting that many factors beyond the economy influence elections. 22.5.4 The Math Behind Correlation Now let us look at how we actually calculate a correlation coefficient. Let’s call one variable (economic performande) \\(x\\) and the other variable (incumbent vote share) \\(y\\). The formula is: \\[r = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left[\\frac{(x_i - \\bar{x})}{s_x} \\times \\frac{(y_i - \\bar{y})}{s_y}\\right]\\] This formula may look a bit complicated, but let’s break it down. First, remember that \\(\\bar{x}\\) is the mean of the variable \\(x\\) and \\(s_x\\) is the standard deviation. Likewise, \\(\\bar{y}\\) is the mean of the variable \\(y\\) and \\(s_y\\) is the standard deviation. So we can break down the equation like this. For observation \\(i\\), we can calculate how much it deviates from the mean, and divide by the standard deviation. \\[\\frac{(x_i-\\bar{x})}{s_x} \\] Then we need to do the same thing for y: \\[\\frac{(y_i-\\bar{y})}{s_y} \\] Then we just multiply them together. \\[\\frac{(x_i-\\bar{x})}{s_x} \\times \\frac{(y_i-\\bar{y})}{s_y} \\] So when both of these numbers are positive or both are negative, this will result in a positive number. When one is positive and other is negative, it will result in a negative number. Finally, to find the overall correlation we just have to add these up for each observation and divide by \\(n-1\\). \\[r = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left[\\frac{(x_i - \\bar{x})}{s_x} \\times \\frac{(y_i - \\bar{y})}{s_y}\\right]\\] Let us break this down step by step using our actual data. We will calculate the correlation manually to understand what is happening, then verify using R’s built-in function. # Calculate means mean_rdi4 &lt;- mean(votes$rdi4) mean_vote &lt;- mean(votes$vote) # Calculate deviations from the mean votes$demean_rdi4 &lt;- votes$rdi4 - mean_rdi4 votes$demean_vote &lt;- votes$vote - mean_vote # Look at a few observations to understand the pattern votes %&gt;% select(year, rdi4, vote, demean_rdi4, demean_vote) %&gt;% glimpse() ## Rows: 17 ## Columns: 5 ## $ year &lt;int&gt; 1948, 1952, 1956, 1960, 1964, 1968, 1972, 1976, 1980, 1984, 1988, 1992, 1996… ## $ rdi4 &lt;dbl&gt; 3.450990, 1.470730, 2.978460, 0.574562, 5.575570, 3.460140, 3.655830, 2.9389… ## $ vote &lt;dbl&gt; 52.32560, 44.62310, 57.74650, 49.89920, 61.34540, 49.59350, 61.81260, 48.929… ## $ demean_rdi4 &lt;dbl&gt; 0.7846907, -1.1955694, 0.3121608, -2.0917373, 2.9092708, 0.7938407, 0.989530… ## $ demean_vote &lt;dbl&gt; 0.2797369, -7.4227631, 5.7006349, -2.1466631, 9.2995389, -2.4523651, 9.76673… For each observation, we calculate how far that election’s income growth is from the average income growth, and how far its vote share is from the average vote share. When both variables are above their means (both deviations positive) or both below their means (both deviations negative), they contribute to positive correlation. When one is above and one below, they contribute to negative correlation. The next step, is we need to calculate the product of these deviations. # Calculate the product of deviations votes$corr_contribution &lt;- votes$demean_rdi4 * votes$demean_vote Now let us complete the calculation. To do this, we’ll first calculate the standard deviations, and then complete the calculations # Calculate standard deviations sd_rdi4 &lt;- sd(votes$rdi4) sd_vote &lt;- sd(votes$vote) # Manual calculation of correlation coefficient n &lt;- nrow(votes) correlation_manual &lt;- (1/(n-1)) * sum(votes$corr_contribution) / (sd_rdi4 * sd_vote) print(paste(&quot;Manually calculated correlation:&quot;, round(correlation_manual, 3))) ## [1] &quot;Manually calculated correlation: 0.742&quot; Of course, usually we don’t need todo this all ourselves. R provides a built-in function cor() that calculates this very easily. # Verify with built-in function correlation_builtin &lt;- cor(votes$rdi4, votes$vote) round(correlation_builtin, 3) ## [1] 0.742 The correlation of approximately 0.742 indicates a moderately strong positive relationship. In political science terms, this is quite substantial. It tells us that economic performance and electoral outcomes move together consistently, though not perfectly. 22.5.5 Visualizing How Correlation Works To better understand correlation, let us visualize which observations contribute to the positive correlatio. Points in the upper-right quadrant (good economy, high vote share) and lower-left quadrant (poor economy, low vote share) contribute to positive correlation. These are elections where economic performance and electoral outcomes moved in the same direction. Points in the other two quadrants represent elections where the economy and electoral outcomes diverged, perhaps due to other factors like war, scandal, or candidate characteristics. 22.6 Limitations of Correlation While correlation coefficients are useful, they have important limitations that often make them insufficient for research purposes. Understanding these limitations helps us see why we need regression analysis. First, correlation coefficients do not allow us to make specific predictions. Knowing that income growth and vote share have a correlation of 0.74 tells us they are related, but it does not tell us what vote share to expect when income grows by 2%. The correlation coefficient describes the strength of the relationship but not its specific form. Second, correlation is invariant to linear transformations. If we measured income growth as a decimal (0.02) instead of a percentage (2%), the correlation would remain identical. This invariance means correlation cannot tell us about the actual scale of the relationship. How much does one percentage point of income growth actually translate into vote share? Correlation cannot answer this question. Third, correlation does not provide easily interpretable effect sizes. If a campaign strategist wants to know how much economic growth helps the incumbent party, the correlation coefficient provides limited guidance. Saying “the correlation is 0.74” does not translate into actionable predictions about how specific economic conditions might affect electoral outcomes. 22.7 From Correlation to Regression 22.7.1 Why We Need More Than Correlation The limitations of correlation lead us naturally to regression analysis. While correlation tells us whether and how strongly two variables relate, regression specifies the actual relationship. Regression allows us to make specific predictions, understand effect sizes in meaningful units, and communicate results in substantively interpretable terms. Consider our economic voting example. A correlation of 0.74 tells us the economy matters for elections, but it cannot answer crucial questions. If the economy grows by 3% during a presidential term, what vote share should the incumbent party expect? How much does each additional percentage point of growth help? These are the questions regression can answer. 22.7.2 The Regression Line Concept The key insight of regression is that we can summarize the relationship between two variables with a line. Let us add a line to our scatterplot that represents our best guess about the relationship: # Create scatterplot with regression line plot(x = votes$rdi4, y = votes$vote, main = &quot;Economic Voting with Regression Line&quot;, xlab = &quot;Average Annual Income Growth (%)&quot;, ylab = &quot;Incumbent Party Vote Share (%)&quot;, ylim = c(40, 70), pch = 19, col = &quot;darkblue&quot;) # Add the regression line (we&#39;ll explain how to calculate this soon) abline(lm(vote ~ rdi4, data = votes), col = &quot;red&quot;, lwd = 2) # Add grid grid() This red line represents our best summary of how income growth relates to vote share. For any level of economic growth, the line gives us a specific prediction for the expected vote share. But which line should we choose? We could draw many different lines through these points. We need a principled way to choose the best line. 22.8 Simple Linear Regression 22.8.1 The Basic Model A linear regression model expresses the relationship between two variables as a straight line plus some error: \\[Y = \\alpha + \\beta X + \\epsilon\\] In our economic voting example, Y is the incumbent party vote share, X is income growth during the presidential term, α (alpha) is the intercept telling us the expected vote share when income growth is zero, β (beta) is the slope telling us how much vote share changes for each percentage point of income growth, and ε (epsilon) is the error term capturing all other factors that influence elections beyond the economy. The error term is crucial. It acknowledges that we cannot perfectly predict election outcomes from economic data alone. Many factors influence elections: candidate quality, campaign effectiveness, international events, social issues, and more. The error term captures all these other influences. 22.8.2 Finding the Best Line: Least Squares Given our model, we need to find the specific values of α and β that best fit our data. The most common criterion is least squares: we choose the line that minimizes the sum of squared vertical distances between the observed points and the line. Let’s break that down a bit. If the actual outcome for observation \\(i\\) is \\(Y_i\\) and our prediction is \\(\\hat{Y}_i = \\alpha + \\beta X_i\\), then the prediction error is \\(e_i=(Y_i - \\hat{Y}_i)\\). (This is often called the residual.) The idea here is that we are going to choose the values of \\(\\alpha\\) and \\(\\beta\\) that minimize the sum of squared errors (SSE): \\[SSE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (Y_i-\\hat{Y}_i)^2 = \\sum_{i=1}^n\\left(Y_i - (\\alpha + \\beta X_i)\\right)^2\\] Let us visualize these concepts: # Fit the model (we&#39;ll explain the details shortly) model &lt;- lm(vote ~ rdi4, data = votes) # Get predictions and residuals votes$predicted &lt;- fitted(model) votes$residual &lt;- residuals(model) # Create a plot showing residuals plot(x = votes$rdi4, y = votes$vote, main = &quot;Residuals: Actual vs. Predicted Values&quot;, xlab = &quot;Average Annual Income Growth (%)&quot;, ylab = &quot;Incumbent Party Vote Share (%)&quot;, ylim = c(40, 70), pch = 19, col = &quot;darkblue&quot;) # Add the regression line abline(model, col = &quot;red&quot;, lwd = 2) # Draw residual lines for a few points to illustrate for(i in c(2, 5, 8, 11, 14)) { segments(votes$rdi4[i], votes$predicted[i], votes$rdi4[i], votes$vote[i], col = &quot;gray&quot;, lty = 2, lwd = 1.5) } # Add text text(3.5, 67, &quot;Residuals shown as\\ngray dashed lines&quot;, cex = 0.8) The least squares method chooses α and β to minimize the sum of all these squared residuals. Why do we square them? First, squaring prevents positive and negative residuals from canceling out. Second, squaring penalizes large errors more than small ones, encouraging a line that avoids extremely bad predictions. Using calculus (which we will not derive here), we can find formulas for the values that minimize the sum of squared residuals: \\[\\hat{\\beta} = \\frac{\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}\\] \\[\\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\] These formulas look complex, but R calculates them for us automatically. I will only ever make you calculate these by hand one time in this class. 22.8.3 Fitting the Model in R The lm() function (short for “linear model”) fits regression models in R. Let us fit our economic voting model: # Fit the regression model model &lt;- lm(vote ~ rdi4, data = votes) # View the results summary(model) ## ## Call: ## lm(formula = vote ~ rdi4, data = votes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6842 -3.7406 -0.2731 2.6357 7.5002 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 45.9385 1.6919 27.152 3.62e-14 *** ## rdi4 2.2906 0.5342 4.288 0.000648 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.765 on 15 degrees of freedom ## Multiple R-squared: 0.5507, Adjusted R-squared: 0.5207 ## F-statistic: 18.38 on 1 and 15 DF, p-value: 0.0006477 This is a lot of output, and you won’t understand all of it right away. But we can start to understand some of it. The intercept of 45.94 tells us that when income growth is zero (no growth), the model predicts the incumbent party will receive about 46% of the vote. This is the \\(\\alpha\\) term in the equations above. This makes substantive sense as zero growth represents economic stagnation, which voters typically punish. The slope coefficient on rdi4 is 2.29. This is the key finding, and corresponds to the \\(\\beta\\) term in the equation above. Intuitively, this is the slope of the line. It tells us that for each additional percentage point of annual income growth, the incumbent party’s vote share increases by about 2.29 percentage points. (You might remember “rise over run” from your middle school teacher?) This is a substantial effect. The difference between a recession (-2% growth) and a boom (+3% growth) would be about 11.5 percentage points in vote share, often the difference between a crushing defeat and a comfortable victory. These specific predictions illustrate the power of regression over correlation. While correlation told us the economy and elections are related, regression tells us exactly how much economic performance matters in practical terms. 22.8.4 Visualizing the Complete Picture Let us create a comprehensive visualization showing the data, regression line, and confidence in our predictions: # Create the final visualization using base R plot(x = votes$rdi4, y = votes$vote, main = &quot;Economic Voting Model: Income Growth and Electoral Success&quot;, xlab = &quot;Average Annual Income Growth (%)&quot;, ylab = &quot;Incumbent Party Vote Share (%)&quot;, ylim = c(40, 70), pch = 19, col = &quot;darkblue&quot;) # Add the regression line abline(model, col = &quot;red&quot;, lwd = 3) # Add grid grid() This visualization captures our complete analysis. The regression line shows the systematic relationship between economic growth and electoral outcomes. The equation provides specific predictions. We can also create this visualization using ggplot2 for a more polished appearance: library(ggplot2) ggplot(votes, aes(x = rdi4, y = vote)) + geom_point(size = 3, color = &quot;darkblue&quot;, alpha = 0.7) + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;red&quot;, fill = &quot;pink&quot;, alpha = 0.2) + geom_hline(yintercept = 50, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + labs(x = &quot;Average Annual Income Growth (%)&quot;, y = &quot;Incumbent Party Vote Share (%)&quot;, title = &quot;Economic Voting in U.S. Presidential Elections&quot;, subtitle = paste(&quot;Each 1% of income growth increases vote share by 2.29 percentage points&quot;)) + theme_minimal() + theme(text = element_text(size = 12), plot.title = element_text(face = &quot;bold&quot;), plot.subtitle = element_text(color = &quot;gray30&quot;)) + annotate(&quot;text&quot;, x = min(votes$rdi4) + 0.5, y = 48, label = &quot;Incumbent loses&quot;, color = &quot;gray50&quot;, size = 3) + annotate(&quot;text&quot;, x = min(votes$rdi4) + 0.5, y = 52, label = &quot;Incumbent wins&quot;, color = &quot;gray50&quot;, size = 3) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The shaded band around the regression line represents uncertainty in our estimate. We will learn more about this when we cover statistical inference, but for now, note that the band is narrower near the center of the data (where we have more observations) and wider at the extremes. 22.8.5 Making Predictions One of regression’s key advantages is the ability to make specific predictions. Let us use our model to predict electoral outcomes under different economic scenarios: # Create scenarios for prediction scenarios &lt;- data.frame( scenario = c(&quot;Deep Recession&quot;, &quot;Mild Recession&quot;, &quot;Stagnation&quot;, &quot;Moderate Growth&quot;, &quot;Strong Growth&quot;, &quot;Boom&quot;), rdi4 = c(-3, -1, 0, 1.5, 3, 5) ) # Make predictions scenarios$predicted_vote &lt;- predict(model, newdata = scenarios) # Display results print(&quot;Electoral Predictions Under Different Economic Scenarios:&quot;) ## [1] &quot;Electoral Predictions Under Different Economic Scenarios:&quot; print(scenarios) ## scenario rdi4 predicted_vote ## 1 Deep Recession -3.0 39.06685 ## 2 Mild Recession -1.0 43.64798 ## 3 Stagnation 0.0 45.93854 ## 4 Moderate Growth 1.5 49.37438 ## 5 Strong Growth 3.0 52.81023 ## 6 Boom 5.0 57.39135 # Visualize scenarios plot(x = votes$rdi4, y = votes$vote, main = &quot;Predictions for Different Economic Scenarios&quot;, xlab = &quot;Average Annual Income Growth (%)&quot;, ylab = &quot;Incumbent Party Vote Share (%)&quot;, ylim = c(35, 70), xlim = c(-4, 6), pch = 19, col = &quot;darkblue&quot;) # Add regression line abline(model, col = &quot;red&quot;, lwd = 2) # Add prediction points points(scenarios$rdi4, scenarios$predicted_vote, pch = 18, col = &quot;darkgreen&quot;, cex = 2) # Add 50% threshold abline(h = 50, lty = 2, col = &quot;gray&quot;) # Label scenarios text(scenarios$rdi4, scenarios$predicted_vote + 2, scenarios$scenario, cex = 0.7, col = &quot;darkgreen&quot;) These predictions show the practical importance of economic conditions for electoral outcomes. The model suggests that incumbent parties facing recessions (negative growth) are predicted to lose, while those presiding over economic growth are predicted to win. The strength of the economy can mean the difference between a narrow victory and a landslide. 22.9 Summary and Key Takeaways We have covered a lot of content in this reading, moving from correlation to regression using real data on economic voting. Let us review the key concepts. Correlation coefficients provide a single number summary of the linear relationship between two variables, ranging from -1 to 1. In our example, the correlation of 0.742 between income growth and incumbent vote share tells us these variables move together consistently. When the economy does well, incumbent parties tend to perform better electorally. However, correlation alone could not tell us how much economic growth matters in practical terms. Linear regression extends correlation by specifying the exact relationship. Our regression equation, Vote Share = 45.94 + 2.29 × Income Growth, provides concrete predictions. Each percentage point of income growth translates to about 2.29 percentage points of additional vote share for the incumbent party. We find the best-fitting regression line using the least squares criterion, minimizing the sum of squared residuals. R makes this easy with the lm() function, which handles all calculations and provides detailed output. The residuals remind us that while our model captures an important relationship, many other factors also influence elections. Next week, we will extend these ideas to multiple regression, where we can examine how several variables simultaneously relate to an outcome. This allows us to ask more sophisticated questions and begin to address issues of confounding and causation. For now, make sure you are comfortable with correlation and simple regression, as everything else builds from here. 22.10 Review/Study Questions 22.10.1 Understanding Concepts What does a correlation coefficient tell us, and what does it NOT tell us? Explain why knowing that income growth and vote share have a correlation of 0.74 is useful but limited. Explain the difference between correlation and regression. If correlation tells us the strength of a relationship, what additional information does regression provide? What are residuals in a regression model? Using the economic voting example, explain what it means when an election has a large positive residual versus a large negative error term (sometimes called a residual). Why do we square the residuals when finding the best-fitting regression line? Give two reasons why the least squares criterion uses squared distances rather than absolute distances. 22.10.2 Calculating and Interpreting Manual Correlation Calculation: Given these three elections: 1980: Income growth = -0.2%, Vote share = 44.7% 1984: Income growth = 6.0%, Vote share = 59.2% 1996: Income growth = 2.1%, Vote share = 54.9% Calculate which election contributes most to the positive correlation between income growth and vote share. Show your reasoning. Interpreting Regression Coefficients: Our model found Vote Share = 45.94 + 2.29 × Income Growth. What does the intercept of 45.94 mean in practical terms? If income grows by 2.5% during a presidential term, what vote share would you predict? Is the slope coefficient of 2.29 practically significant? Explain. 22.10.3 Working with R Model Predictions: Using our regression model, write the R code to: Predict the vote share for income growth of 4% Create a new prediction for a hypothetical “perfect storm” recession with -5% growth 22.10.4 Critical Thinking Model Limitations: Let’s say our model predicts that with 10% income growth, the incumbent party would receive about 68.8% of the vote. Why might this prediction be unreliable? (Think about extrapolating beyond our observed data.) Name two factors other than the economy that could explain why some elections have large residuals. "],["statistical-inference-in-regression.html", "23 Statistical Inference in Regression 23.1 Learning Objectives 23.2 Why Do We Need Inference in Regression? 23.3 Quick Review: The Regression Line 23.4 Our Estimates Vary from Sample to Sample 23.5 The Gauss-Markov Assumptions (Simplified) 23.6 Testing Whether the Slope is Real: The t-statistic 23.7 Understanding p-values 23.8 Let’s Read Real R Output Together 23.9 Standard Errors: Where Do They Come From? 23.10 Confidence Intervals: A Range of Plausible Values 23.11 From Statistics to Substance 23.12 Reading Comprehension Questions", " 23 Statistical Inference in Regression knitr::opts_chunk$set(echo = TRUE) 23.1 Learning Objectives By the end of this module, you will be able to: Understand what assumptions must hold for regression inference to be valid Test whether relationships between variables are “statistically significant.” Interpret key components of R’s regression output Calculate and interpret confidence intervals for regression coefficients 23.2 Why Do We Need Inference in Regression? In our last module, we learned how to estimate regression lines. We discovered how to calculate the slope and intercept that best fit our data. We even applied this to a real question: does economic performance influence presidential elections? When we regressed incumbent vote share on GDP growth, we found a positive slope. Better economic performance seemed to predict better electoral outcomes for the incumbent party. But finding a positive slope in our sample raises a crucial question we have not yet answered. Could this pattern have occurred just by random chance? If we had data from a different set of elections, would we find the same relationship? Or did we just get lucky with our particular data? This is where statistical inference comes in. Inference allows us to move beyond describing our specific sample to making statements about the broader population. It helps us determine whether the patterns we observe are likely to be real relationships or just statistical noise. Let us start by looking at what R tells us when we run a regression. Do not worry if you cannot interpret all of this yet. By the end of this module, you will understand every number in this output: ## ## Call: ## lm(formula = vote ~ rdi4, data = votes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6842 -3.7406 -0.2731 2.6357 7.5002 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 45.9385 1.6919 27.152 3.62e-14 *** ## rdi4 2.2906 0.5342 4.288 0.000648 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.765 on 15 degrees of freedom ## Multiple R-squared: 0.5507, Adjusted R-squared: 0.5207 ## F-statistic: 18.38 on 1 and 15 DF, p-value: 0.0006477 This output contains estimates, standard errors, t-statistics, p-values, and more. Each piece tells us something important about the relationship between economic performance and electoral outcomes. Let us learn how to read and interpret more of this information. 23.3 Quick Review: The Regression Line Let’s briefly review what we are estimating. The regression model describes a linear relationship between two variables: \\[Y_i = \\alpha + \\beta X_i + \\epsilon_i\\] In this equation, \\(Y_i\\) is our outcome (incumbent vote share for election \\(i\\)), \\(X_i\\) is our predictor (GDP growth for election \\(i\\)), \\(\\alpha\\) is the intercept, \\(\\beta\\) is the slope, and \\(\\epsilon_i\\) is the error term that captures everything else affecting the outcome. When we estimate this model using our data, we get: \\[\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i\\] The hats indicate these are estimates based on our sample. The intercept \\(\\hat{\\alpha}\\) tells us the expected vote share when GDP growth equals zero. The slope \\(\\hat{\\beta}\\) tells us how much vote share changes for each one percentage point increase in GDP growth. Let us visualize this with our presidential elections data: # Create a scatterplot with regression line ggplot(votes, aes(x = rdi4, y = vote)) + geom_point(size = 3, color = &quot;darkblue&quot;) + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;red&quot;, fill = &quot;pink&quot;, alpha = 0.3) + labs(title = &quot;Economic Performance and Electoral Success&quot;, subtitle = &quot;US Presidential Elections 1948-2012&quot;, x = &quot;Real Disposable Income Growth (%)&quot;, y = &quot;Incumbent Party Vote Share (%)&quot;) + theme_minimal() + theme(text = element_text(size = 12)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The red line shows our estimated regression line. The shaded area represents uncertainty about where the true line might be. This uncertainty is what we need to quantify through statistical inference. 23.4 Our Estimates Vary from Sample to Sample The uncertainty we are trying to quantify here is based on the idea that there is a fundamental “randomness” to our data. In a survey, this randomness might come from the sampling procedure. Extending that analogy, we can imagine that elections in our dataset are just one possible sample from a broader population of elections that could occur under similar democratic institutions. If history had unfolded slightly differently, we would have different elections with different economic conditions and outcomes. This randomness is captured by the error term \\(\\epsilon_i\\) in our regression equation. These errors represent all the unpredictable factors that influence elections beyond economic growth: candidate quality, campaign strategies, unexpected scandals, international events, weather on election day, and countless other influences. Each election we observe represents one possible realization of these random factors. If we could rerun history with the same GDP growth values but different realizations of the random errors—different candidates, different campaign decisions, different news cycles—we would see different vote shares. This would lead to different estimates of and when we fit our regression line. This means our estimates \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) are not fixed numbers. They are random variables that would take different values if we had different samples. If we could somehow replay history and get a different set of election results, we would get different estimates. Statistical inference is all about understanding and quantifying this variation. How much might our estimates change from sample to sample? How confident can we be that our estimated slope is close to the true relationship? The key tool for answering these questions is the standard error. The standard error tells us how much our estimate would typically vary if we could repeat our study many times with different samples. A small standard error means our estimate is precise; we would get similar estimates across different samples. A large standard error means our estimate is imprecise; different samples might give quite different estimates. 23.5 The Gauss-Markov Assumptions (Simplified) For our statistical inference to be valid, we need certain conditions to hold. These are called the Gauss-Markov assumptions, named after the mathematicians who developed the theory. We will present a simplified version appropriate for our purposes. Assumption 1: Linearity. The relationship between X and Y follows a straight line. In our example, this means the effect of GDP growth on vote share is constant. A 1% increase in GDP growth has the same effect whether we are going from 0% to 1% or from 3% to 4%. Assumption 2: Zero mean of errors. On average, our errors equal zero. This means we are not systematically over-predicting or under-predicting. Our regression line goes through the center of the data cloud. Assumption 3: Variation in X. We need different values of our predictor variable. We cannot learn about the effect of GDP growth if growth was exactly 2% in every election. Fortunately, economic performance varies considerably across elections. Assumption 4: Independence of errors. The error in predicting one election does not affect errors in other elections. Knowing that we over-predicted vote share in 1992 tells us nothing about whether we will over or under-predict in 1996. Assumption 5: Constant variance (Homoscedasticity). The spread of points around the regression line is roughly the same whether GDP growth is high or low. The uncertainty in our predictions does not systematically change across different values of X. Let us visualize what happens when these assumptions are violated: When these assumptions hold reasonably well, we can trust our statistical inference. When they are severely violated, we need to be more cautious about our conclusions. 23.6 Testing Whether the Slope is Real: The t-statistic Now we come to one of the most important questions in regression analysis: is there really a relationship between our variables, or could the pattern we see be due to random chance? To answer this, we use hypothesis testing. Our null hypothesis is that there is no relationship between GDP growth and vote share: \\[H_0: \\beta = 0\\] Our alternative hypothesis is that there is a relationship: \\[H_a: \\beta \\neq 0\\] To test this hypothesis, we calculate a t-statistic: \\[t = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}\\] This formula tells us how many standard errors our estimated slope is away from zero. The intuition is straightforward. If our estimate is many standard errors away from zero, it would be very unlikely to get such an estimate if the true slope were actually zero. A rule of thumb: if the absolute value of t is greater than 2, the result is usually statistically significant at the 0.05 level. This is because, with reasonable sample sizes, values more than 2 standard errors from zero occur less than 5% of the time by chance. Let us calculate this for our data: # Get the coefficient information model &lt;- lm(vote ~ rdi4, data = votes) summary(model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 45.938538 1.6919240 27.151656 3.619493e-14 ## rdi4 2.290562 0.5342247 4.287639 6.477151e-04 Looking at the rdi4 row, we can see the t-value is quite large (greater than 2 in absolute value), suggesting strong evidence against the null hypothesis of no relationship. 23.7 Understanding p-values The p-value gives us a more precise measure of the evidence against the null hypothesis. In the output, it is shown on the far right column with the name Pr(&gt;|t|). It answers the question: if there were truly no relationship between GDP growth and vote share, what is the probability we would see a relationship as strong as (or stronger than) what we observed? A small p-value means our observed relationship would be very unlikely if there were truly no relationship. By convention, we often use 0.05 as a threshold. If p &lt; 0.05, we say the result is “statistically significant” and reject the null hypothesis. Several factors affect p-values: First, effect size matters. Larger effects (bigger slopes) lead to smaller p-values, all else equal. A relationship where GDP growth changes vote share by 2 percentage points will have a smaller p-value than one where it changes vote share by 0.1 percentage points. Second, precision matters. More precise estimates (smaller standard errors) lead to smaller p-values. This is why the t-statistic divides the estimate by its standard error. Third, sample size matters. More data leads to more precise estimates (smaller standard errors), which leads to smaller p-values. This is why large studies can find “significant” effects that are practically tiny. It is very important to note that the p-value we are interested in (and there will be one for each regression term) is located in the column labeled Pr(&gt;|t|). There is another p-value in the output that looks like this. ## F-statistic: 18.38 on 1 and 15 DF, p-value: 0.0006477 This is not (always) the same thing, and is related to a test of the overall model fit. So don’t get confused! (We will come back to this issue next class.) 23.8 Let’s Read Real R Output Together Now we are ready to fully interpret regression output. Let us run our regression and examine each piece: # Run the regression model &lt;- lm(vote ~ rdi4, data = votes) # Display the full output summary(model) ## ## Call: ## lm(formula = vote ~ rdi4, data = votes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6842 -3.7406 -0.2731 2.6357 7.5002 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 45.9385 1.6919 27.152 3.62e-14 *** ## rdi4 2.2906 0.5342 4.288 0.000648 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.765 on 15 degrees of freedom ## Multiple R-squared: 0.5507, Adjusted R-squared: 0.5207 ## F-statistic: 18.38 on 1 and 15 DF, p-value: 0.0006477 Let us walk through the coefficients table line by line: The (Intercept) row: Estimate: This is \\(\\hat{\\alpha}\\), the expected incumbent vote share when GDP growth equals zero. In our data, it is about 46.3%, meaning incumbents typically lose when there is zero economic growth. Std. Error: This tells us how precisely we estimated the intercept. A smaller value means more precision. t value: This tests whether the intercept equals zero. Usually we are not interested in this test. Pr(&gt;|t|): The p-value for testing if the intercept equals zero. The rdi4 row (this is our main focus): Estimate: This is \\(\\hat{\\beta}\\), telling us that each 1% increase in income growth increases incumbent vote share by about 2.29 percentage points. Std. Error: The precision of our slope estimate. Here it is about 0.16, meaning our estimate is fairly precise. t value: Our key test statistic. At 6.97, it is much larger than 2, providing strong evidence against the null hypothesis. Pr(&gt;|t|): The p-value is extremely small (1.66e-08), far below 0.05. We have very strong statistical evidence for a relationship. Stars: Visual aids where *** means p &lt; 0.001, ** means p &lt; 0.01, and * means p &lt; 0.05. The bottom of the output shows additional information including the residual standard error and degrees of freedom, which we will discuss more in future modules. 23.9 Standard Errors: Where Do They Come From? Understanding what affects standard errors helps us design better studies and interpret results more thoughtfully. The standard error of the slope depends on three main factors: Sample size (n). More observations lead to smaller standard errors and more precise estimates. This is why researchers always want more data. Doubling the sample size roughly cuts the standard error by \\(\\sqrt{2}\\). Variation in X. More variation in our predictor variable leads to smaller standard errors. If GDP growth ranges from -5% to +10%, we will get more precise estimates than if it only ranges from 1% to 3%. We learn more about the relationship when we see X take many different values. Unexplained variation. Less scatter around the regression line leads to smaller standard errors. If other factors create a lot of noise in the relationship, our estimates become less precise. We can express this conceptually as: \\[SE(\\hat{\\beta}) \\approx \\frac{\\text{unexplained variation}}{\\text{variation in X} \\times \\sqrt{n}}\\] This formula, while simplified, captures the essential intuition. It explains why some studies find significant results while others studying the same question do not. A study with more data, more variation in the predictor, or less noise will have more power to detect relationships. 23.10 Confidence Intervals: A Range of Plausible Values While hypothesis tests tell us whether we can reject the null hypothesis, confidence intervals give us more information. They provide a range of plausible values for the population parameter. The interpretation is subtle but important. If we were to repeat our study many times with different samples, 95% of the confidence intervals calculated this way would contain the true population parameter \\(\\beta\\). Let us calculate the confidence interval for our economic voting example: # Get confidence intervals confint(model) ## 2.5 % 97.5 % ## (Intercept) 42.33229 49.544789 ## rdi4 1.15189 3.429235 For the slope (rdi4), the 95% confidence interval runs from about 1.15 to 3.43. Notice that this interval does not include zero. This is another way to see that our result is statistically significant at the 0.05 level. If the confidence interval includes zero, we cannot reject the null hypothesis. If it excludes zero, we can reject the null. 23.11 From Statistics to Substance Finding statistical significance is not the end of our analysis. We must always ask whether the effect size is substantively meaningful. Statistical significance tells us the relationship is probably not due to chance. Substantive significance tells us whether the relationship matters in the real world. In our economic voting example, we found that each 1% increase in income growth increases incumbent vote share by about 2.29 percentage points. Is this a meaningful effect? To answer this, consider the range of economic growth in typical elections. GDP growth in our data ranges from about -4% during severe recessions to +7% during economic booms. The difference between a bad economy (-2%) and a good economy (+4%) is 6 percentage points of growth. Our model suggests this would translate to about 13.7 percentage points in vote share (6 × 2.29). Given that many presidential elections are decided by margins of less than 5 percentage points, a 13.7 point swing is politically substantial. Most presidential elections are decided by much less! So while economic growth explains only part of electoral outcomes, the part it explains is large enough to swing close elections. 23.12 Reading Comprehension Questions Understanding the Basics: If our regression gives us \\(\\hat{\\alpha} = 48\\) and \\(\\hat{\\beta} = 0.7\\), what is the predicted incumbent vote share when GDP growth is 3%? Interpreting t-statistics: You run a regression and find a slope coefficient of 1.1 with a standard error of 0.5. What is the t-statistic? Based on the rule of thumb, is this likely to be statistically significant? Understanding p-values: If you get a p-value of 0.03 for your slope coefficient, what does this tell you? What would you conclude about the null hypothesis at the 0.05 significance level? Assumptions: A researcher studying campaign spending finds that the first $100,000 has large effects on vote share, but additional spending has diminishing returns. Which Gauss-Markov assumption might be violated here and why? Confidence Intervals: A regression of vote share on GDP growth gives a slope of 0.6 with a 95% confidence interval of [0.2, 1.0]. Can we reject the null hypothesis that β = 0? How do you know? What does this interval tell us about the relationship? Standard Errors: You have two studies of economic voting. Study A uses 20 elections, Study B uses 100 elections. Both find the same slope estimate. All else equal, which study will likely have the smaller standard error and why? Substantive Significance: A study finds that each additional $1 million in campaign spending increases vote share by 0.01 percentage points (p &lt; 0.001). The result is highly statistically significant. Should a candidate be excited about this finding? Why or why not? "],["multiple-regression-and-controlling-for-confounders.html", "24 Multiple Regression and Controlling for Confounders 24.1 Learning Objectives 24.2 What Is This For? 24.3 The Problem of Confounding 24.4 Multiple Regression: The Model 24.5 Multiple Regression in R: The mtcars Example 24.6 Application to Political Science: Trump Thermometer Scores 24.7 A Step-by-Step Guide to Interpreting Regression Coefficients 24.8 Visualizing Multiple Regression Results 24.9 Review Questions", " 24 Multiple Regression and Controlling for Confounders knitr::opts_chunk$set(echo = TRUE) 24.1 Learning Objectives By the end of this module, you will be able to: Understand the concept of confounding and why we need to control for other variables Interpret coefficients in multiple regression models where we include control variables Read and interpret standard regression output from R, focusing on coefficients, standard errors, t-statistics, and p-values Explain how adding control variables can change our conclusions about relationships 24.2 What Is This For? In our previous modules on correlation and regression, we learned how to measure and describe relationships between two variables. We discovered, for example, that economic growth predicts incumbent vote share in presidential elections. But finding a relationship between two variables raises an important question we have not yet fully addressed: is this relationship causal, or could it be explained by other factors? Consider a concrete example. Suppose we find that older Americans express warmer feelings toward Donald Trump on feeling thermometer scales. Does age directly cause these warmer feelings? Or could this relationship be explained by something else? Perhaps older Americans are more likely to identify as Republicans, and it is really party identification driving feelings toward Trump, not age itself. This is the problem of confounding. A confounder is a variable that influences both our predictor and our outcome, creating the appearance of a relationship that might not reflect a direct causal connection. If we want to understand whether age truly predicts Trump evaluations, we need to account for party identification. In statistical language, we need to “control for” party ID. Multiple regression gives us a tool to address confounding by including additional variables in our model. Instead of just regressing Trump thermometer scores on age, we can regress Trump scores on age while simultaneously accounting for party identification. This allows us to better estimate the relationship between age and Trump evaluations among people with the same party identification, effectively removing the confounding influence of partisanship. There may may still be other confounders, but at least we hav partially addressed the confounder of partisanship. This module introduces multiple regression as a method for controlling confounders. We will learn how to include multiple predictors in a regression model, how to interpret the resulting coefficients, and how to read the standard regression output that R provides. These skills are essential for conducting credible empirical research in political science, where confounding is ubiquitous and careful analysis requires accounting for alternative explanations. 24.3 The Problem of Confounding Before we dive into the statistics, let us build intuition about confounding using a simple example that has nothing to do with politics. 24.3.1 An Intuitive Example: Cars, Weight, and Fuel Efficiency Consider the relationship between a car’s weight and its fuel efficiency (miles per gallon). Heavier cars tend to get worse gas mileage. We can see this relationship using the classic mtcars dataset, which contains data on 32 different car models from the 1970s: # Load the mtcars data (built into R) data(mtcars) # Look at the first few rows head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 # Simple regression: mpg predicted by weight model_simple &lt;- lm(mpg ~ wt, data = mtcars) summary(model_simple) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 The coefficient on weight is about -5.3, meaning that for each additional 1,000 pounds of weight, fuel efficiency decreases by about 5.3 miles per gallon. This seems straightforward: heavier cars use more fuel. But wait. What makes cars heavier? Often, it is a more powerful engine. Cars with bigger engines weigh more AND consume more fuel because the engine itself burns more gasoline. This means engine size could be a confounder. Let us visualize this: # Visualize the relationship, coloring by horsepower ggplot(mtcars, aes(x = wt, y = mpg, color = hp)) + geom_point(size = 3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + scale_color_gradient(low = &quot;blue&quot;, high = &quot;orange&quot;) + labs(x = &quot;Weight (1000 lbs)&quot;, y = &quot;Miles Per Gallon&quot;, color = &quot;Horsepower&quot;, title = &quot;Weight, Engine Power, and Fuel Efficiency&quot;, subtitle = &quot;Heavier cars tend to have more powerful engines (orange)&quot;) + theme_minimal() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Notice how the orange points (high horsepower) cluster in the bottom right. Heavy cars tend to have powerful engines, and both weight and horsepower reduce fuel efficiency. This is confounding in action. When we ignore horsepower and just look at the weight-MPG relationship, we are mixing together two effects: the direct effect of weight on fuel consumption, and the indirect effect that heavier cars have bigger engines. If we want to isolate the direct effect of weight, we need to control for horsepower. 24.3.2 What Does “Controlling For” Mean? When we say we are controlling for a variable, we mean we are trying to compare observations that are similar on that variable. In our car example, controlling for horsepower means we are comparing cars with similar engine power to see how weight affects fuel efficiency among that group. Think of it this way. If we compare a lightweight car with a small engine to a heavy car with a large engine, we cannot tell whether the difference in fuel efficiency is due to weight, engine size, or both. But if we compare two cars with the same horsepower where one weighs more, then we are isolating the effect of weight. Multiple regression accomplishes this by simultaneously estimating the relationship between fuel efficiency and weight while accounting for differences in horsepower. It effectively asks: “Among cars with the same horsepower, how does weight relate to fuel efficiency?” There are several ways to describe what controlling for a variable does. All of these phrases mean essentially the same thing: We are holding the control variable constant We are removing the influence of the control variable We are making observations comparable on the control variable We are adjusting for the control variable We are partialling out the effect of the control variable We are blocking alternative explanations through the control variable These are all different ways of expressing the same core idea: we are trying to isolate the relationship between our predictor of interest and the outcome by accounting for other variables that might create spurious associations. 24.4 Multiple Regression: The Model Multiple regression extends simple bivariate regression by including additional predictor variables. The mathematical model looks like this: \\[Y_i = \\alpha + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_i\\] Let us break down what this notation means: \\(Y_i\\) is the outcome for observation \\(i\\) (fuel efficiency for car \\(i\\)) \\(X_{1i}\\) is the first predictor for observation \\(i\\) (weight of car \\(i\\)) \\(X_{2i}\\) is the second predictor for observation \\(i\\) (horsepower of car \\(i\\)) \\(\\alpha\\) is the intercept (expected fuel efficiency when both weight and horsepower equal zero) \\(\\beta_1\\) is the coefficient on the first predictor (effect of weight, holding horsepower constant) \\(\\beta_2\\) is the coefficient on the second predictor (effect of horsepower, holding weight constant) \\(\\epsilon_i\\) is the error term (all other factors affecting fuel efficiency) The key insight is in how we interpret the coefficients. In bivariate regression, \\(\\beta\\) told us how much \\(Y\\) changes for a one-unit increase in \\(X\\). In multiple regression, \\(\\beta_1\\) tells us how much \\(Y\\) changes for a one-unit increase in \\(X_1\\) while holding \\(X_2\\) constant. This “holding constant” phrase is crucial. It means we are comparing observations with the same value of \\(X_2\\). Just like in simple regression, we estimate these coefficients using ordinary least squares. The computer finds the values of \\(\\alpha\\), \\(\\beta_1\\), and \\(\\beta_2\\) that minimize the sum of squared residuals. The only difference is that now we are fitting a plane through a three-dimensional space rather than a line through a two-dimensional space. But the underlying principle is the same: find the model that makes the smallest prediction errors. 24.5 Multiple Regression in R: The mtcars Example Let us fit a multiple regression model to our car data, including both weight and horsepower as predictors: # Multiple regression: mpg predicted by weight AND horsepower model_multiple &lt;- lm(mpg ~ wt + hp, data = mtcars) summary(model_multiple) ## ## Call: ## lm(formula = mpg ~ wt + hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.941 -1.600 -0.182 1.050 5.854 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.22727 1.59879 23.285 &lt; 2e-16 *** ## wt -3.87783 0.63273 -6.129 1.12e-06 *** ## hp -0.03177 0.00903 -3.519 0.00145 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.593 on 29 degrees of freedom ## Multiple R-squared: 0.8268, Adjusted R-squared: 0.8148 ## F-statistic: 69.21 on 2 and 29 DF, p-value: 9.109e-12 This output contains a wealth of information. Let us focus on the most important parts for now: the coefficients table. 24.5.1 Reading the Coefficients Table The coefficients table shows four key pieces of information for each variable: Estimate: This is the coefficient itself, the \\(\\beta\\) values from our equation Std. Error: This measures the uncertainty in our estimate t value: This is the test statistic for whether the coefficient differs from zero Pr(&gt;|t|): This is the p-value for the hypothesis test Let us interpret each row: The intercept (37.23): This tells us that a car with zero weight and zero horsepower would have a predicted fuel efficiency of 37.23 mpg. This is not particularly meaningful in practice because no car has zero weight or horsepower. The intercept is often not the focus of our interpretation. The weight coefficient (-3.88): This is the key finding we care about. It tells us that for each additional 1,000 pounds of weight, fuel efficiency decreases by 3.88 miles per gallon, holding horsepower constant. Notice that this coefficient is less negative than the -5.3 we found in the bivariate model. Why? Because part of the weight effect in the simple model was actually due to heavier cars having more powerful engines. When we control for horsepower, the weight effect is smaller. The horsepower coefficient (-0.032): This tells us that for each additional unit of horsepower, fuel efficiency decreases by 0.032 miles per gallon, holding weight constant. Put differently, a car with 100 more horsepower gets about 3.2 fewer miles per gallon, comparing cars of the same weight. Both coefficients have very small p-values (look at the Pr(&gt;|t|) column), indicating strong evidence that these relationships are not due to random chance. Note that there is one p-value for each coefficient. We are probably not interested in all of them, and in this case we really are only interested in the one in the wt column. This one is very small (\\(1.12 \\times 10^{-6} = \\frac{1.12}{1,000,000}\\)). 24.5.2 Comparing the Models Let us compare the coefficient on weight across our two models: # Simple model coefficient on weight coef(model_simple)[&quot;wt&quot;] ## wt ## -5.344472 # Multiple model coefficient on weight coef(model_multiple)[&quot;wt&quot;] ## wt ## -3.877831 The weight coefficient changed from -5.34 in the simple model to -3.88 in the multiple model. This change happened because we controlled for horsepower. In the simple model, the weight coefficient was capturing both the direct effect of weight AND the indirect effect that heavier cars have bigger engines. In the multiple model, we have separated these effects. This demonstrates a crucial point: the relationship we observe between two variables can change when we account for confounders. Sometimes controlling for confounders makes relationships weaker, sometimes stronger, and sometimes relationships disappear entirely. This is why multiple regression is so important for understanding relationships in observational data. 24.6 Application to Political Science: Trump Thermometer Scores Now let us apply these concepts to a real political science question. We will use data from the 2020 American National Election Study to investigate how age relates to feelings toward Donald Trump. One argument is that age will have a strong relationship, since older voters were so much more supportive of President Trump relative to young voters. But just like in our cars example above, there is the chance of confounding. Older voters are also more likely to be Republican. Could it be that the relationship between age and Trump support is simiply a result of party affiliation? Maybe age actually has no direct relationship at all! To test this, we will need run a regression where we “control” for partisanship. 24.6.1 Loading and Preparing the Data To get started, we need to get our data ready. # Load the ANES 2020 data anes &lt;- read.csv(&quot;anes2020.csv&quot;) # Examine the structure glimpse(anes) ## Rows: 8,280 ## Columns: 12 ## $ rid &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, … ## $ age &lt;int&gt; 46, 37, 40, 41, 72, 71, 37, 45, 70, 43, 37, 55, 30, 38, 41, 66, 54, 55, 62, 80,… ## $ race &lt;int&gt; 3, 4, 1, 4, 5, 1, 1, 1, 1, 3, 3, 1, 1, 4, 2, 3, 1, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1… ## $ latino &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2… ## $ vote12 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1… ## $ vote16 &lt;int&gt; 1, 1, 1, 1, 1, 2, 1, 2, NA, NA, NA, 1, 2, NA, 2, NA, 1, NA, NA, 1, NA, NA, 1, N… ## $ media &lt;int&gt; 2, 3, 1, 3, 2, 2, 3, 4, 3, 2, 4, 3, 1, 1, 3, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 3, 2… ## $ partyid &lt;int&gt; 2, 5, 3, 2, 3, 3, 2, 2, 3, 1, 1, 1, 2, 1, 3, 3, 3, 1, 1, 1, 2, 1, 3, 2, 1, 1, 3… ## $ income &lt;int&gt; 21, 13, 17, 7, 22, 3, 4, 3, 10, 11, 9, 18, 1, 21, 3, 7, 22, 20, NA, 5, 12, 6, 2… ## $ obama.th &lt;int&gt; 0, 50, 90, 85, 10, 60, 15, 50, 60, 100, 100, 85, 0, 100, 85, 0, 85, 100, 100, 1… ## $ trump.th &lt;int&gt; 100, 0, 0, 15, 85, 0, 75, 100, 0, 0, 0, 0, 100, 0, 40, 100, 0, 0, 15, 15, 95, 0… ## $ biden.th &lt;int&gt; 0, 0, 65, 70, 15, 85, 50, 50, 85, 85, 100, 60, 0, 85, 60, 0, 60, 85, 85, 70, 0,… Our key variables are: trump.th: Feeling thermometer for Donald Trump (0-100 scale, where 0 is very cold/negative and 100 is very warm/positive) age: Age of respondent in years partyid: Party identification (1=Democrat, 2=Republican, 3=Independent, 5=Other party) We need to prepare our data by creating dummy variables for party identification. A dummy variable takes the value 1 if a condition is true and 0 otherwise. We will create two dummy variables: # Create party ID dummy variables anes &lt;- anes %&gt;% mutate( Democrat = ifelse(partyid == 1, 1, 0), Republican = ifelse(partyid == 2, 1, 0) ) # Check our new variables table(anes$Democrat) ## ## 0 1 ## 5366 2865 table(anes$Republican) ## ## 0 1 ## 5668 2563 Why do we create two dummy variables when there are more than two party categories? We need to leave one category out as a reference group. In this case, we are leaving out Independents and other party identifiers. The coefficients on our dummy variables will tell us how Democrats and Republicans differ from this reference group. (We will talk about this more in a future class.) 24.6.2 The Bivariate Relationship: Age and Trump Evaluations Let us start by examining the simple relationship between age and Trump thermometer scores: # Simple regression: Trump thermometer predicted by age model_age &lt;- lm(trump.th ~ age, data = anes) summary(model_age) ## ## Call: ## lm(formula = trump.th ~ age, data = anes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.15 -38.13 -11.79 41.26 67.97 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.64585 1.43952 19.205 &lt;2e-16 *** ## age 0.24380 0.02661 9.162 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 40.11 on 7725 degrees of freedom ## (553 observations deleted due to missingness) ## Multiple R-squared: 0.01075, Adjusted R-squared: 0.01062 ## F-statistic: 83.95 on 1 and 7725 DF, p-value: &lt; 2.2e-16 The coefficient on Age is positive and statistically significant (the p-value is very small). Specifically, each additional year of age is associated with about 0.24 points higher on the Trump thermometer scale. Over a 40-year age difference (say, comparing a 30-year-old to a 70-year-old), this amounts to about 9.6 points on the thermometer. Let us visualize this relationship: ggplot(anes, aes(x = age, y = trump.th)) + geom_jitter(alpha = 0.08, width = 0.5, height = 5) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = TRUE) + labs(x = &quot;Age (years)&quot;, y = &quot;Trump Feeling Thermometer (0-100)&quot;, title = &quot;Older Americans Express Warmer Feelings Toward Trump&quot;, subtitle = &quot;Each additional year of age predicts 0.24 points higher on thermometer&quot;) + theme_minimal() ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 553 rows containing non-finite outside the scale range (`stat_smooth()`). ## Warning: Removed 553 rows containing missing values or values outside the scale range ## (`geom_point()`). The positive relationship is hard to see visually given how many points there are, but notice the relative empty quadrant in the upper left (young voters with favorable feelings towards Trump). the red line recovers the overall linear relationship. But is this a direct effect of age on Trump evaluations? Or could it be that older Americans are more likely to be Republicans, and Republicans obviously rate Trump much more warmly? This is where controlling for party identification becomes essential. 24.6.3 Adding the Control: Party Identification Now we will add party identification to our model to see whether the age relationship persists when we compare people of the same party: # Multiple regression: Trump thermometer predicted by age AND party model_age_party &lt;- lm(trump.th ~ age + Democrat + Republican, data = anes) summary(model_age_party) ## ## Call: ## lm(formula = trump.th ~ age + Democrat + Republican, data = anes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -83.106 -11.436 -6.096 18.446 94.792 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.55327 1.05530 28.95 &lt; 2e-16 *** ## age 0.11086 0.01857 5.97 2.48e-09 *** ## Democrat -27.56271 0.76322 -36.11 &lt; 2e-16 *** ## Republican 43.68392 0.79204 55.15 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27.74 on 7702 degrees of freedom ## (574 observations deleted due to missingness) ## Multiple R-squared: 0.5273, Adjusted R-squared: 0.5271 ## F-statistic: 2864 on 3 and 7702 DF, p-value: &lt; 2.2e-16 Let us carefully interpret each coefficient in this model: The intercept (30.55): This is the expected Trump thermometer score for a person with age zero who is neither a Democrat nor a Republican (i.e., an Independent or other party identifier). The age zero part is nonsensical, but the party interpretation is meaningful. It tells us that Independents and others give Trump an average score around 30 (actually at age zero, but we will see how age modifies this). The Age coefficient (0.11): This is the change in Trump thermometer score for each additional year of age, holding party identification constant. Notice that this coefficient is much smaller than the 0.24 we found in the bivariate model. When we control for party, the age effect is reduced. This tells us that part of the age relationship we observed earlier was due to older people being more likely to be Republicans. However, the coefficient is still positive and statistically significant, meaning there is still an age effect even within party groups. The Democrat coefficient (-27.56): This tells us how much Democrats differ from the reference group (Independents and others) in their Trump evaluations, holding age constant. Democrats rate Trump about 27.5 points lower than Independents of the same age. This is a massive difference and is highly statistically significant (p-value near zero). This makes sense given partisan polarization. The Republican coefficient (43.68): This tells us that Republicans rate Trump about 43.6 points higher than Independents of the same age. Combined with the intercept, this means older Republicans give Trump very high thermometer scores (approaching the maximum of 100), while younger Republicans give somewhat lower scores. All three predictors are statistically significant, indicated by the very small p-values and the *** symbols. Their standard errors, t-statistics, and p-values are shown in the same rows as the coefficients. 24.6.4 What Did Controlling for Party Accomplish? By including party identification in our model, we learned something important: the age-Trump relationship is partially, but not entirely, explained by partisanship. Let us think about what this means: The bivariate model found that each year of age increases Trump thermometer scores by 0.24 points. This mixes together two things: (1) older people tend to be more Republican, and Republicans love Trump, and (2) within each party, older people rate Trump somewhat more warmly. The multiple regression model separates these effects. The reduced coefficient on Age (0.11 instead of 0.24) tells us the effect of age within party categories. The large coefficients on the party dummies tell us how much party identification matters. If we had found that the Age coefficient dropped to near zero or became statistically insignificant after controlling for party, we would conclude that the age relationship was spurious, driven entirely by the partisan composition of age groups. But we did not find that. Even after controlling for party, age still predicts Trump evaluations. This suggests there is something about age itself (perhaps generational experiences, life circumstances, or values that shift with age) that influences Trump evaluations beyond just party identification. This is the power of multiple regression: it allows us to disentangle relationships and test whether observed associations persist after accounting for potential confounders. 24.7 A Step-by-Step Guide to Interpreting Regression Coefficients When you encounter regression output, follow these steps to interpret the coefficients: Step 1: Identify the coefficient of interest. Look at the row for the predictor variable you care about. In our Trump example, if we care about age, we look at the Age row. Step 2: Read the estimate. The number in the Estimate column is the coefficient. This tells you the change in the outcome for a one-unit increase in the predictor. Step 3: Add the “holding other variables constant” phrase. In multiple regression, every coefficient represents the relationship between that predictor and the outcome among observations with the same values of the control variables. Step 4: State the direction and magnitude. Say whether the relationship is positive or negative, and give the specific magnitude. For Age in our multiple model: “Each additional year of age is associated with a 0.11 point increase in Trump thermometer scores, holding party identification constant.” Step 5: Assess statistical significance. Look at the p-value (Pr(&gt;|t|) column). If it is below 0.05, we typically conclude there is strong evidence the relationship is not zero. Look for the asterisks as a quick visual cue. Step 6: Consider substantive significance. Even if a relationship is statistically significant, ask whether it is large enough to matter in practice. A 0.11 point difference per year of age is statistically significant but fairly small substantively compared to the 48-point difference between Democrats and Independents. 24.7.1 Understanding Standard Errors and t-statistics The standard error (Std. Error column) measures the uncertainty in our coefficient estimate. Smaller standard errors mean more precise estimates. The t-statistic is calculated by dividing the coefficient by its standard error. Large absolute values of the t-statistic (typically above 2 or below -2) indicate statistical significance. R then converts this t-statistic to a p-value, which tells us the probability we would see a coefficient this large if the true relationship were zero. You do not need to calculate these by hand, but understanding what they represent helps you read regression output intelligently. When you see a small p-value and asterisks, you know there is strong statistical evidence for a relationship. When you see a large p-value and no asterisks, you know the evidence is weak. 24.8 Visualizing Multiple Regression Results While we cannot easily plot a three-dimensional regression plane, we can create visualizations that show how the relationship between our main predictor and outcome varies across levels of our control variable. This code is a bit advanced, and I will not ask you to do this on your problem sets or exam. But I’ll show the whole code in case you want to make something like this for your poster project. # Create predictions for different party groups pred_data &lt;- expand.grid( age = seq(18, 80, by = 1), Democrat = c(0, 1), Republican = c(0, 1) ) # Remove impossible combinations (can&#39;t be both Democrat and Republican) pred_data &lt;- pred_data %&gt;% filter(!(Democrat == 1 &amp; Republican == 1)) # Make predictions pred_data$predicted_trump &lt;- predict(model_age_party, newdata = pred_data) # Create party label pred_data &lt;- pred_data %&gt;% mutate(Party = case_when( Democrat == 1 ~ &quot;Democrat&quot;, Republican == 1 ~ &quot;Republican&quot;, TRUE ~ &quot;Independent/Other&quot; )) # Plot ggplot(pred_data, aes(x = age, y = predicted_trump, color = Party)) + geom_line(size = 1.2) + labs(x = &quot;Age (years)&quot;, y = &quot;Predicted Trump Thermometer Score&quot;, title = &quot;Age and Trump Evaluations Across Party Groups&quot;, subtitle = &quot;Controlling for party reveals age effects within partisan groups&quot;, color = &quot;Party ID&quot;) + scale_color_manual(values = c(&quot;Democrat&quot; = &quot;blue&quot;, &quot;Republican&quot; = &quot;red&quot;, &quot;Independent/Other&quot; = &quot;gray50&quot;)) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) This visualization shows three parallel lines, one for each party group. The fact that the lines are parallel (they have the same slope) reflects that the Age coefficient is the same for all groups in our model. The vertical distance between the lines shows the effect of party identification. And the slope of each line shows the effect of age within that party group. 24.9 Review Questions 24.9.1 Conceptual Understanding What is a confounding variable? Why do we need to control for confounders when estimating relationships in observational data? In your own words, explain what it means to say we are “holding a variable constant” or “controlling for” a variable in multiple regression. True or False: If we control for a confounder and the coefficient on our main predictor does not change, this means the confounder was not actually confounding the relationship. Explain your answer. 24.9.2 Interpretation Practice Suppose we regress vote choice (0 = voted for Democrat, 1 = voted for Republican) on income (in thousands of dollars) and education (in years). We get the following results: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.15 0.12 1.25 0.211 Income 0.008 0.002 4.00 0.000 Education 0.025 0.013 1.92 0.006 Interpret the coefficient on Income in a complete sentence, including the “holding constant” phrase. Based on the output in Question 4, which variables have statistically significant relationships with vote choice? How do you know? In the mtcars example, the coefficient on weight changed from -5.34 in the bivariate model to -3.88 in the multiple model that controlled for horsepower. Explain in plain language why this coefficient changed. 24.9.3 Application to the Trump Example In our Trump thermometer example, the Age coefficient was 0.24 in the bivariate model and 0.11 in the multiple model that controlled for party. What does this tell us about the relationship between age, party identification, and Trump evaluations? The coefficient on the Democrat dummy variable was large and negative. Write out the interpretation of this coefficient in a complete sentence. Make sure to mention what the reference group is and the fact we are controlling for age. Imagine we added a control variable for education to our Trump model. What would it mean if the Age coefficient stayed at 0.11? What would it mean if the Age coefficient dropped to 0.05 and became statistically insignificant? 24.9.4 Critical Thinking We controlled for party identification in our Trump example. Can you think of other variables that might confound the age-Trump relationship? For each confounder you identify, explain how it might relate to both age and Trump evaluations. Controlling for confounders is essential for causal inference, but it is not sufficient. Even with many controls, why might we still be cautious about making causal claims from observational data? "],["assessing-model-fit.html", "25 Assessing Model Fit 25.1 Overview: When Predictions Go Wrong 25.2 The 2016 Election in Context 25.3 Understanding Model Fit Through Residuals 25.4 From Residuals to R-Squared 25.5 The F-Test: Testing Overall Model Significance 25.6 Testing Nested Models: The Case of Categorical Variables 25.7 Practical Guidelines for Categorical Variables 25.8 A Word of Caution 25.9 Summary and Key Takeaways 25.10 Review Questions 25.11 Practical Application", " 25 Assessing Model Fit knitr::opts_chunk$set(echo = TRUE) 25.1 Overview: When Predictions Go Wrong In November 2016, political forecasters faced widespread criticism for “getting it wrong” about the presidential election. Headlines proclaimed the failure of statistics, the death of polling, and the impossibility of prediction. But if we step back from the media narrative and examine the actual predictions through the lens of statistical modeling, we find something interesting. The basic prediction models for predicting presidential elections created by political scientists actually did quite well. The question is not whether they predicted the exact outcome, but whether the actual result fell within the range of reasonable expectations given model uncertainty. Today we tackle a fundamental question in statistical analysis: How do we know when our models are doing well? We have learned to fit regression models, interpret coefficients, and test hypotheses about individual predictors. But we have not yet addressed the bigger picture of overall model assessment. A model might have statistically significant predictors and still explain very little variation in the outcome. Conversely, a model might explain substantial variation but still miss important patterns in the data. This module introduces the tools data scientists use to evaluate model performance. We begin with residuals, the individual prediction errors that form the foundation of model assessment. From there, we build up to R-squared, the most commonly reported measure of model fit. We then explore the F-test, which tells us whether our model as a whole performs better than simply predicting the mean for everyone. Finally, we address a practical challenge that arises frequently in political science: testing whether categorical variables like party identification improve our models, even when some categories appear non-significant individually. 25.2 The 2016 Election in Context Let us begin by examining the 2016 election through the lens of economic voting models . Political scientists have long known that economic performance strongly predicts electoral outcomes. When the economy is growing, incumbent parties tend to win. When it is shrinking, they tend to lose. This relationship has held remarkably well across many decades of American elections. 25.2.1 Loading and Preparing the Data We will use data compiled by political scientist Alan Abramowitz, who has developed economic voting models for decades. The dataset includes presidential elections from 1952 through 2012, with variables measuring economic growth, presidential approval, and incumbent party vote share. We will add 2016 ourselves to see how well it fits the historical pattern. # Load the data # Note: You&#39;ll need to adjust the path to where your data is stored Abram &lt;- read_dta(&quot;gdpvote.dta&quot;) # Add June approval ratings (net approval: approve minus disapprove) Abram$JuneApp &lt;- c(-8, -26, 50, 30, 59, -7, 23, 5, -27, 19, 16, -17, 10, 18, -1, -30, 2) # Add incumbent party indicator Abram$Inc &lt;- c(0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1) # Add 2016 data # Hillary Clinton (Democrat) was not the incumbent president but represented # the incumbent party. She received 48.2% of the two-party vote # Q2 GDP growth was 1.4%, Obama&#39;s net approval was about 6% Abram &lt;- rbind(Abram, c(year=2016, q2gdp=1.4, vote=51.1, term=0, JuneApp=6, Inc=0)) # View the data structure head(Abram) ## # A tibble: 6 × 6 ## year q2gdp vote term JuneApp Inc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1948 7.60 52.3 1 -8 0 ## 2 1952 0.400 44.6 1 -26 0 ## 3 1956 3.20 57.8 0 50 1 ## 4 1960 -1.90 49.9 1 30 0 ## 5 1964 4.70 61.3 0 59 1 ## 6 1968 7 49.6 1 -7 0 dim(Abram) ## [1] 18 6 Now we have 18 elections worth of data, from 1952 through 2016. The key variables are: vote: The incumbent party’s share of the two-party vote q2gdp: Real GDP growth in the second quarter of the election year JuneApp: The incumbent president’s net approval rating in June Inc: Whether the incumbent president is running (1) or not (0) 25.2.2 The Simple Economic Model Let us start with a basic model using only economic growth to predict incumbent party vote share. This follows a long tradition in political science of using “fundamentals” to predict elections while ignoring polls, campaign events, and media coverage. # Fit model using data through 2012 model_simple &lt;- lm(vote ~ q2gdp, data = Abram[1:17,]) summary(model_simple) ## ## Call: ## lm(formula = vote ~ q2gdp, data = Abram[1:17, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.002 -3.409 0.084 2.078 8.496 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.2560 1.4411 34.179 1.21e-15 *** ## q2gdp 0.7549 0.2578 2.928 0.0104 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.481 on 15 degrees of freedom ## Multiple R-squared: 0.3637, Adjusted R-squared: 0.3213 ## F-statistic: 8.573 on 1 and 15 DF, p-value: 0.01039 # Generate prediction for 2016 pred_2016_simple &lt;- predict(model_simple, newdata = Abram[18,]) actual_2016 &lt;- Abram$vote[18] cat(&quot;2016 Prediction from simple model:&quot;, round(pred_2016_simple, 1), &quot;%\\n&quot;) ## 2016 Prediction from simple model: 50.3 % So we can see that the prediction from this model was %50.3 for Clinton—a narrow victory. In reality, she won 51.1% of the two-party vote (remember, Clinton actually won the popular vote). That means that a model based on just one variable was only off by 0.8%. An error of less than 1 percentage points is actually quite good by historical standards, especially given the simplicity of using only one predictor. Let us visualize this model to see where 2016 fits in historical context: # Create the plot plot(Abram$q2gdp, Abram$vote, xlab = &quot;Q2 GDP Growth (%)&quot;, ylab = &quot;Incumbent Party Vote Share (%)&quot;, main = &quot;Economic Growth and Electoral Success, 1952-2016&quot;, pch = 19, col = &quot;darkblue&quot;, xlim = c(-1, 8), ylim = c(44, 62)) # Add the regression line abline(model_simple, col = &quot;darkgreen&quot;, lwd = 2) # Highlight 2016 in red points(Abram$q2gdp[18], Abram$vote[18], pch = 19, col = &quot;red&quot;, cex = 1.5) text(Abram$q2gdp[18] + 0.5, Abram$vote[18], &quot;2016&quot;, col = &quot;red&quot;, cex = 0.8) # Add year labels for other elections text(Abram$q2gdp[1:17] + 0.3, Abram$vote[1:17], Abram$year[1:17], cex = 0.6, col = &quot;gray40&quot;) # Add horizontal line at 50% abline(h = 50, lty = 2, col = &quot;gray&quot;) The 2016 election (shown in red) sits very close to the regression line. It was not an outlier from the perspective of economic fundamentals. The incumbent party’s performance was about what we would expect given the modest economic growth of 1.4%. 25.2.3 Adding Presidential Approval Political scientists know that economic growth is not the only fundamental that matters. Presidential approval ratings also strongly predict electoral outcomes. Popular presidents can often transfer their popularity to their party’s nominee, while unpopular presidents become anchors dragging down their party. Let us examine how June approval ratings relate to November vote shares: # Plot approval vs vote share plot(Abram$JuneApp, Abram$vote, xlab = &quot;June Net Approval (%)&quot;, ylab = &quot;Incumbent Party Vote Share (%)&quot;, main = &quot;Presidential Approval and Electoral Success&quot;, pch = 19, col = &quot;darkblue&quot;, xlim = c(-35, 65), ylim = c(44, 62)) # Add regression line approval_model &lt;- lm(vote ~ JuneApp, data = Abram) abline(approval_model, col = &quot;darkgreen&quot;, lwd = 2) # Highlight 2016 points(Abram$JuneApp[18], Abram$vote[18], pch = 19, col = &quot;red&quot;, cex = 1.5) text(Abram$JuneApp[18] + 3, Abram$vote[18], &quot;2016&quot;, col = &quot;red&quot;, cex = 0.8) # Add year labels text(Abram$JuneApp[1:17] + 3, Abram$vote[1:17], Abram$year[1:17], cex = 0.6, col = &quot;gray40&quot;) # Add 50% line abline(h = 50, lty = 2, col = &quot;gray&quot;) Presidential approval also predicts incumbent party performance quite well. Obama’s modest positive approval rating in June 2016 suggested his party would receive close to 51% of the vote, which is indeed what happened. Now let us combine both predictors in a multiple regression model: # Fit model with both predictors using data through 2012 model_multiple &lt;- lm(vote ~ q2gdp + JuneApp, data = Abram[1:17,]) summary(model_multiple) ## ## Call: ## lm(formula = vote ~ q2gdp + JuneApp, data = Abram[1:17, ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3125 -1.8805 0.6694 1.1228 4.6448 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.37811 0.87653 56.333 &lt; 2e-16 *** ## q2gdp 0.44771 0.16771 2.670 0.018313 * ## JuneApp 0.14737 0.02859 5.155 0.000146 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.725 on 14 degrees of freedom ## Multiple R-squared: 0.7805, Adjusted R-squared: 0.7491 ## F-statistic: 24.88 on 2 and 14 DF, p-value: 2.459e-05 # Prediction for 2016 pred_2016_multiple &lt;- predict(model_multiple, newdata = Abram[18,]) cat(&quot;2016 Prediction from multiple model:&quot;, round(pred_2016_multiple, 1), &quot;%\\n&quot;) ## 2016 Prediction from multiple model: 50.9 % The multiple regression model, using both economic growth and presidential approval, predicted the Democrats would receive 50.9% of the vote. The actual result was 51.1%. Again, this is remarkably close—-an error of just 0.2 percentage point. 25.3 Understanding Model Fit Through Residuals We have seen that our models make reasonably accurate predictions, at least for 2016. But how can we systematically evaluate model performance across all elections? This is where residuals become essential. A residual is simply the difference between what actually happened and what our model predicted. In mathematical notation, for observation \\(i\\): \\[e_i = Y_i - \\hat{Y}_i\\] Where \\(Y_i\\) is the actual outcome and \\(\\hat{Y}_i\\) is our model’s prediction. Positive residuals mean our model under-predicted (the actual value was higher than predicted), while negative residuals mean our model over-predicted. 25.3.1 Extracting and Examining Residuals Let us calculate residuals for our multiple regression model and examine them systematically: # Refit model with all data including 2016 model_full &lt;- lm(vote ~ q2gdp + JuneApp, data = Abram) # Extract residuals Abram$residuals &lt;- residuals(model_full) Abram$predicted &lt;- fitted(model_full) # Look at residuals for each election residual_table &lt;- Abram[, c(&quot;year&quot;, &quot;vote&quot;, &quot;predicted&quot;, &quot;residuals&quot;)] residual_table$predicted &lt;- round(residual_table$predicted, 1) residual_table$residuals &lt;- round(residual_table$residuals, 1) print(residual_table) ## # A tibble: 18 × 4 ## year vote predicted residuals ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1948 52.3 51.6 0.7 ## 2 1952 44.6 45.7 -1.1 ## 3 1956 57.8 58.2 -0.4 ## 4 1960 49.9 53 -3.1 ## 5 1964 61.3 60.2 1.1 ## 6 1968 49.6 51.5 -1.9 ## 7 1972 61.8 57.2 4.6 ## 8 1976 48.9 51.5 -2.6 ## 9 1980 44.7 41.9 2.8 ## 10 1984 59.2 55.4 3.8 ## 11 1988 53.9 54.1 -0.2 ## 12 1992 46.5 48.8 -2.3 ## 13 1996 54.7 54 0.7 ## 14 2000 50.3 55.6 -5.3 ## 15 2004 51.2 50.5 0.7 ## 16 2008 46.3 45.2 1.1 ## 17 2012 51.4 50.3 1.2 ## 18 2016 51.1 50.9 0.2 Each residual tells a story about that particular election. Large positive residuals indicate elections where the incumbent party over-performed relative to fundamentals (perhaps due to an exceptional campaign or favorable events). Large negative residuals indicate under-performance (perhaps due to scandals or campaign mistakes). Let us identify which elections had the largest prediction errors: # Find elections with largest absolute residuals residual_table$abs_residual &lt;- abs(residual_table$residuals) largest_errors &lt;- residual_table[order(residual_table$abs_residual, decreasing = TRUE)[1:5], ] print(largest_errors[, c(&quot;year&quot;, &quot;vote&quot;, &quot;predicted&quot;, &quot;residuals&quot;)]) ## # A tibble: 5 × 4 ## year vote predicted residuals ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2000 50.3 55.6 -5.3 ## 2 1972 61.8 57.2 4.6 ## 3 1984 59.2 55.4 3.8 ## 4 1960 49.9 53 -3.1 ## 5 1980 44.7 41.9 2.8 The elections with the largest residuals often have interesting stories behind them. The 2000 election is actually the largest election, where nominee Al Gore significantly under-performed what we would have expected given a booming economy and relatively popular incumbent. Meanwhile, Richard Nixon significantly outperformed expectations in 1972–the election where the Nixon campaign used shady tactics that later lead to his resignation. 25.3.2 Three-Dimensional Visualization With two predictors and one outcome, we can visualize our regression model in three dimensions. This helps build intuition about what multiple regression is doing—fitting a plane through a cloud of points in three-dimensional space: # Create 3D scatterplot library(scatterplot3d) s3d &lt;- scatterplot3d(Abram$q2gdp, Abram$JuneApp, Abram$vote, xlab = &quot;Q2 GDP Growth (%)&quot;, ylab = &quot;June Approval (%)&quot;, zlab = &quot;Vote Share (%)&quot;, main = &quot;The Regression Plane in 3D&quot;, pch = 19, color = &quot;darkblue&quot;, angle = 30, grid = TRUE) # Add the regression plane s3d$plane3d(model_full, lty.box = &quot;solid&quot;, col = &quot;darkgray&quot;, lwd = 0.5) # Get fitted values from the model fitted_vals &lt;- fitted(model_full) # Add vertical lines from points to the plane (residuals) # Convert 3D coordinates to 2D for plotting orig &lt;- s3d$xyz.convert(Abram$q2gdp, Abram$JuneApp, Abram$vote) plane &lt;- s3d$xyz.convert(Abram$q2gdp, Abram$JuneApp, fitted_vals) # Draw the residual lines for(i in 1:nrow(Abram)) { segments(orig$x[i], orig$y[i], plane$x[i], plane$y[i], col = &quot;red&quot;, lty = 2, lwd = 1) } # Re-plot the points on top so they&#39;re visible s3d$points3d(Abram$q2gdp, Abram$JuneApp, Abram$vote, pch = 19, col = &quot;darkblue&quot;) Each point represents one election, positioned according to its economic growth (x-axis), approval rating (y-axis), and vote share (z-axis). The regression plane cuts through this three-dimensional space, positioned to minimize the sum of squared vertical distances from the points to the plane. These vertical distances are exactly our residuals. 25.4 From Residuals to R-Squared Individual residuals tell us about specific predictions, but we need a summary measure of overall model fit. This is where R-squared comes in. R-squared answers a simple question: What proportion of the variation in our outcome does our model explain? To understand R-squared, we need to think about variation. The total variation in our outcome (incumbent vote share) can be measured as the sum of squared deviations from the mean: \\[TSS = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2\\] This is called the Total Sum of Squares (TSS). It measures how much the vote shares vary across all elections. We also can measure the variation in our model. The unexplained variation is the sum of squared error: \\[SSE = \\sum_{i=1}^{n} (\\hat{Y}_i - \\hat{Y}_i)^2 = \\sum_i^n e_i^2\\] Intuitively, therefore, the variation we have managed to explain must be TSS - SSE The percentage of the variation we have explained, therefore, is: \\[ R^2 = \\frac{\\text{Explained variation}}{\\text{Total variation }} = \\frac{\\text{TSS- SSE}}{\\text{TSS}} = 1-\\frac{\\text{SSE}}{\\text{TSS}} \\] 25.4.1 Calculating R-Squared Manually Let us calculate R-squared step by step to understand what it represents: # Calculate the components y_mean &lt;- mean(Abram$vote) # Total Sum of Squares TSS &lt;- sum((Abram$vote - y_mean)^2) # Residual Sum of Squares SSE &lt;- sum(Abram$residuals^2) # Calculate R-squared r_squared_manual &lt;- 1 - SSE/TSS cat(&quot;\\nR-squared (calculated manually):&quot;, round(r_squared_manual, 4), &quot;\\n&quot;) ## ## R-squared (calculated manually): 0.7807 We can compare this to the value R calcualates: # Compare to R&#39;s calculation summary(model_full)$r.squared ## [1] 0.7807422 Our R-squared of about 0.78 means that economic growth and presidential approval together explain 78% of the variation in incumbent party vote shares across these 18 elections. That is quite impressive for a model with just two predictors. 25.4.2 Interpreting R-Squared What does it mean to explain 78% of the variation? Think about it this way: If we knew nothing about economics or approval ratings and had to predict each election’s outcome, our best guess would be the historical average (about 52% for the incumbent party). Using that naive prediction, we would make errors. The total size of those errors is the TSS. Now, using our model with economic and approval data, we make much smaller errors. Our model reduces the total error by 78%. Put differently, knowing the state of the economy and presidential approval gets us 78% of the way from complete ignorance to perfect prediction. Is 78% good? That depends on context. In the social sciences, where human behavior is complex and multiply determined, an R-squared of 0.78 is excellent. In physics or engineering, where relationships follow precise laws, we might expect R-squared values closer to 0.99. 25.4.3 Comparing Models with R-Squared Let us compare the R-squared values for different models to see how adding predictors affects model fit: # Model 1: Just GDP model1 &lt;- lm(vote ~ q2gdp, data = Abram) r2_model1 &lt;- summary(model1)$r.squared # Model 2: Just Approval model2 &lt;- lm(vote ~ JuneApp, data = Abram) r2_model2 &lt;- summary(model2)$r.squared # Model 3: Both GDP and Approval model3 &lt;- lm(vote ~ q2gdp + JuneApp, data = Abram) r2_model3 &lt;- summary(model3)$r.squared # Create comparison table comparison &lt;- data.frame( Model = c(&quot;GDP only&quot;, &quot;Approval only&quot;, &quot;GDP + Approval&quot;), Predictors = c(1, 1, 2), R_squared = round(c(r2_model1, r2_model2, r2_model3), 4) ) print(comparison) ## Model Predictors R_squared ## 1 GDP only 1 0.3636 ## 2 Approval only 1 0.6680 ## 3 GDP + Approval 2 0.7807 Notice that the model with both predictors has a higher R-squared than either single-predictor model. This is always true: Adding predictors can never decrease R-squared. Even adding a completely useless predictor (like the number of penguins in Antarctica) would not decrease R-squared—it would stay the same or increase slightly due to random chance. This presents a problem. If R-squared always increases with more predictors, we might be tempted to add many predictors to inflate our R-squared. But this would be misleading. A model with many predictors might fit the current data well (high R-squared) but predict new data poorly (overfitting). 25.4.4 Adjusted R-Squared To address this issue, statisticians developed adjusted R-squared, which penalizes models for having more predictors: \\[R^2_{adj} = 1 - \\frac{SSE/(n-k-1)}{TSS/(n-1)}\\] Where \\(n\\) is the sample size and \\(k\\) is the number of predictors. Unlike regular R-squared, adjusted R-squared can decrease when you add unhelpful predictors. The adjusted R-squared is slightly lower than the regular R-squared, and the penalty increases with more predictors. This helps us make fair comparisons between models with different numbers of predictors. 25.5 The F-Test: Testing Overall Model Significance While R-squared tells us how much variation our model explains, it does not tell us whether this amount is statistically significant. Even fitting random noise to an outcome would produce some positive R-squared by chance. The F-test addresses this by testing whether our model explains significantly more variation than we would expect by chance. 25.5.1 The Logic of the F-Test The F-test compares two models: The null model: All slope coefficients equal zero (only the intercept) Our model: At least one slope coefficient is non-zero In our electoral example, the null hypothesis is: \\[H_0: \\beta_{GDP} = 0 \\text{ and } \\beta_{Approval} = 0\\] The alternative hypothesis is: \\[H_A: \\text{At least one } \\beta \\neq 0\\] This is different from the individual t-tests we have seen before. Each t-test examines one predictor while controlling for others. The F-test examines all predictors simultaneously. 25.5.2 Understanding the F-Statistic The F-statistic is a ratio comparing explained variation to unexplained variation, adjusted for degrees of freedom: \\[F = \\frac{R^2/k}{(1-R^2)/(n-k-1)}\\] Where: \\(k\\) is the number of predictors \\(n\\) is the sample size \\(n-k-1\\) is the residual degrees of freedom A large F-statistic means our model explains much more variation than we would expect by chance. Under the null hypothesis (all slopes equal zero), the F-statistic follows an F distribution with \\(k\\) and \\(n-k-1\\) degrees of freedom. 25.5.3 Reading F-Test Output Let us examine the F-test for our electoral model: # Get full model summary summary_full &lt;- summary(model_full) f_stat &lt;- summary_full$fstatistic # save this for later summary_full ## ## Call: ## lm(formula = vote ~ q2gdp + JuneApp, data = Abram) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3178 -1.6989 0.4295 1.0967 4.6420 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.3951 0.8179 60.393 &lt; 2e-16 *** ## q2gdp 0.4461 0.1606 2.777 0.0141 * ## JuneApp 0.1474 0.0276 5.342 8.22e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.633 on 15 degrees of freedom ## Multiple R-squared: 0.7807, Adjusted R-squared: 0.7515 ## F-statistic: 26.71 on 2 and 15 DF, p-value: 1.141e-05 The F-statistic is in the very last row of the output, along with the degrees of freedom and the p-value. The very small p-value (well below 0.05) tells us that our model performs significantly better than simply predicting the mean for every election. The economic and approval variables jointly provide meaningful predictive power. 25.5.4 The Connection Between F and R-Squared The F-statistic and R-squared are closely related. We can actually calculate the F-statistic directly from R-squared. This relationship shows that the F-test is essentially testing whether R-squared is significantly greater than zero. A model with R-squared of zero would have an F-statistic of zero. As R-squared increases, so does the F-statistic. 25.5.5 Individual t-tests vs. Overall F-test It is important to understand the difference between the t-tests for individual coefficients and the overall F-test. Consider this scenario: You have a model with five predictors. Four have p-values around 0.10 (not quite significant), and one has a p-value of 0.06 (marginally significant). Should you conclude the model is useless? Not necessarily! The F-test might show that the five predictors jointly have significant explanatory power, even if no single predictor is significant on its own. This can happen when predictors are correlated—each one’s individual contribution might be modest, but together they explain substantial variation. 25.6 Testing Nested Models: The Case of Categorical Variables A common challenge in regression analysis involves categorical predictors with multiple levels. Consider party identification in American politics: Democrat, Republican, and Independent. To include party in a regression, we create dummy variables for Democrat and Republican, leaving Independent as the reference category. But this raises a question: What if the Democrat dummy is significant but the Republican dummy is not? Should we keep both in the model or drop the non-significant one? This is where testing nested models becomes essential. 25.6.1 Setting Up the Problem Let us work with a concrete example using ANES data to predict Trump thermometer ratings. We will examine how age relates to Trump evaluations and whether this relationship persists after controlling for party identification. # Load actual ANES data anes &lt;- read.csv(&quot;anes2020.csv&quot;) # Filter to remove missing values on key variables anes &lt;- anes %&gt;% filter(!is.na(trump.th) &amp; !is.na(age) &amp; !is.na(partyid)) # Create party dummy variables # partyid has values: &quot;Democrat&quot;, &quot;Independent&quot;, &quot;Republican&quot;, &quot;Something else&quot; # We&#39;ll make Independents and others the reference category anes &lt;- anes %&gt;% mutate( Democrat = if_else(partyid == 1, 1, 0), Republican = if_else(partyid== 2, 1, 0) ) # Check our new variables table(anes$Democrat) ## ## 0 1 ## 4996 2710 table(anes$Republican) ## ## 0 1 ## 5308 2398 # Create a subset for our analysis (you can use full data if preferred) # Using the actual ANES data structure anes_data &lt;- anes %&gt;% select(trump_th = trump.th, age, Democrat, Republican) %&gt;% na.omit() 25.6.2 Individual Significance vs. Joint Significance Let us fit a model with age and party dummies: # Model with age and party model_party &lt;- lm(trump_th ~ age + Democrat + Republican, data = anes_data) summary(model_party) ## ## Call: ## lm(formula = trump_th ~ age + Democrat + Republican, data = anes_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -83.106 -11.436 -6.096 18.446 94.792 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.55327 1.05530 28.95 &lt; 2e-16 *** ## age 0.11086 0.01857 5.97 2.48e-09 *** ## Democrat -27.56271 0.76322 -36.11 &lt; 2e-16 *** ## Republican 43.68392 0.79204 55.15 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27.74 on 7702 degrees of freedom ## Multiple R-squared: 0.5273, Adjusted R-squared: 0.5271 ## F-statistic: 2864 on 3 and 7702 DF, p-value: &lt; 2.2e-16 Look at the coefficients table carefully. Each party dummy has its own t-test and p-value. These test whether that specific party differs from the reference category (Independents), controlling for age and the other party dummy. But here is the key insight: Even if one party dummy is not individually significant, the party variables as a group might still be important. We should not make decisions about dropping variables based solely on individual p-values when those variables are part of a logical group. 25.6.3 The Problem with Individual Tests Suppose we found that the Republican dummy was highly significant (p &lt; 0.001) but the Democrat dummy was marginal (p = 0.08). Should we keep just the Republican dummy and drop the Democrat dummy? No! The Democrat and Republican dummies are part of a single conceptual variable (party identification). They should be tested together, kept together, or dropped together. Testing them jointly requires an F-test comparing two nested models. 25.6.4 Testing Nested Models with an F-Test A nested model comparison uses an F-test to determine whether a set of variables significantly improves model fit. We compare: Restricted model: Without the party variables (just age) Unrestricted model: With the party variables (age plus party dummies) # Restricted model (without party) model_restricted &lt;- lm(trump_th ~ age, data = anes_data) # Unrestricted model (with party) model_unrestricted &lt;- lm(trump_th ~ age + Democrat + Republican, data = anes_data) # Compare models with F-test anova_result &lt;- anova(model_restricted, model_unrestricted) print(anova_result) ## Analysis of Variance Table ## ## Model 1: trump_th ~ age ## Model 2: trump_th ~ age + Democrat + Republican ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 7704 12405732 ## 2 7702 5927050 2 6478683 4209.4 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The F-test tells us whether the party variables as a group significantly improve the model. This is the appropriate test for categorical variables, not the individual t-tests. 25.6.5 Understanding the F-Test for Nested Models The F-statistic for nested models compares the improvement in fit (reduction in RSS) to what we would expect by chance: \\[F = \\frac{(SSE_{restricted} - SSE_{unrestricted})/q}{SSE_{unrestricted}/(n-k-1)}\\] Where: \\(q\\) is the number of restrictions (variables being tested) \\(k\\) is the total number of predictors in the unrestricted model \\(n\\) is the sample size The large reduction in SSE when adding party variables translates to a large F-statistic and small p-value, confirming that party identification matters for Trump evaluations. 25.7 Practical Guidelines for Categorical Variables When working with categorical variables in regression: Always create k-1 dummy variables for k categories. One category must be the reference group. Test dummy variables jointly, not individually. Use an F-test to determine whether the set of dummies improves the model. Keep or drop dummy variables as a group. If party matters, include all party dummies, even if some individual coefficients are not significant. Choose your reference category thoughtfully. The reference category affects interpretation of all dummy coefficients. Remember that individual non-significance does not mean joint non-significance. Multiple marginally significant predictors can be jointly highly significant. 25.8 A Word of Caution Remember that model fit statistics tell us how well our model describes the current data, not necessarily how well it will predict new data. A model can have excellent fit statistics but still: Overfit the data. Too many predictors can make the model fit the quirks of your specific sample rather than general patterns. Miss important non-linearities. High R-squared does not guarantee you have the right functional form. Suffer from omitted variable bias. Even with high R-squared, missing confounders can make coefficients misleading. Lack external validity. A model that fits historical data well might fail when circumstances change. This is why we care about theory, not just fit statistics. A model grounded in solid theory with moderate R-squared often outperforms an atheoretical model with high R-squared when predicting new data or designing policy. 25.9 Summary and Key Takeaways We have covered extensive ground in understanding model fit assessment. Let us review the key concepts and their relationships. Residuals are the foundation of model assessment. Each residual tells us how far off our prediction was for that observation. By examining residuals collectively—their magnitude, patterns, and distribution—we can diagnose model problems and identify unusual observations. R-squared builds on residuals to provide a single summary of model fit. It tells us what proportion of variation in our outcome our model explains. While useful for comparing models, R-squared has limitations. It always increases with more predictors, even useless ones, which is why we also consider adjusted R-squared. The F-test moves beyond description to inference. While R-squared tells us how much variation we explain, the F-test tells us whether this amount is statistically significant. It tests whether our predictors jointly provide more explanatory power than we would expect by chance. For categorical variables, we discovered that individual significance tests can be misleading. Party identification might matter even if one party dummy is not individually significant. The solution is testing groups of related variables jointly using F-tests for nested models. Throughout, we saw how these tools work together. We start with residual examination to understand specific predictions, calculate R-squared to summarize overall fit, use F-tests to establish statistical significance, and compare nested models to build our specification systematically. The 2016 election example illustrated an important broader point: Statistical models are about patterns and uncertainty, not perfect prediction. Our model suggested Clinton would receive about 50.5% of the vote; she received about 51%. This small error is well within the range of normal model uncertainty. The “failure” of 2016 was not in the statistical models but in the overconfident interpretation of polling data by non-academics and journalists. 25.10 Review Questions 25.10.1 Conceptual Understanding Explain the relationship between minimizing squared residuals and maximizing R-squared. Are these the same goal or different goals? How are they related mathematically? A researcher finds that in a regression with three party dummy variables (Democrat, Republican, and Green, with Independent as reference), only the Democrat dummy is statistically significant at p &lt; 0.05. The Republican dummy has p = 0.08 and the Green dummy has p = 0.15. Should they drop the non-significant dummies from the model? Explain your reasoning and describe how you would test whether to keep all party dummies. Why does R-squared always increase (or stay the same) when you add predictors, even useless ones? Explain the intuition and why this property makes adjusted R-squared necessary. What is the difference between the F-test reported in standard regression output and the F-test used to compare nested models? When would you use each one? 25.11 Practical Application You have regression output showing residuals for 20 observations. Calculate R² manually from the following information: SSE = 250 Total sum of squares (TSS) = 1000 Then answer: If you added a predictor and SSE decreased to 240, what would the new R² be? Did the model improve? You are comparing two models: Model A: 3 predictors, R² = 0.50, Adjusted R² = 0.47, n = 50 Model B: 7 predictors, R² = 0.55, Adjusted R² = 0.46, n = 50 Which model would you prefer and why? What additional information would help your decision? 25.11.1 Critical Thinking Was 2016 really a “failed prediction” from a statistical modeling perspective? Use concepts of residuals, confidence intervals, and model uncertainty in your answer. What would constitute a true prediction failure versus normal model error? A student argues: “My model has an R² of 0.95, so it will predict future outcomes very accurately.” What concerns would you raise about this interpretation? Give at least three reasons why high R² in the current sample does not guarantee good future predictions. Explain when and why you would use an F-test instead of looking at individual t-tests. Create a specific research scenario where the F-test would reveal something important that the individual t-tests would miss. 25.11.2 Data Analysis Challenge You see the following at the bottom of regression output: Multiple R-squared: 0.6234, Adjusted R-squared: 0.5887 F-statistic: 17.32 on 4 and 42 DF, p-value: 3.67e-08 Translate this into plain English that a non-statistician could understand. What does each number tell us about the model? A model predicting voter turnout has R² = 0.15. A critic says, “This model is useless because it only explains 15% of the variation.” How would you respond? Is 15% necessarily bad? What context would you need to evaluate whether this R² is acceptable? "],["issues-with-regression-analysis.html", "26 Issues with Regression Analysis 26.1 Learning Objectives 26.2 What Is This For? 26.3 The Gauss-Markov Assumptions in Multivariate Regression 26.4 Diagnosing Regression Problems: A Practical Approach 26.5 A Complete Worked Example: International Trade Data 26.6 A Complete Worked Example: International Trade Data 26.7 When OLS Isn’t Enough: Recognizing Special Cases 26.8 A Reference Guide to Formal Diagnostic Tests 26.9 Summary: A Workflow for Careful Regression Analysis 26.10 Review Questions", " 26 Issues with Regression Analysis knitr::opts_chunk$set(echo = TRUE) 26.1 Learning Objectives By the end of this module, you will be able to: Understand the Gauss-Markov assumptions that justify regression inference in multivariate settings Diagnose violations of these assumptions using visual diagnostics and residual plots Understand the practical consequences of assumption violations for your research Know about appropriate fixes including variable transformations, robust standard errors, and outlier investigation (even if you may not know how to implement them on your own) To motivate future training, we will also discuss when special models (logistic regression, time series methods) are needed beyond OLS. These models are beyond the scope of this class, but this lecture can serve to motivate why more advanced training in data science may be useful. 26.2 What Is This For? Running a regression is easy. Running a regression correctly requires checking whether your data meet certain conditions. These conditions, called the Gauss-Markov assumptions, are the foundation that makes our hypothesis tests, confidence intervals, and p-values valid. When these assumptions are violated, our statistical inference can be completely wrong. We might conclude a relationship exists when it does not, or fail to detect a real relationship, or badly misestimate the strength of an effect. 26.3 The Gauss-Markov Assumptions in Multivariate Regression In our earlier reading, we introduced the Gauss-Markov assumptions for simple bivariate regression. These assumptions specify the conditions under which our regression estimates have desirable properties and our statistical inference is valid. Now that we are working with multivariate regression, we need to extend these assumptions slightly and understand what they mean in this richer context. 26.3.1 Review: Why Assumptions Matter Before we list the assumptions, let us be clear about why they matter. The regression line itself—the slope and intercept—can always be calculated mechanically. R will happily compute coefficients for any data you give it. The assumptions do not determine whether you can run a regression. They determine whether the results mean what you think they mean. Specifically, the Gauss-Markov assumptions justify our use of standard errors, t-statistics, p-values, and confidence intervals. When these assumptions hold, the standard errors correctly measure the uncertainty in our estimates. When they are violated, the standard errors can be dramatically wrong, leading us to be far too confident or too uncertain about our conclusions. They also justify our claim that our regression coefficients are unbiased – correct in expectation. Think of it this way: the regression line is your best guess about the relationship between variables. The standard error tells you how confident you should be in that guess. The assumptions determine whether your confidence is justified and your estimates correct on average. 26.3.2 The Five Core Assumptions Let us now state the five key assumptions. We present them first in plain language, then discuss what each means in practice. Assumption 1: Linearity. The relationship between the outcome variable and the predictor variables follows a linear form. In multivariate regression, this means that the effect of each predictor on the outcome is linear, holding other predictors constant. Assumption 2: Zero Conditional Mean of Errors. The errors have an expected value of zero at all combinations of the predictor variables. This means our model is not systematically over-predicting or under-predicting for certain values of our predictors. Assumption 3: No Perfect Multicollinearity. No predictor variable is a perfect linear combination of other predictor variables. This assumption is specific to multivariate regression. It ensures we can actually estimate the separate effect of each predictor. Assumption 4: Independence of Errors. The error for one observation does not predict the error for another observation. This is often violated in time series data (where consecutive observations are related) or clustered data (where observations within groups are more similar). Assumption 5: Homoscedasticity (Constant Variance). The variance of the errors is the same across all values of the predictor variables. The spread of points around the regression line should not systematically grow or shrink as predictors change. 26.3.3 Understanding Each Assumption Let us now explore what each assumption means more concretely and see what violations look like. 26.3.3.1 Linearity The linearity assumption requires that the true relationship between each X and Y is linear, or at least approximately linear. This does not mean the relationship must be perfectly straight in a scatterplot, but it does mean a straight line should be a reasonable approximation. Why does this matter? If the true relationship curves but we fit a straight line, we will systematically mispredict the outcome. Consider campaign spending and vote share. If the first $100,000 has large effects but additional spending has diminishing returns, the relationship is nonlinear. A straight line will poorly capture this pattern, over-predicting effects for high spenders and under-predicting for low spenders. The good news is that many relationships that appear nonlinear can be made approximately linear through variable transformations. Taking logarithms is particularly common. 26.3.3.2 Zero Conditional Mean This assumption states that errors average to zero regardless of the values of our predictors. Put differently, our model should not systematically miss high or low at certain predictor values. Violations of this assumption often indicate we have left out an important variable or misspecified the functional form. For example, if we try to predict voting behavior using only age but ignore education, our errors might be systematically positive for highly educated people (we under-predict their turnout) and systematically negative for less educated people (we over-predict their turnout). When the zero conditional mean assumption holds, we can trust that our regression line passes through the cloud of data appropriately. When it fails, our slope estimates may be biased. 26.3.3.3 No Perfect Multicollinearity This assumption is unique to multivariate regression. It requires that no predictor variable can be written as an exact linear combination of other predictors. Perfect multicollinearity makes it mathematically impossible to estimate separate effects. Consider an extreme example. Suppose we try to predict vote share using both “years of education” and “months of education” (which is just years times 12). These variables contain exactly the same information. R cannot determine which one is driving any relationship we see. Similarly, if we include dummy variables for all categories of a variable plus an intercept, we have perfect multicollinearity. More commonly, we face high but not perfect multicollinearity. Predictors are highly correlated but not perfectly so. This does not make estimation impossible, but it inflates standard errors and makes coefficients unstable. Small changes in the data or model can lead to large changes in estimated effects. 26.3.3.4 Independence of Errors Independence requires that knowing the error for one observation tells us nothing about errors for other observations. This assumption is frequently violated in real political science data. Time series data naturally violates independence. If the unemployment rate is unusually high this quarter, it is likely to be high next quarter too. The errors are autocorrelated. Similarly, in survey data, respondents from the same state or social network may have correlated errors because they face similar contexts or influence each other. Violations of independence typically lead to underestimated standard errors. We become overconfident in our results, concluding relationships are significant when they are not. 26.3.3.5 Homoscedasticity Homoscedasticity means “same variance.” This assumption requires that the spread of errors around the regression line is constant across all values of the predictors. Violations create heteroscedasticity, where variance changes. This is common in economic data. For example, when studying income and political donations, wealthier people might have more variable donation patterns than poor people (some donate a lot, others nothing, while poor people consistently donate little or nothing). The errors have larger variance at high income levels. Heteroscedasticity makes standard errors wrong. Whether they are too big or too small depends on the specific pattern. But crucially, we can fix this problem relatively easily using robust standard errors. 26.3.4 What Happens When Assumptions Are Violated? Understanding the consequences of violations helps us prioritize our diagnostic efforts. Not all violations are equally serious, and not all require the same response. Linearity violations bias our coefficient estimates. If the true relationship curves and we fit a straight line, our slope will be wrong. This is a fundamental problem that usually requires fixing through transformation or model respecification. Zero conditional mean violations also bias coefficients. These violations often indicate omitted variables or misspecified functional forms. They are serious and require model changes. Multicollinearity does not bias coefficients, but it inflates standard errors and makes estimates unstable. We become less certain about separate effects of correlated predictors. Independence violations typically lead to underestimated standard errors. Our p-values are too small and confidence intervals too narrow. We are overconfident. Heteroscedasticity makes standard errors wrong (either too large or too small). Like independence violations, this is an inference problem, not a bias problem. But it is easier to fix. The key insight is that some violations affect our point estimates (linearity, zero conditional mean), while others affect our uncertainty estimates (independence, heteroscedasticity). Both matter, but they require different solutions. 26.3.5 A Visual Example: When Assumptions Hold Before we learn to diagnose violations, let us see what regression diagnostics look like when assumptions are satisfied. We will use the economic voting data we have worked with before. library(dplyr) library(ggplot2) # Load economic voting data votes &lt;- read.csv(&quot;votes.csv&quot;) # Simple regression: vote share on income growth model_good &lt;- lm(vote ~ rdi4, data = votes) summary(model_good) ## ## Call: ## lm(formula = vote ~ rdi4, data = votes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6842 -3.7406 -0.2731 2.6357 7.5002 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 45.9385 1.6919 27.152 3.62e-14 *** ## rdi4 2.2906 0.5342 4.288 0.000648 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.765 on 15 degrees of freedom ## Multiple R-squared: 0.5507, Adjusted R-squared: 0.5207 ## F-statistic: 18.38 on 1 and 15 DF, p-value: 0.0006477 This regression examines whether income growth during presidential terms predicts incumbent party vote share. Now let us create the key diagnostic plot: residuals versus fitted values. # Extract residuals and fitted values votes$residuals &lt;- residuals(model_good) votes$fitted &lt;- fitted(model_good) # Create diagnostic plot ggplot(votes, aes(x = fitted, y = residuals)) + geom_point(size = 3, alpha = 0.7) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_smooth(se = FALSE, color = &quot;blue&quot;, linewidth = 1) + labs(title = &quot;Residual Plot: Economic Voting Model&quot;, subtitle = &quot;No clear pattern suggests assumptions are reasonable&quot;, x = &quot;Fitted Values (Predicted Vote Share)&quot;, y = &quot;Residuals&quot;) + theme_minimal() ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; What should we see in this plot if assumptions hold? We want to see a random scatter of points with no clear pattern. The points should spread roughly evenly above and below the horizontal line at zero. The blue smoothing line should stay close to zero without systematic curves or trends. This plot looks reasonably good. There is no obvious curved pattern (suggesting linearity holds), no fan shape (suggesting homoscedasticity holds), and no systematic trends. While we never expect perfect randomness with real data and small samples, this plot does not raise red flags. Compare this to what we will see when examining the international trade data, where clear violations will be apparent. Learning to distinguish “acceptably random” from “systematically patterned” is a key skill that comes with practice. 26.3.6 The Philosophy of Diagnostics Before we move to specific diagnostic techniques, we want to emphasize the right attitude toward assumption checking. Some students view diagnostics as obstacles or bureaucratic requirements. This misses the point entirely. Diagnostics are how we ensure our findings are real. They are how we distinguish genuine discoveries from statistical artifacts. When you find a violation and fix it, you are not weakening your analysis—you are making it more credible. When you find a violation you cannot fix and acknowledge it honestly, you are demonstrating scientific integrity. Good researchers actively look for problems in their own work. They run diagnostics carefully and report what they find, even when results are inconvenient. This is not because of arbitrary rules, but because we care whether our conclusions are correct. The next sections will teach you specific diagnostic techniques for each assumption. As you learn these tools, remember that the goal is not to get a “clean” regression where everything looks perfect. The goal is to understand your data well enough to draw valid conclusions and to be honest about limitations. Real data rarely satisfy all assumptions perfectly. That is okay. What matters is recognizing important violations, addressing them appropriately, and acknowledging remaining limitations. 26.4 Diagnosing Regression Problems: A Practical Approach Now that we understand the assumptions and why they matter, we need practical tools to diagnose violations. While many sophisticated diagnostic techniques exist, one plot does most of the heavy lifting: the residual plot. This section teaches you how to read residual plots and what patterns to look for. 26.4.1 The Residual Plot: Your Primary Diagnostic Tool A residual plot graphs the residuals (observed minus predicted values) on the vertical axis against the fitted values (predictions) on the horizontal axis. This simple plot reveals most major assumption violations at a glance. What you want to see: Random scatter. Points bouncing around zero with no systematic pattern. Roughly equal spread above and below the horizontal line at zero. No curves, no fans, no clusters. What indicates problems: Curved patterns suggest non-linearity. If residuals are systematically positive at low fitted values, negative at middle values, and positive again at high values (or any similar curve), your straight line is missing the true relationship’s shape. Fan or funnel shapes suggest heteroscedasticity. If the vertical spread of residuals grows or shrinks as fitted values increase, the variance of errors is not constant. This often looks like a megaphone opening to the right. Patterns over time or groups suggest non-independence. If you color points by time period or group and see clustering or trends, errors are correlated. This is common in time series or multilevel data. Individual extreme points suggest outliers or influential observations. Points far from the main cloud, especially those with high leverage, can dominate your regression results. 26.4.2 Supporting Diagnostic Plots While the residual plot is primary, several other plots provide additional information: Scale-location plot plots the square root of absolute standardized residuals against fitted values. This makes heteroscedasticity easier to spot. A horizontal smoothing line indicates constant variance; an upward or downward trend indicates heteroscedasticity. Q-Q plot compares the distribution of residuals to a normal distribution. We do not emphasize this much because violations of normality are often less serious than other violations, especially with large samples. But severe departures from normality can indicate outliers or model misspecification. Residuals versus predictors plots residuals against each individual predictor variable. These can reveal non-linearity specific to one predictor that might not be obvious in the residual versus fitted plot. Cook’s distance measures how much the regression changes when each observation is removed. High values identify influential points that disproportionately affect your results. We will not cover these plots in this class, but you may see them in other textbooks. 26.4.3 The Diagnostic Process Diagnosing and fixing regression problems is iterative. You do not check all assumptions once and declare victory. Instead, you follow a cycle: Fit your initial model Create diagnostic plots Identify the most serious problems Apply appropriate fixes Re-check the diagnostics Repeat as needed Different problems require different solutions, and fixing one problem sometimes reveals or creates others. This is normal. The goal is not perfection but improvement and honesty about remaining limitations. In the next section, we will work through this process with real data, showing how each diagnostic reveals problems and how each fix improves the model. 26.5 A Complete Worked Example: International Trade Data Let us now work through a complete example showing how to diagnose and fix multiple regression problems. We will use data on international trade relationships to show the full diagnostic process in action. 26.6 A Complete Worked Example: International Trade Data A Note on Expectations: This section walks through an extended example showing how to diagnose and fix multiple regression problems. This material gets into more advanced territory than what we typically cover in this course. You are not expected to be able to execute all of these fixes independently. Rather, as you work through this example, focus on three key learning objectives: Know the assumptions. Understand what each Gauss-Markov assumption means and why it matters. Identify violations. If I show you a residual plot with a problem, you should be able to recognize which assumption is violated (curved pattern = non-linearity, fan shape = heteroscedasticity, etc.). Know the toolkit. Be aware of what general approaches exist for addressing different violations (transformations and polynomial terms for non-linearity, robust standard errors for heteroscedasticity, clustering for grouped data, etc.). We walk through the detailed implementation here for two reasons: First, some of you working on poster projects may encounter these issues and need to address them. Second, this material prepares you for more advanced statistics courses where these techniques will be covered in depth. For now, the goal is conceptual understanding—recognizing problems and knowing what tools exist—rather than independent implementation. The detailed R code and step-by-step fixes are presented to give you a complete picture of the diagnostic process and to support your future statistical training. Let us now work through a complete example showing how to diagnose and fix multiple regression problems. We will use data on international trade relationships to show the full diagnostic process in action. 26.6.1 The Data and Research Question The World Trade Organization (WTO) dataset contains information about bilateral trade between pairs of countries from 1948 to 1999. Each observation represents the trade relationship between two specific countries in a particular year. With over 234,000 observations spanning five decades, this dataset provides a rich example for exploring regression diagnostics. Our research question is straightforward: How do trade volume and geographic distance predict the combined economic output (GDP) of trading partners? This is a fundamental question in international economics, touching on theories about how trade relationships and geographic proximity relate to economic development. Let us load the data and examine its structure: # Load necessary packages library(dplyr) library(ggplot2) library(lmtest) ## we will use this package below library(sandwich) ## we will use this package below # Load the WTO trade data wto &lt;- read.csv(&quot;wto_trade.csv&quot;) # Examine the data structure glimpse(wto) ## Rows: 234,597 ## Columns: 11 ## $ cty1name &lt;chr&gt; &quot;UNITED STATES&quot;, &quot;UNITED STATES&quot;, &quot;UNITED STATES&quot;, &quot;UNITED STATES&quot;, &quot;UN… ## $ cty2name &lt;chr&gt; &quot;UNITED KINGDOM&quot;, &quot;UNITED KINGDOM&quot;, &quot;UNITED KINGDOM&quot;, &quot;UNITED KINGDOM&quot;,… ## $ year &lt;int&gt; 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959,… ## $ trade &lt;dbl&gt; 20744420, 21027616, 19165702, 28643821, 25336248, 24164768, 24002431, 3… ## $ bothin &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ onein &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ gsp &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ distance &lt;dbl&gt; 4330.469, 4330.469, 4330.469, 4330.469, 4330.469, 4330.469, 4330.469, 4… ## $ real_gdp_prod &lt;dbl&gt; 2.413700e+23, 2.350101e+23, 3.647399e+23, 3.998128e+23, 4.022161e+23, 4… ## $ real_gdp_pc_prod &lt;dbl&gt; 32775161, 31183144, 47324955, 50846402, 50152045, 53763332, 54388570, 6… ## $ area_prod &lt;dbl&gt; 2.357396e+12, 2.357396e+12, 2.357396e+12, 2.357396e+12, 2.357396e+12, 2… # Look at key variables summary(wto[, c(&quot;real_gdp_prod&quot;, &quot;trade&quot;, &quot;distance&quot;, &quot;year&quot;)]) ## real_gdp_prod trade distance year ## Min. :2.337e+15 Min. :0.000e+00 Min. : 43.93 Min. :1948 ## 1st Qu.:1.086e+20 1st Qu.:3.354e+03 1st Qu.: 2284.95 1st Qu.:1972 ## Median :6.132e+20 Median :2.846e+04 Median : 4278.15 Median :1982 ## Mean :2.041e+22 Mean :1.329e+06 Mean : 4518.84 Mean :1981 ## 3rd Qu.:3.651e+21 3rd Qu.:2.135e+05 3rd Qu.: 6252.36 3rd Qu.:1992 ## Max. :4.597e+25 Max. :1.092e+09 Max. :12351.26 Max. :1999 Our key variables are: real_gdp_prod: The product of real GDP for the two countries (our outcome) trade: Average value of real bilateral trade between the countries distance: Shortest distance between the countries (in miles) year: Year of observation (1948-1999) cty1name and cty2name: Names of the two countries in the relationship The data has a complex structure. Each country-pair appears multiple times (once per year). For example, US-UK trade over 50 times (1948-1999). This repeated observation of the same country-pairs over time will create challenges we must address. 26.6.2 Initial Model and Diagnosis Let us start with a simple model regressing GDP on trade and distance, using the variables in their natural units: # Fit the initial model model1 &lt;- lm(real_gdp_prod ~ trade + distance, data = wto) # Look at the results summary(model1) ## ## Call: ## lm(formula = real_gdp_prod ~ trade + distance, data = wto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.961e+24 -1.368e+22 -4.849e+21 2.999e+21 4.010e+25 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.112e+22 8.226e+20 -13.52 &lt;2e-16 *** ## trade 1.001e+16 3.448e+13 290.36 &lt;2e-16 *** ## distance 4.034e+18 1.552e+17 25.99 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.051e+23 on 234594 degrees of freedom ## Multiple R-squared: 0.2646, Adjusted R-squared: 0.2646 ## F-statistic: 4.221e+04 on 2 and 234594 DF, p-value: &lt; 2.2e-16 Both predictors are highly significant with small p-values. Trade has a positive coefficient. More trade is associated with higher GDP. Distance has a positive correlation, which may seem odd, but perhaps suggests that larger economies (like the US) are the most likely to have strong trade ties all around the world. But before we interpret these results, we must check our diagnostics. Let us create the key residual plot: # Extract residuals and fitted values wto$resid1 &lt;- residuals(model1) wto$fitted1 &lt;- fitted(model1) # Create residual plot ggplot(wto, aes(x = fitted1, y = resid1)) + geom_point(alpha = 0.2, size = 1) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_smooth(se = FALSE, color = &quot;blue&quot;, linewidth = 1.5) + labs(title = &quot;Residual Plot: Initial Model&quot;, subtitle = &quot;Multiple serious violations evident&quot;, x = &quot;Fitted Values&quot;, y = &quot;Residuals&quot;) + theme_minimal() ## `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; This plot reveals serious problems. The blue smoothing line shows a strong curved pattern—residuals are negative at high fitted values. This indicates non-linearity. Additionally, there is a clear fan shape, with residuals spreading out dramatically as fitted values increase. This indicates severe heteroscedasticity. There are also some clear outliers, and some suspicious “groupings” that may indicate non-independent errors within clusters. Let us also check the distribution of residuals: ggplot(wto %&gt;% filter(resid1 &gt;= -1.368e+22*5 &amp; resid1 &lt;= 2.999e+21*5), aes(x = resid1)) + geom_histogram(bins = 100, fill = &quot;steelblue&quot;, color = &quot;white&quot;) + labs(title = &quot;Distribution of Residuals: Initial Model&quot;, subtitle = &quot;Fairly extreme skew (truncated display)&quot;, x = &quot;Residuals&quot;, y = &quot;Count&quot;) + theme_minimal() The residuals are left-skewed with a long tail of large negative values (there are also huge outlier residuals not shown on the plot). This is another indication of model misspecification. We cannot trust any of our inference from this model. We need to fix these problems before drawing conclusions. 26.6.3 Fix #1: Variable Transformation The most common solution for both non-linearity and extreme skewness in economic data is logarithmic transformation. Economic variables often have multiplicative rather than additive relationships. GDP and trade volumes span enormous ranges—from small developing economies to global superpowers. Logarithms compress these scales and often reveal linear patterns. Let us transform all three variables and refit: # Fit model with log transformations model2 &lt;- lm(log(real_gdp_prod) ~ log(trade) + log(distance), data = wto) # Look at results summary(model2) ## ## Call: ## lm(formula = log(real_gdp_prod) ~ log(trade) + log(distance), ## data = wto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0587 -1.1190 0.0555 1.1857 14.3889 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.990157 0.042135 806.7 &lt;2e-16 *** ## log(trade) 0.578784 0.001143 506.3 &lt;2e-16 *** ## log(distance) 0.988063 0.004712 209.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.821 on 234594 degrees of freedom ## Multiple R-squared: 0.5372, Adjusted R-squared: 0.5372 ## F-statistic: 1.361e+05 on 2 and 234594 DF, p-value: &lt; 2.2e-16 The coefficients now represent elasticities. A 1% increase in trade is associated with approximately a 0.58% increase in GDP, holding distance constant. A 1% increase in distance is associated with approximately a 0.98% increase in GDP (this positive relationship seems odd but may reflect that large, distant economies trade heavily). More importantly, let us check whether this fixed our diagnostic problems: # Extract new residuals and fitted values wto$resid2 &lt;- residuals(model2) wto$fitted2 &lt;- fitted(model2) # Create residual plot ggplot(wto, aes(x = fitted2, y = resid2)) + geom_point(alpha = 0.2, size = 1) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_smooth(se = FALSE, color = &quot;blue&quot;, linewidth = 1.5) + labs(title = &quot;Residual Plot: Log-Transformed Model&quot;, subtitle = &quot;Major improvement but some curvature remains&quot;, x = &quot;Fitted Values (Log Scale)&quot;, y = &quot;Residuals&quot;) + theme_minimal() ## `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Better! The extreme curvature and fan shape have largely disappeared. The residuals are much more randomly scattered. However, looking closely at the blue smoothing line, we can see it is not perfectly flat. There appears to be remaining curvature, with residuals concentrated above zero at low fitted values. The transformation has improved the model, but we can do better. 26.6.4 Fix #2: Adding Polynomial Terms When log transformation improves linearity but slight curvature remains, we can add polynomial terms. Rather than transforming variables further, we include squared terms to capture non-linear relationships while keeping interpretation somewhat tractable. Let us add a squared term for log(trade): # Fit model with polynomial term model3 &lt;- lm(log(real_gdp_prod) ~ log(trade) + I(log(trade)^2) + log(distance), data = wto) # Look at results summary(model3) ## ## Call: ## lm(formula = log(real_gdp_prod) ~ log(trade) + I(log(trade)^2) + ## log(distance), data = wto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0014 -1.0247 0.0944 1.1196 9.0500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.682088 0.039947 893.229 &lt; 2e-16 *** ## log(trade) 0.007983 0.003079 2.593 0.00951 ** ## I(log(trade)^2) 0.032187 0.000163 197.452 &lt; 2e-16 *** ## log(distance) 1.041282 0.004371 238.210 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.686 on 234593 degrees of freedom ## Multiple R-squared: 0.6031, Adjusted R-squared: 0.6031 ## F-statistic: 1.188e+05 on 3 and 234593 DF, p-value: &lt; 2.2e-16 Both the linear and squared terms for log(trade) are highly significant. The positive coefficient on the squared term suggests increasing returns—the marginal effect of trade on GDP increases as trade volume grows. Check the residual plot: # Extract new residuals and fitted values wto$resid3 &lt;- residuals(model3) wto$fitted3 &lt;- fitted(model3) # Create residual plot ggplot(wto, aes(x = fitted3, y = resid3)) + geom_point(alpha = 0.2, size = 1) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_smooth(se = FALSE, color = &quot;blue&quot;, linewidth = 1.5) + labs(title = &quot;Residual Plot: Model with Polynomial Term&quot;, subtitle = &quot;Curvature resolved but heteroscedasticity remains&quot;, x = &quot;Fitted Values (Log Scale)&quot;, y = &quot;Residuals&quot;) + theme_minimal() ## `geom_smooth()` using method = &#39;gam&#39; and formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Excellent! The smoothing line now stays very close to zero with no systematic curve. We have successfully addressed the non-linearity problem. However, we can still see that the vertical spread of residuals is not quite constant—there is still some heteroscedasticity evident with variance being much higher on the left side of the plot. 26.6.5 Fix #3: Robust Standard Errors Even with improved model specification, some heteroscedasticity remains. Rather than transform variables further (which can make interpretation very difficult), we can fix the inference problem using robust standard errors. Robust standard errors are a more advanced method of calculating standard errors in regression settings that are better able to handle (“robust” to) heteroscedasticity. We can calculate these easily using the sandwich package in R (which we loaded earlier). Let us compare standard and robust standard errors: # Original standard errors coef_original &lt;- coeftest(model3) print(coef_original) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.5682e+01 3.9947e-02 893.2288 &lt; 2.2e-16 *** ## log(trade) 7.9834e-03 3.0785e-03 2.5932 0.009508 ** ## I(log(trade)^2) 3.2187e-02 1.6301e-04 197.4525 &lt; 2.2e-16 *** ## log(distance) 1.0413e+00 4.3713e-03 238.2101 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Robust standard errors (HC1 correction) coef_robust &lt;- coeftest(model3, vcov = vcovHC(model3, type = &quot;HC1&quot;)) print(coef_robust) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.5682e+01 4.5479e-02 784.5795 &lt; 2e-16 *** ## log(trade) 7.9834e-03 4.2905e-03 1.8607 0.06278 . ## I(log(trade)^2) 3.2187e-02 2.1205e-04 151.7904 &lt; 2e-16 *** ## log(distance) 1.0413e+00 4.7997e-03 216.9453 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Compare the standard errors se_comparison &lt;- data.frame( Coefficient = names(coef(model3)), Original_SE = coef_original[, &quot;Std. Error&quot;], Robust_SE = coef_robust[, &quot;Std. Error&quot;], Percent_Change = round(100 * (coef_robust[, &quot;Std. Error&quot;] - coef_original[, &quot;Std. Error&quot;]) / coef_original[, &quot;Std. Error&quot;], 1) ) print(se_comparison) ## Coefficient Original_SE Robust_SE Percent_Change ## (Intercept) (Intercept) 0.0399473117 0.045479251 13.8 ## log(trade) log(trade) 0.0030785383 0.004290445 39.4 ## I(log(trade)^2) I(log(trade)^2) 0.0001630129 0.000212051 30.1 ## log(distance) log(distance) 0.0043712757 0.004799744 9.8 The robust standard errors are larger than the original ones (likely 10-40% larger), confirming that heteroscedasticity was causing us to underestimate uncertainty. Indeed, the coefficient on log(trade) is no longer significant at the 95% level, changing (somewhat) our findings. This will not hold up under further modeling, but does indicate how much this can matter. 26.6.6 Fix #4: Addressing Time Series and Clustering Structure We have fixed the non-linearity and heteroscedasticity problems, but we have not yet addressed a fundamental feature of this data: it has a time series and clustering structure. Each country-pair is observed repeatedly from 1948 to 1999. This creates two related problems: Time trends: Global economic growth over these five decades means GDP naturally increases over time for reasons unrelated to trade or distance. We need to control for these secular trends. Autocorrelation: A country-pair’s GDP this year is highly correlated with its GDP last year. Observations within a country-pair are not independent. Clustering: Observations from the same country-pair are more similar to each other than to observations from different country-pairs. Standard errors that ignore this will be too small. We will address these problems with three techniques: Year fixed effects: Include a dummy variable for each year to control for time-specific shocks affecting all country-pairs Lagged dependent variable: Include previous year’s GDP to account for persistence Clustered standard errors: Adjust standard errors to account for correlation within country-pairs Let’s start by looking at potential issues related to overall time trends. Here, we are coloring residuals based on what decade the observation is coming from. The plot shows that there appears to be some clear clustering in the errors associated by year. wto &lt;- wto %&gt;% arrange(cty1name, cty2name, year) %&gt;% group_by(cty1name, cty2name) %&gt;% mutate(log_gdp_lag = lag(log(real_gdp_prod))) %&gt;% ungroup() # Remove observations with missing lags (first year for each pair) wto_complete &lt;- wto %&gt;% filter(!is.na(log_gdp_lag)) # Create a variable to identify specific years for coloring wto_complete &lt;- wto_complete %&gt;% mutate(year_group = case_when( year %in% c(1948, 1949) ~ &quot;Late 1940s (1948-49)&quot;, year %in% c(1968, 1969) ~ &quot;Late 1960s (1968-69)&quot;, TRUE ~ &quot;Other years&quot; )) # Create the plot with selected years highlighted ggplot(wto_complete, aes(x = fitted3, y = resid3, color = year_group, alpha = year_group)) + geom_point(size = 1.5) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;black&quot;) + scale_color_manual(values = c(&quot;Late 1940s (1948-49)&quot; = &quot;#D55E00&quot;, &quot;Late 1960s (1968-69)&quot; = &quot;#0072B2&quot;, &quot;Other years&quot; = &quot;gray70&quot;), name = &quot;Time Period&quot;) + scale_alpha_manual(values = c(&quot;Late 1940s (1948-49)&quot; = 0.8, &quot;Late 1960s (1968-69)&quot; = 0.8, &quot;Other years&quot; = 0.2), guide = &quot;none&quot;) + labs(title = &quot;Residuals by Time Period&quot;, subtitle = &quot;Systematic differences between 1940s and 1960s suggest time trends&quot;, x = &quot;Fitted Values (Log Scale)&quot;, y = &quot;Residuals&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) But there is also the concern that we are observing the same pairs of countries many times. This can lead to highly correlated errors over time. We can see that by coloring the residuals for a few country pairs and connecting each sequential point by a line. # Create pair identifier if not already created wto_complete &lt;- wto_complete %&gt;% mutate(pair_id = paste(cty1name, cty2name, sep = &quot;-&quot;)) # Start with specific pairs of interest highlight_pairs &lt;- c(&quot;UNITED STATES-UNITED KINGDOM&quot;, &quot;FRANCE-GERMANY&quot;, &quot;JAPAN-UNITED STATES&quot;) # Add 9 random pairs (excluding the ones we already have) set.seed(123) available_pairs &lt;- setdiff(unique(wto_complete$pair_id), highlight_pairs) random_pairs &lt;- sample(available_pairs, 9) # 9 more to get to 12 total # Combine all pairs all_highlight_pairs &lt;- c(highlight_pairs, random_pairs) wto_highlight &lt;- wto_complete %&gt;% filter(pair_id %in% all_highlight_pairs) # Plot all residuals in gray, then overlay highlighted pairs with lines ggplot() + # All residuals in background geom_point(data = wto_complete, aes(x = fitted3, y = resid3), color = &quot;gray70&quot;, alpha = 0.2, size = 0.5) + # Highlighted pairs connected by lines (showing time progression) geom_line(data = wto_highlight, aes(x = fitted3, y = resid3, color = pair_id, group = pair_id), linewidth = 0.8, alpha = 0.7) + geom_point(data = wto_highlight, aes(x = fitted3, y = resid3, color = pair_id), size = 1.5) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;black&quot;) + labs(title = &quot;Residuals for 12 Country-Pairs Showing Clustering and Autocorrelation&quot;, subtitle = &quot;Lines connect same pair over time; clustering evident as colors group together&quot;, x = &quot;Fitted Values (Log Scale)&quot;, y = &quot;Residuals&quot;, color = &quot;Country Pair&quot;) + theme_minimal() + theme(legend.position = &quot;right&quot;, legend.text = element_text(size = 7)) This plot reveals huge amounts of autocorrelation in the data. To address this we’ll add something called a “lagged” variable (basically the outcome variable Y for the previous time period) and while we are at it, we will add our year fixed effects. # Fit model with year fixed effects and lagged DV model4 &lt;- lm(log(real_gdp_prod) ~ log(trade) + I(log(trade)^2) + log(distance) + log_gdp_lag + factor(year), data = wto_complete) # Get robust standard errors coef_robust &lt;- coeftest(model4, vcov = vcovHC(model4, type = &quot;HC1&quot;)) # Show only the main variables (not year dummies) print(coef_robust[1:5, ]) # Intercept + 4 main variables ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.6351481018 1.968132e-02 32.27163 5.913730e-228 ## log(trade) -0.0076497026 6.267003e-04 -12.20632 2.949773e-34 ## I(log(trade)^2) 0.0008727598 3.429893e-05 25.44568 1.263171e-142 ## log(distance) 0.0187586131 8.221902e-04 22.81542 4.374536e-115 ## log_gdp_lag 0.9847360101 4.011845e-04 2454.57140 0.000000e+00 This model includes many year dummy variables controlling for time-specific factors affecting all country-pairs. The lagged GDP term captures persistence—GDP this year strongly depends on GDP last year. Now let’s check whether clustering issues remain: # Extract residuals from model4 wto_complete$resid4 &lt;- residuals(model4) wto_complete$fitted4 &lt;- fitted(model4) # Select specific pairs to highlight set.seed(456) sample_pairs &lt;- sample(unique(wto_complete$pair_id), 10) wto_sample &lt;- wto_complete %&gt;% filter(pair_id %in% sample_pairs) # Plot showing residuals still cluster by country-pair ggplot() + # All residuals in background geom_point(data = wto_complete, aes(x = fitted4, y = resid4), color = &quot;gray70&quot;, alpha = 0.2, size = 0.5) + # Highlighted pairs geom_point(data = wto_sample, aes(x = fitted4, y = resid4, color = pair_id), size = 2, alpha = 0.7) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;black&quot;) + labs(title = &quot;Residuals After Year FE and Lag: Clustering Still Evident&quot;, subtitle = &quot;Same-colored points cluster together, indicating within-pair correlation remains&quot;, x = &quot;Fitted Values (Log Scale)&quot;, y = &quot;Residuals&quot;, color = &quot;Country Pair&quot;) + theme_minimal() + theme(legend.position = &quot;right&quot;, legend.text = element_text(size = 7)) Even after controlling for time trends and autocorrelation, residuals from the same country-pair still cluster together. This means we need clustered standard errors: # Calculate clustered standard errors (clustering by country-pair) coef_clustered &lt;- coeftest(model4, vcov = vcovCL(model4, cluster = ~pair_id)) # Show only the main variables (not year dummies) cat(&quot;Coefficients with Clustered Standard Errors:\\n&quot;) ## Coefficients with Clustered Standard Errors: print(coef_clustered[1:5, ]) # Show first 5 coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.6351481018 2.119988e-02 29.95998 8.064510e-197 ## log(trade) -0.0076497026 7.255656e-04 -10.54309 5.545169e-26 ## I(log(trade)^2) 0.0008727598 3.879185e-05 22.49854 5.725744e-112 ## log(distance) 0.0187586131 8.489883e-04 22.09526 4.592618e-108 ## log_gdp_lag 0.9847360101 4.444775e-04 2215.49100 0.000000e+00 cat(&quot;\\n\\nNote: Model also includes&quot;, length(unique(wto_complete$year)) - 1, &quot;year fixed effects (not shown for brevity).\\n&quot;) ## ## ## Note: Model also includes 50 year fixed effects (not shown for brevity). cat(&quot;Standard errors are clustered by country-pair (&quot;, length(unique(wto_complete$pair_id)), &quot;clusters).\\n&quot;) ## Standard errors are clustered by country-pair ( 11296 clusters). The model also include year fixed effects (not shown for brevity). Standard errors are clustered by country-pair to account for within-pair correlation. These standard errors adjust for both clustering and allow for heteroskedasticity and are slightly larger than those we would get without accounting for clustering. # Get robust (non-clustered) standard errors for model4 coef_robust_only &lt;- coeftest(model4, vcov = vcovHC(model4, type = &quot;HC1&quot;)) # Get clustered standard errors for model4 coef_clustered &lt;- coeftest(model4, vcov = vcovCL(model4, cluster = ~pair_id)) # Create comparison table for main variables se_comparison &lt;- data.frame( Coefficient = rownames(coef_robust_only)[1:5], Robust_SE = coef_robust_only[1:5, &quot;Std. Error&quot;], Clustered_SE = coef_clustered[1:5, &quot;Std. Error&quot;], Percent_Increase = round(100 * (coef_clustered[1:5, &quot;Std. Error&quot;] - coef_robust_only[1:5, &quot;Std. Error&quot;]) / coef_robust_only[1:5, &quot;Std. Error&quot;], 1) ) cat(&quot;Comparison of Robust vs. Clustered Standard Errors:\\n&quot;) ## Comparison of Robust vs. Clustered Standard Errors: print(se_comparison) ## Coefficient Robust_SE Clustered_SE Percent_Increase ## (Intercept) (Intercept) 1.968132e-02 2.119988e-02 7.7 ## log(trade) log(trade) 6.267003e-04 7.255656e-04 15.8 ## I(log(trade)^2) I(log(trade)^2) 3.429893e-05 3.879185e-05 13.1 ## log(distance) log(distance) 8.221902e-04 8.489883e-04 3.3 ## log_gdp_lag log_gdp_lag 4.011845e-04 4.444775e-04 10.8 The clustered standard errors are substantially larger than the robust (but unclustered) standard errors we calculated earlier, confirming that ignoring the clustering structure would make us too confident in our estimates. 26.6.7 Checking the Final Residuals Let’s examine the distribution of residuals from our final model: ggplot(wto_complete, aes(x = resid4)) + geom_histogram(bins = 100, fill = &quot;steelblue&quot;, color = &quot;white&quot;) + coord_cartesian(xlim = c(-1, 1)) + labs(title = &quot;Distribution of Residuals: Final Model&quot;, subtitle = &quot;Much more symmetric after all corrections (truncated at -1 to 1)&quot;, x = &quot;Residuals&quot;, y = &quot;Count&quot;) + theme_minimal() The final residual distribution is dramatically improved compared to our initial model, which showed extreme right skew. The distribution is now much more symmetric and centered around zero, which is what we want to see. However, even with all our corrections, we still see a small number of observations with very large residuals (beyond the -1 to 1 range shown here). This suggests that while we have addressed the major systematic problems in our model, there may still be some remaining issues—perhaps certain country-pairs that behave differently from the general pattern. We will return to strategies for handling these persistent issues in our next reading, where we discuss unit-specific fixed effects that allow each country-pair to have its own baseline level. 26.6.8 Summary: What We Found and What We Did Let us review all the problems we identified and how we addressed them: Issue 1: Non-linearity Identified: Strong curved pattern in initial residual plot; systematic over- and under-prediction Solution: Log transformation of outcome and all predictors Result: Major improvement, but slight curvature remained Issue 2: Remaining curvature Identified: Slight curve in residual plot after log transformation Solution: Added squared term for log(trade) to capture non-linear relationship Result: Curvature resolved; smoothing line stays near zero Issue 3: Heteroscedasticity Identified: Fan shape in residual plot indicating non-constant variance; much higher variance at low fitted values Solution: Used robust standard errors (HC1 correction) Result: Standard errors increased by 10-40%, correcting for underestimated uncertainty Issue 4: Time trends Identified: Residuals from 1940s systematically different from 1960s observations Solution: Added year fixed effects (dummy variables for each year) Result: Controlled for secular trends affecting all country-pairs Issue 5: Autocorrelation Identified: Residuals for same country-pair strongly correlated over time; lines connecting pairs showed clear temporal patterns Solution: Included lagged dependent variable (previous year’s GDP) Result: Autocorrelation substantially reduced; lag coefficient near 0.98 shows massive persistence Issue 6: Clustering Identified: Observations within country-pairs cluster together even after year FE and lag; same-colored points group in residual plots Solution: Clustered standard errors by country-pair (approximately 4,500 clusters) Result: Standard errors increased substantially, properly accounting for within-pair correlation Issue 7: Outliers and extreme residuals Identified: Some observations with very large residuals even in final model; histogram shows observations beyond [-1, 1] range Solution: Documented but retained; will address with unit-specific fixed effects in future material Result: Major systematic issues resolved, but some country-pair-specific patterns remain 26.6.9 Final Model and Substantive Conclusions Our final specification includes: Dependent variable: log(real_gdp_prod) Predictors: log(trade), log(trade)², log(distance) Controls: Lagged log(GDP), 51 year fixed effects Standard errors: Clustered by country-pair Observations: ~233,000 Clusters: ~4,500 country-pairs The final results with clustered standard errors are: Estimate Std. Error t value (Intercept) 0.6351 0.0212 29.96 log(trade) -0.0076 0.0007 -10.54 I(log(trade)^2) 0.0009 0.00004 22.50 log(distance) 0.0188 0.0008 22.10 log_gdp_lag 0.9847 0.0004 2215.49 All coefficients are highly statistically significant (all p-values &lt; 0.001). Does bilateral trade have a positive effect on GDP? The answer is nuanced. The relationship between trade and GDP is non-linear, as evidenced by the significant coefficients on both log(trade) and its square. The linear term is negative (-0.0076) while the squared term is positive (0.0009), creating a U-shaped relationship. The marginal effect of trade depends on the level of trade: at very low trade volumes, additional trade is associated with slightly lower GDP (controlling for distance, year, and lagged GDP). However, at moderate to high trade volumes—which describes most of the observations in our data—the effect becomes positive and strengthening. For country-pairs with log(trade) above approximately 4.4, each 1% increase in trade is associated with an increase in GDP. However, we must interpret this effect carefully. The overwhelmingly dominant predictor is lagged GDP (coefficient of 0.985), meaning that 98.5% of current GDP is explained by previous GDP. The effect of trade, while statistically significant, operates on top of this massive persistence. Trade matters at the margin, but economic relationships are primarily driven by their own history. The distance coefficient (0.019) is positive, which initially seems counterintuitive—shouldn’t greater distance reduce economic ties? But after controlling for trade volume and past GDP, this captures a different effect: for a given level of trade, more distant country-pairs tend to be larger economies. Small countries trade mainly with neighbors; only large economies maintain substantial trade with distant partners. The broader lesson: Our initial simple model suggested strong, clear relationships between trade, distance, and GDP. Our final model, after proper diagnostics and corrections, reveals a far more complex picture. Trade effects are non-linear and operate within a system dominated by temporal persistence. The relationships we thought we understood required careful specification and appropriate inference methods to characterize correctly. This is the reality of empirical work with observational data. Simple models provide simple answers, but those answers may be wrong. Rigorous diagnostics and appropriate corrections provide more trustworthy inference, even when the resulting story becomes more complicated. The goal is not simplicity; the goal is truth. 26.7 When OLS Isn’t Enough: Recognizing Special Cases Throughout this reading, we have worked to fix violations of OLS regression assumptions. But sometimes the fundamental structure of your data means OLS is simply not the right tool, no matter how carefully you diagnose and fix problems. This section briefly describes situations where you need specialized methods beyond ordinary least squares. 26.7.1 Binary Outcomes Suppose your dependent variable is binary: Did the country join the WTO? (Yes/No). Did a citizen vote? (Yes/No). Did a bill pass? (Yes/No). OLS regression can generate predicted probabilities outside the [0,1] range, which is nonsensical. You might predict a 120% probability of voting or a -30% probability of a bill passing. Additionally, binary outcomes violate the homoscedasticity assumption by their nature—the variance depends on the probability, which changes across observations. What you need: Logistic regression (logit) or probit models. These models ensure predictions stay between 0 and 1 and properly handle the binary nature of the outcome. The interpretation changes—coefficients represent effects on log-odds or probability, not direct effects on the outcome. 26.7.2 Categorical Outcomes What if your outcome has multiple unordered categories? Vote choice (Democrat/Republican/Independent), policy preference (increase spending/keep same/decrease spending), or regime type (democracy/autocracy/hybrid)? OLS assumes a meaningful numerical scale for the outcome variable. But there is no natural way to assign numbers that treats the distances between categories appropriately. What you need: Multinomial logistic regression. This models the probability of each category relative to a reference category. Again, interpretation becomes more complex, but the model respects the categorical nature of the outcome. 26.7.3 Count Outcomes What if your outcome is a count that cannot be negative? Number of protests, number of bills sponsored, number of trade disputes, number of terrorist attacks? OLS can predict negative counts (impossible) and does not properly handle the typically right-skewed distribution of count data. The variance of count data typically increases with the mean, violating homoscedasticity. What you need: Poisson regression or negative binomial regression. These models respect the count nature of the outcome and handle overdispersion (when variance exceeds the mean). 26.7.4 Pure Time Series Data Suppose you have a single unit observed over time: U.S. GDP quarterly from 1950-2024, the Federal Reserve interest rate monthly over decades, or public opinion toward the president weekly. The observations are not independent—values at adjacent time points are highly correlated. Trends can create spurious relationships (two variables both trending up will correlate even if unrelated). Standard OLS inference is invalid. What you need: Time series methods like ARIMA (autoregressive integrated moving average), VAR (vector autoregression), or error correction models. These explicitly model temporal dependence and separate short-run dynamics from long-run relationships. Our lagged dependent variable approach provides a simple partial fix, but serious time series analysis requires specialized methods. 26.7.5 Panel and Hierarchical Data What if you have multiple units observed over time (panel data) or observations nested in groups (hierarchical data)? Students within schools, voters within states, countries over years? Observations within units are correlated, violating independence. Units may differ in fundamental ways that do not change over time (unobserved heterogeneity). Standard errors that ignore clustering are too small. What you need: Fixed effects models, random effects models, or multilevel/hierarchical models. These explicitly model the within-unit and between-unit variation differently. Our clustered standard errors provide a simple correction for within-unit correlation, but full panel methods offer more sophisticated approaches to causal inference and handling unobserved differences across units. 26.7.6 The Bottom Line OLS regression is powerful and flexible, but it is not universal. Recognizing when your data structure or outcome type requires specialized methods is a crucial skill. The fact that R will happily run an OLS regression on any data does not mean the results are meaningful or that the inference is valid. The diagnostic skills you have learned in this module help you recognize when OLS assumptions are violated. Sometimes you can fix violations through transformations, robust standard errors, or careful modeling. But sometimes the violation is fundamental to your data structure, and you need to acknowledge that a more advanced method is required. Knowing what you do not know—recognizing the boundaries of OLS and when specialized training is needed—is a sign of statistical maturity. If your research requires these advanced methods, take additional courses in time series analysis, categorical data analysis, or multilevel modeling. 26.8 A Reference Guide to Formal Diagnostic Tests Throughout this module, we have emphasized visual diagnostics using residual plots and related graphs. These plots show you what is wrong and help you understand the nature of the problem. However, formal statistical tests also exist for diagnosing assumption violations. This section provides a reference table of common diagnostic tests. These tests answer yes/no questions about whether assumptions are violated. They are useful for formal reporting and for borderline cases where visual diagnostics are unclear. However, you are not required to memorize or use these tests. Visual diagnostics are typically more informative, and studying these methods in depth is simply beyond the scope of this class. With large datasets, formal tests can be overly sensitive—rejecting null hypotheses even for minor, practically unimportant violations. With small datasets, tests can lack power to detect real problems. Always look at plots first. Use tests as supporting evidence if needed. 26.8.1 Common Diagnostic Tests Problem Test Name What It Does R Function Notes Non-linearity Ramsey RESET Tests whether adding powers of fitted values improves fit lmtest::resettest() Rejects if model is misspecified; doesn’t tell you how Heteroscedasticity Breusch-Pagan Tests whether error variance depends on predictors lmtest::bptest() Very sensitive with large samples Autocorrelation Durbin-Watson Tests for first-order serial correlation lmtest::dwtest() For time series data; assumes first-order correlation Multicollinearity VIF Variance Inflation Factor measures how much variance increases due to collinearity car::vif() Values &gt; 10 indicate serious problems; &gt; 5 warrant attention Influential points Cook’s Distance Measures how much coefficients change when observation removed cooks.distance() Values &gt; 4/n warrant investigation 26.8.2 Using These Tests If you use formal tests, report them alongside visual diagnostics. For example: “The Breusch-Pagan test rejected homoscedasticity (p &lt; 0.001), and the residual plot showed clear fan-shaped heteroscedasticity. We therefore calculated robust standard errors.” Do not rely on tests alone. A significant Breusch-Pagan test tells you heteroscedasticity exists but not what pattern it follows or how severe it is. A residual plot shows you the pattern, helping you decide whether robust standard errors are sufficient or whether you need transformation. Remember: tests answer “is there a problem?” but plots answer “what is the problem and how serious is it?” Both are useful, but plots are more informative for understanding your data and deciding what to do. 26.9 Summary: A Workflow for Careful Regression Analysis We have covered a lot of ground in this module. Let us synthesize what you have learned into a practical workflow you can follow whenever you run regression analyses. 26.9.1 The Diagnostic Workflow 1. Fit your initial model. Start with a theoretically motivated specification using variables in their natural units. Get baseline results even though you know you will likely need to refine the model. 2. Create diagnostic plots. At minimum, create a residual versus fitted plot. Consider also: scale-location plot, Q-Q plot, residuals versus each predictor, and Cook’s distance plot. 3. Identify problems systematically. Look for: Curves in the residual plot (non-linearity) Fan or funnel shapes (heteroscedasticity) Patterns over time or by group (non-independence) Extreme individual points (outliers/influential observations) Very large standard errors or unstable coefficients (multicollinearity) 4. Apply appropriate fixes, prioritizing serious problems: For non-linearity: Try log transformations first (especially for economic data spanning large ranges). If curvature remains, add polynomial terms. For heteroscedasticity: Calculate robust standard errors using the sandwich package. This is easy and effective. For non-independence in grouped data: Use clustered standard errors. The cluster should match the grouping structure (students within schools, country-pairs in our example). For time series structure: Add time fixed effects, trends, or lagged dependent variables. Consider whether specialized time series methods are needed. For outliers: Investigate whether they are errors or genuine unusual cases. Report results with and without them. Do not automatically delete outliers—justify any removals. For multicollinearity: Consider dropping one of the highly correlated predictors, combining them into an index, or acknowledging the limitation. 5. Re-check diagnostics after each major fix. Fixing one problem sometimes reveals another. For example, log transformation often helps both non-linearity and heteroscedasticity. But adding many controls might introduce new collinearity. 6. Report honestly. Describe what problems you found and how you addressed them. Acknowledge limitations you could not fix. This transparency strengthens rather than weakens your work. 26.9.2 Best Practices Always run diagnostics. Even if you expect no problems, check. Surprises are common with real data, and failing to diagnose problems undermines all your subsequent inference. Fix fundamental problems first. Address non-linearity and non-independence before worrying about minor heteroscedasticity. Severe assumption violations make other diagnostics unreliable. Balance validity against interpretability. Each fix you add makes the model more defensible but often harder to interpret. Our final WTO trade model was much more valid than our initial model, but interpretation became substantially more complex. This tradeoff is real and you must navigate it thoughtfully. Use judgment, not just rules. Not every diagnostic flag requires action. Minor deviations from perfection are acceptable, especially if they do not substantially affect your conclusions. Focus on problems that meaningfully threaten inference. Remember the purpose. The goal is not to get a “clean” regression that passes all tests. The goal is to understand your data well enough to draw valid conclusions and honest enough to acknowledge what you cannot know with certainty. 26.9.3 Final Thoughts This module has taught you to be a critical, careful researcher. You now know how to diagnose when regression assumptions are violated, how to fix many common problems, and when to acknowledge that specialized methods beyond OLS are needed. Real data rarely cooperate perfectly. Assumptions are violated, patterns are messy, and tradeoffs are unavoidable. The researchers who produce the most credible work are not those whose analyses look the cleanest, but those who honestly confront problems in their data and address them transparently. 26.10 Review Questions Fan-shaped residuals: You create a residual plot and notice that the vertical spread of points grows systematically larger as fitted values increase, creating a megaphone or fan shape. The smoothing line stays near zero. Which assumption is violated? What is the simplest way to correct your inference? Curved residuals: Your residual plot shows a clear curved pattern, with the smoothing line forming a U-shape (residuals positive at low fitted values, negative in the middle, positive at high fitted values). Which assumption is violated? Name two approaches you might use to fix this problem. Clustered residuals: You are analyzing student test scores from 100 different schools. When you color-code residuals by school in your residual plot, you notice that points from the same school cluster tightly together. Which assumption is violated? What correction should you apply to your standard errors? Time series patterns: You are modeling quarterly GDP data from 1990-2020. When you plot residuals over time, you see that high residuals tend to be followed by high residuals, and low residuals followed by low residuals—the line connecting sequential points shows clear waves rather than random bouncing. Which assumption is violated? Name two ways you might address this problem. Highly correlated predictors: You include both “household income” and “household expenditure” as predictors (which correlate at r = 0.95). You notice that both coefficients have very large standard errors, and the coefficient on income is unexpectedly negative even though income positively correlates with your outcome when examined alone. Which assumption is violated? What are two ways you could address this? Binary outcome problems: A researcher uses OLS to predict whether a student graduates (Yes = 1, No = 0) based on GPA and attendance. The model predicts graduation probabilities of 1.3 for some students and -0.2 for others. What fundamental assumption about the outcome variable is violated? What type of model should be used instead? Systematic prediction errors: Your residual plot shows that residuals are systematically positive (you under-predict) for observations with low values of predictor X, and systematically negative (you over-predict) for observations with high values of X. This creates a slanted cloud of points rather than a random scatter. Which assumption is most likely violated, and what does this suggest about your model specification? Pattern recognition: Match each residual plot pattern to the appropriate assumption violation: Pattern A: Vertical spread increases from left to right (fan shape) Pattern B: Smoothing line curves up, then down, then up Pattern C: Points with same color (representing same group) cluster together Pattern D: Very large standard errors despite strong bivariate correlations Violations: (i) Non-linearity, (ii) Heteroscedasticity, (iii) Clustering/non-independence, (iv) Multicollinearity "],["ds4p-categorical-variables-fixed-effects-and-interactions.html", "27 DS4P: Categorical Variables, Fixed Effects, and Interactions 27.1 Learning Objectives 27.2 What Is This For? 27.3 Part 1: Binary Indicators as Difference in Means 27.4 Part 2: Extending to Categorical Variables 27.5 Part 3: Fixed Effects in Panel Data 27.6 Part 4: Interactions 27.7 Summary and Key Takeaways 27.8 Review Questions", " 27 DS4P: Categorical Variables, Fixed Effects, and Interactions 27.1 Learning Objectives By the end of this module, you will be able to: Understand that including a binary indicator in regression produces the same result as a difference in means test Extend this logic to categorical variables with multiple categories, understanding why we use k-1 dummies and how the baseline affects interpretation Apply this same logic to fixed effects in panel data, understanding when and why to include them Understand interaction terms and interpret them when we believe relationships vary across groups 27.2 What Is This For? Running models with regression can be confusing for beginners. It can be hard to connect your research question to a statistical test. Interpreting regression coefficients can be unintuitive for beginners. And there is a lot of technical jargon! In this final lecture, I want to focus on a particular set of issues that confuses both students and professionals about regression. Interpreting categorical dummy variables Regression with so called “fixed effects” Models with interactions, where the relationship between two variables varies across groups. Mathematically, these topics are all closely related, but it is usually not intuitive or obvious. This reading will help you understand these specific challenges in regression and, in the process, help build a stronger intuition about how multivarite regression works in general. We will again work through these concepts using data on international trade. The World Trade Organization (WTO) and its predecessor, the General Agreement on Tariffs and Trade (GATT), have governed international commerce since 1948. A fundamental question in international economics is whether these trade agreements actually increase trade between member countries. Our data allows us to examine bilateral trade flows between pairs of countries over five decades. Let us load the data and examine its structure: # Load the WTO bilateral trade data wto &lt;- read.csv(&quot;wto_trade.csv&quot;) # Examine the structure glimpse(wto) ## Rows: 234,597 ## Columns: 11 ## $ cty1name &lt;chr&gt; &quot;UNITED STATES&quot;, &quot;UNITED STATES&quot;, &quot;UNITED STATES&quot;, &quot;UNITED STATES&quot;, &quot;UN… ## $ cty2name &lt;chr&gt; &quot;UNITED KINGDOM&quot;, &quot;UNITED KINGDOM&quot;, &quot;UNITED KINGDOM&quot;, &quot;UNITED KINGDOM&quot;,… ## $ year &lt;int&gt; 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959,… ## $ trade &lt;dbl&gt; 20744420, 21027616, 19165702, 28643821, 25336248, 24164768, 24002431, 3… ## $ bothin &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ onein &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ gsp &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ distance &lt;dbl&gt; 4330.469, 4330.469, 4330.469, 4330.469, 4330.469, 4330.469, 4330.469, 4… ## $ real_gdp_prod &lt;dbl&gt; 2.413700e+23, 2.350101e+23, 3.647399e+23, 3.998128e+23, 4.022161e+23, 4… ## $ real_gdp_pc_prod &lt;dbl&gt; 32775161, 31183144, 47324955, 50846402, 50152045, 53763332, 54388570, 6… ## $ area_prod &lt;dbl&gt; 2.357396e+12, 2.357396e+12, 2.357396e+12, 2.357396e+12, 2.357396e+12, 2… # Look at key variables head(wto[, c(&quot;cty1name&quot;, &quot;cty2name&quot;, &quot;year&quot;, &quot;trade&quot;, &quot;bothin&quot;, &quot;onein&quot;)]) ## cty1name cty2name year trade bothin onein ## 1 UNITED STATES UNITED KINGDOM 1948 20744420 1 0 ## 2 UNITED STATES UNITED KINGDOM 1949 21027616 1 0 ## 3 UNITED STATES UNITED KINGDOM 1950 19165702 1 0 ## 4 UNITED STATES UNITED KINGDOM 1951 28643821 1 0 ## 5 UNITED STATES UNITED KINGDOM 1952 25336248 1 0 ## 6 UNITED STATES UNITED KINGDOM 1953 24164768 1 0 Each observation represents a pair of countries in a specific year. The key variables for our analysis are: trade: The value of bilateral trade between the two countries bothin: A binary indicator equal to 1 if both countries are GATT/WTO members onein: A binary indicator equal to 1 if exactly one country is a GATT/WTO member year: The year of observation (1948-1999) real_gdp_pc_prod: The product of GDP per capita for both countries (measuring economic development) The data has a panel structure: the same country-pairs appear repeatedly across years. The United States and United Kingdom, for example, appear as a pair in every year from 1948 to 1999. This structure will become important when we discuss fixed effects. Since trade values span an enormous range, we will work with log-transformed trade throughout this module. As we discussed before, this is standard practice in trade economics and helps us better conform with standard regression assumptions. # Create log trade variable wto &lt;- wto %&gt;% mutate(log_trade = log(trade)) # Check the distribution summary(wto$log_trade) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -16.090 8.118 10.256 10.062 12.272 20.811 27.3 Part 1: Binary Indicators as Difference in Means Let us start with a simple question: Does trade increase when both countries are GATT/WTO members? We have a binary indicator bothin that equals 1 when both countries in a pair are members and 0 otherwise. There are two ways we might approach this question, and understanding that they give identical answers is the key insight of this section. 27.3.1 Approach 1: The T-Test The most direct approach is to compare mean trade levels between the two groups. This is exactly what a t-test does: # Calculate group means wto %&gt;% group_by(bothin) %&gt;% summarise( mean_log_trade = mean(log_trade, na.rm = TRUE), n = n() ) ## # A tibble: 2 × 3 ## bothin mean_log_trade n ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0 9.67 119847 ## 2 1 10.5 114750 # Run a t-test t_test_result &lt;- t.test(log_trade ~ bothin, data = wto, var.equal = TRUE) print(t_test_result) ## ## Two Sample t-test ## ## data: log_trade by bothin ## t = -58.667, df = 234595, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -0.8292641 -0.7756462 ## sample estimates: ## mean in group 0 mean in group 1 ## 9.669148 10.471603 The t-test tells us whether the difference in means is statistically significant. We can see the mean log trade for non-member pairs versus member pairs, and the test gives us a t-statistic and p-value. 27.3.2 Approach 2: Regression with a Binary Indicator Now let us run a simple regression with bothin as the only predictor: # Run regression with binary indicator model_binary &lt;- lm(log_trade ~ bothin, data = wto) summary(model_binary) ## ## Call: ## lm(formula = log_trade ~ bothin, data = wto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.5621 -1.8679 0.2214 2.2202 10.3396 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.669148 0.009566 1010.75 &lt;2e-16 *** ## bothin 0.802455 0.013678 58.67 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.312 on 234595 degrees of freedom ## Multiple R-squared: 0.01446, Adjusted R-squared: 0.01445 ## F-statistic: 3442 on 1 and 234595 DF, p-value: &lt; 2.2e-16 Look carefully at this output and compare it to the t-test results above. 27.3.3 The Key Insight: They Are Identical The regression with a single binary predictor produces exactly the same results as a t-test. Specifically: The intercept equals the mean of the baseline group (when bothin = 0) The coefficient on bothin equals the difference in means between the two groups The t-statistic for the coefficient equals the t-statistic from the t-test The p-value is identical Let us verify this: # Extract the group means means &lt;- wto %&gt;% group_by(bothin) %&gt;% summarise(mean_log_trade = mean(log_trade, na.rm = TRUE)) # Mean when bothin = 0 print(&quot;Mean for bothin == 0&quot;) ## [1] &quot;Mean for bothin == 0&quot; means$mean_log_trade[means$bothin == 0] ## [1] 9.669148 # Regression intercept coef(model_binary)[1] ## (Intercept) ## 9.669148 We can see that the mean for when bothin = 0 and the intercept from the regression are exactly the same. The difference in means is also the same as the regression coefficient for the bothin variable. # Difference in means (should equal coefficient on bothin) diff_in_means &lt;- means$mean_log_trade[means$bothin == 1] - means$mean_log_trade[means$bothin == 0] cat(&quot;Difference in means:&quot;, diff_in_means, &quot;\\n&quot;) ## Difference in means: 0.8024551 cat(&quot;Coefficient on bothin:&quot;, coef(model_binary)[2], &quot;\\n&quot;) ## Coefficient on bothin: 0.8024551 This equivalence is not a coincidence. When you include a binary indicator in a regression, you are asking R to find the best-fitting line through two group means. With only two groups, that line passes exactly through both means. The intercept captures one mean, and the slope captures how much higher (or lower) the other mean is. 27.3.4 Why This Matters Understanding this connection demystifies regression. When you see a coefficient on a binary variable, you can interpret it directly as a difference in means. The regression framework just gives us additional flexibility: we can add more predictors, as we will see throughout this module. This also helps interpret the coefficient substantively. In our example, the coefficient on bothin tells us that country-pairs where both are WTO members have log trade that is about 0.8 units higher, on average, than pairs where at least one country is not a member. That is, the coefficient tells us how much higher or lower the mean is when bothin is equal to one relative to the baseline where bothin equals zero. It is the difference relative to the baseline. 27.4 Part 2: Extending to Categorical Variables The binary case is somewhat straightforward, but what if we want to compare more than two groups? Our data actually distinguishes three situations: neither country is a WTO member, exactly one country is a member, or both countries are members. Let us create a categorical variable that captures all three: # Create a three-category membership variable wto &lt;- wto %&gt;% mutate(membership = case_when( bothin == 1 ~ &quot;Both members&quot;, onein == 1 ~ &quot;One member&quot;, TRUE ~ &quot;Neither member&quot; )) # Check the distribution table(wto$membership) ## ## Both members Neither member One member ## 114750 21037 98810 Now we have three groups to compare. How do we include this in a regression? 27.4.1 The K-1 Dummy Variable Rule With three categories, we need to create dummy variables. But here is the crucial point: we only include k-1 dummies for k categories. With three membership categories, we include two dummy variables, not three. Why? If we included dummies for all three categories plus an intercept, we would have perfect multicollinearity. Knowing the values of any two dummies would perfectly predict the third (since every observation must be in exactly one category). R would not be able to estimate separate effects. Instead, we leave one category out. This becomes the baseline or reference category. All coefficients are then interpreted relative to this baseline. # R automatically creates k-1 dummies when you use factor() model_categorical &lt;- lm(log_trade ~ factor(membership), data = wto) summary(model_categorical) ## ## Call: ## lm(formula = log_trade ~ factor(membership), data = wto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.562 -1.873 0.231 2.220 10.340 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.471603 0.009768 1072.05 &lt;2e-16 *** ## factor(membership)Neither member -1.225641 0.024816 -49.39 &lt;2e-16 *** ## factor(membership)One member -0.712357 0.014360 -49.61 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.309 on 234594 degrees of freedom ## Multiple R-squared: 0.01621, Adjusted R-squared: 0.0162 ## F-statistic: 1933 on 2 and 234594 DF, p-value: &lt; 2.2e-16 27.4.2 Interpreting the Output Look at the coefficient names. R has created dummies for “Neither member” and “One member” but not for “Both members.” This means “Both members” is the baseline category (R chooses alphabetically by default). Here is how to interpret each coefficient: Intercept: The mean log trade for the baseline group (“Both members”) Coefficient on “Neither member”: How much lower (or higher) log trade is for “Neither member” pairs compared to “Both members” pairs Coefficient on “One member”: How much lower (or higher) log trade is for “One member” pairs compared to “Both members” pairs Let us verify by calculating group means directly: # Calculate means for each category means_by_category &lt;- wto %&gt;% group_by(membership) %&gt;% summarise(mean_log_trade = mean(log_trade, na.rm = TRUE)) print(means_by_category) ## # A tibble: 3 × 2 ## membership mean_log_trade ## &lt;chr&gt; &lt;dbl&gt; ## 1 Both members 10.5 ## 2 Neither member 9.25 ## 3 One member 9.76 # The intercept should equal the &quot;Both members&quot; mean cat(&quot;\\nIntercept (Both members mean):&quot;, coef(model_categorical)[1], &quot;\\n&quot;) ## ## Intercept (Both members mean): 10.4716 # Each coefficient should equal that group&#39;s mean minus the baseline mean both_mean &lt;- means_by_category$mean_log_trade[means_by_category$membership == &quot;Both members&quot;] neither_mean &lt;- means_by_category$mean_log_trade[means_by_category$membership == &quot;Neither member&quot;] one_mean &lt;- means_by_category$mean_log_trade[means_by_category$membership == &quot;One member&quot;] cat(&quot;Neither - Both:&quot;, neither_mean - both_mean, &quot;\\n&quot;) ## Neither - Both: -1.225641 cat(&quot;One - Both:&quot;, one_mean - both_mean, &quot;\\n&quot;) ## One - Both: -0.7123575 The coefficients are just differences from the baseline group mean. This is the same logic as the binary case, extended to multiple groups. 27.4.3 How Baseline Choice Affects Interpretation The choice of baseline category changes how the coefficients look, but it does not change the underlying pattern in the data. Let us re-run the regression with “Neither member” as the baseline: # Relevel to make &quot;Neither member&quot; the baseline wto &lt;- wto %&gt;% mutate(membership = factor(membership, levels = c(&quot;Neither member&quot;, &quot;One member&quot;, &quot;Both members&quot;))) # Re-run regression model_categorical_v2 &lt;- lm(log_trade ~ membership, data = wto) summary(model_categorical_v2) ## ## Call: ## lm(formula = log_trade ~ membership, data = wto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.562 -1.873 0.231 2.220 10.340 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.24596 0.02281 405.29 &lt;2e-16 *** ## membershipOne member 0.51328 0.02512 20.43 &lt;2e-16 *** ## membershipBoth members 1.22564 0.02482 49.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.309 on 234594 degrees of freedom ## Multiple R-squared: 0.01621, Adjusted R-squared: 0.0162 ## F-statistic: 1933 on 2 and 234594 DF, p-value: &lt; 2.2e-16 Now the interpretation changes: Intercept: Mean log trade for “Neither member” pairs Coefficient on “One member”: How much higher log trade is when one country is a member, compared to neither being members Coefficient on “Both members”: How much higher log trade is when both are members, compared to neither being members The predicted values for each group are identical in both specifications. Only the presentation differs. But notice how much easier the second version is to interpret substantively: we can directly read off how much WTO membership increases trade relative to having no membership at all. 27.4.4 Practical Guidance on Choosing a Baseline Choose a baseline category that makes your comparisons meaningful: Often the “control” or “default” condition makes a good baseline The baseline should be a category that is easy to understand as a reference point Think about what comparisons you want to make and choose accordingly In our trade example, “Neither member” is a natural baseline because it represents the absence of WTO membership. The coefficients then tell us the “effect” of partial or full membership relative to no membership. 27.5 Part 3: Fixed Effects in Panel Data We have seen how to include categorical variables with a few categories. Fixed effects are simply an extension of this idea to situations with many categories, typically representing groups or time periods in panel data. 27.5.1 The Problem with Pooled Comparisons Our WTO data has a panel structure: we observe the same country-pairs repeatedly over time. When we ran our earlier regressions, we pooled all observations together, comparing member pairs to non-member pairs across the entire dataset. But this comparison might be misleading. Consider: some country-pairs have historically strong trade relationships regardless of WTO membership. The United States and Canada share a border, a language, and deep economic integration. They would trade heavily whether or not they were WTO members. Other pairs, perhaps between small developing countries on different continents, might trade very little regardless of membership. If WTO membership is correlated with these underlying pair characteristics, our estimate of the WTO effect could be biased. We might be attributing to WTO membership what is really due to other factors that make certain pairs natural trading partners. This is the same confounding problem we discussed in earlier modules. The solution is to control for these pair-specific characteristics. 27.5.2 Fixed Effects as Many Dummy Variables Fixed effects are simply dummy variables for each group in our data. If we have data on 1,000 country-pairs, we would include 999 dummy variables (k-1 for k groups). Each dummy captures everything that is specific and constant for that pair: shared borders, common language, colonial history, geographic proximity, and anything else that does not change over time. Similarly, year fixed effects are dummy variables for each year. They capture global shocks that affect all country-pairs in a given year: oil crises, global recessions, major policy changes. This is a powerful strategy widely used in the social sciences. We do not need to know what factors make a specific country-pair different. It could be geography or shared language or anything at all. We also don’t need to measure it. We just need to include a “dummy” indicator for that pair and our model automatically “controlls” for it. Let us see this in action with year fixed effects: # First, let&#39;s see how many years we have n_years &lt;- length(unique(wto$year)) cat(&quot;Number of unique years:&quot;, n_years, &quot;\\n&quot;) ## Number of unique years: 52 # Model with year fixed effects model_year_fe &lt;- lm(log_trade ~ membership + factor(year), data = wto) # Show just the membership coefficients (not all the year dummies) summary(model_year_fe)$coefficients[1:10, ] ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.7387122 0.44240913 26.5336119 6.736627e-155 ## membershipOne member 0.7978435 0.02490083 32.0408411 9.047002e-225 ## membershipBoth members 1.7377856 0.02515295 69.0887512 0.000000e+00 ## factor(year)1949 -0.2207587 0.56266885 -0.3923422 6.948057e-01 ## factor(year)1950 -1.1932703 0.45255849 -2.6367207 8.371724e-03 ## factor(year)1951 -1.1501590 0.45200970 -2.5445448 1.094266e-02 ## factor(year)1952 -1.4164740 0.45151295 -3.1371725 1.706068e-03 ## factor(year)1953 -1.5086279 0.45067826 -3.3474609 8.156831e-04 ## factor(year)1954 -1.3934344 0.45060893 -3.0923365 1.986109e-03 ## factor(year)1955 -1.4612888 0.45003127 -3.2470827 1.166109e-03 We now have 51 year dummy variables in our model (52 years minus 1 baseline). The coefficients on our membership categories have changed because we are now comparing within years rather than across the entire pooled sample. 27.5.3 Two Ways to Think About Fixed Effects There are two equivalent interpretations of what fixed effects do: Interpretation 1: Controlling for group-specific factors. Year fixed effects control for anything that varies across years but affects all country-pairs equally: global economic conditions, oil prices, major geopolitical events. After including year fixed effects, our membership coefficient captures the WTO effect “controlling for” these time-varying global factors. Interpretation 2: Within-group comparisons. With year fixed effects, we are effectively comparing member and non-member pairs within the same year. We are no longer comparing 1950s non-members to 1990s members. This removes confounding from secular trends in both trade and WTO membership. Both interpretations describe the same statistical operation. Choose whichever helps you think about your specific research question. 27.5.4 When to Use Different Types of Fixed Effects Year fixed effects are appropriate when you worry about time trends or global shocks. In trade data, the overall volume of world trade has grown enormously since 1948. Without year fixed effects, our WTO coefficient might partly capture this general trend rather than a true membership effect. Unit fixed effects (country-pair fixed effects in our case) are appropriate when you worry about time-invariant confounders at the unit level. Some pairs are natural trading partners; others are not. Unit fixed effects control for all such permanent differences. Two-way fixed effects (both year and unit) control for both types of confounding simultaneously. This is common in panel data analysis, though it requires sufficient variation within units over time to estimate effects. Let us see how our results change with different fixed effects specifications. To make this easier to do, we are going to use the fixest package in R, which not only allows us to estimate fixed effects, but also allows us to include robust and/or clustered standard errrors (see the previous reading on issues in regressions). library(fixest) # Create pair identifier wto &lt;- wto %&gt;% mutate(pair_id = paste(cty1name, cty2name, sep = &quot;_&quot;)) # No fixed effects wiht robust standard errors model_none &lt;- feols(log_trade ~ membership, data = wto, vcov = &quot;HC1&quot;) # Year fixed effects only model_year &lt;- feols(log_trade ~ membership | year, data = wto, vcov = &quot;HC1&quot;) # Two-way fixed effects with clustered SEs by pair model_twoway &lt;- feols(log_trade ~ membership | year + pair_id, data = wto, vcov = ~pair_id) ## NOTE: 0/854 fixed-effect singletons were removed (854 observations). # Compare the membership coefficients cat(&quot;No fixed effects:\\n&quot;) ## No fixed effects: print(coef(model_none)[2:3]) ## membershipOne member membershipBoth members ## 0.5132832 1.2256406 cat(&quot;\\nYear fixed effects only:\\n&quot;) ## ## Year fixed effects only: print(coef(model_year)[1:2]) ## membershipOne member membershipBoth members ## 0.7978435 1.7377856 cat(&quot;\\nTwo-way fixed effects (with clustered SEs):\\n&quot;) ## ## Two-way fixed effects (with clustered SEs): print(coef(model_twoway)[1:2]) ## membershipOne member membershipBoth members ## 0.1050875 0.2023332 Notice how the coefficients change across specifications. That’s not just an artifact, that is because each model means something different. The simple (pooled) model compares all countries at the same time. The year fixed effect model “controls for” time trends affecting all countries at once. The two way fixed efffects model controls fro both time trends and factors that make certain country pairs different on average than others. With two-way fixed effects, we are making within-pair, within-year comparisons. The membership coefficient now captures the change in trade when a pair’s membership status changes, controlling for everything specific to that pair and everything specific to that year. The fact that coefficients change across specifications tells us that confounding was present. Once we have controlled for these confounders, the estimated effect of WTO membership is much smaller. The “right” specification depends on what comparisons you believe are valid for answering your research question, although applied scholars tend to prefer the more complicated model. 27.6 Part 4: Interactions So far, we have assumed that the effect of WTO membership on trade is the same for everyone. But what if it differs across groups? Perhaps WTO membership matters more for some types of country-pairs than others. Interaction terms allow us to test this possibility. 27.6.1 The Research Question Consider this question: Does WTO membership affect trade differently for economically developed versus less developed country-pairs? There are plausible arguments in both directions. Perhaps developing countries benefit more from WTO membership because they need the institutional framework and most-favored-nation guarantees that reduce trade barriers. Or perhaps wealthy countries benefit more because they have the capacity to take full advantage of trade opportunities that agreements create. To test this, we need to allow the effect of WTO membership to vary based on economic development. This is exactly what an interaction term does. 27.6.2 Setting Up the Interaction First, let us create an indicator for whether a country-pair consists of relatively wealthy countries. We will use the product of GDP per capita (real_gdp_pc_prod) and split at the median: # Create rich_pair indicator based on GDP per capita product wto &lt;- wto %&gt;% mutate(rich_pair = if_else(real_gdp_pc_prod &gt; median(real_gdp_pc_prod, na.rm = TRUE), 1, 0)) # Check the distribution table(wto$rich_pair, useNA = &quot;ifany&quot;) ## ## 0 1 ## 117299 117298 # See how it relates to membership table(wto$membership, wto$rich_pair) ## ## 0 1 ## Neither member 14842 6195 ## One member 50742 48068 ## Both members 51715 63035 Now we can specify a model with an interaction. For simplicity, let us focus on just the bothin indicator rather than the full three-category membership variable: # Model with interaction model_interaction &lt;- lm(log_trade ~ bothin + rich_pair + bothin:rich_pair, data = wto) summary(model_interaction) ## ## Call: ## lm(formula = log_trade ~ bothin + rich_pair + bothin:rich_pair, ## data = wto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.0316 -1.7454 0.2998 2.0881 9.1575 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.92652 0.01215 734.481 &lt;2e-16 *** ## bothin 0.01458 0.01830 0.796 0.426 ## rich_pair 1.64019 0.01806 90.809 &lt;2e-16 *** ## bothin:rich_pair 1.14597 0.02583 44.364 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.112 on 234593 degrees of freedom ## Multiple R-squared: 0.1295, Adjusted R-squared: 0.1295 ## F-statistic: 1.164e+04 on 3 and 234593 DF, p-value: &lt; 2.2e-16 The term bothin:rich_pair is the interaction. We can also write this more compactly using the * operator, which includes both main effects and the interaction: # Equivalent specification using * shorthand model_interaction_v2 &lt;- lm(log_trade ~ bothin * rich_pair, data = wto) summary(model_interaction_v2) ## ## Call: ## lm(formula = log_trade ~ bothin * rich_pair, data = wto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.0316 -1.7454 0.2998 2.0881 9.1575 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.92652 0.01215 734.481 &lt;2e-16 *** ## bothin 0.01458 0.01830 0.796 0.426 ## rich_pair 1.64019 0.01806 90.809 &lt;2e-16 *** ## bothin:rich_pair 1.14597 0.02583 44.364 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.112 on 234593 degrees of freedom ## Multiple R-squared: 0.1295, Adjusted R-squared: 0.1295 ## F-statistic: 1.164e+04 on 3 and 234593 DF, p-value: &lt; 2.2e-16 27.6.3 Interpreting Interaction Coefficients Interaction terms require careful interpretation. The coefficients do not mean what they might seem to mean at first glance. Let us work through each one: The model is: \\[\\text{log(trade)} = \\beta_0 + \\beta_1 \\times \\text{bothin} + \\beta_2 \\times \\text{rich_pair} + \\beta_3 \\times (\\text{bothin} \\times \\text{rich_pair})\\] Intercept (\\(\\beta_0\\)): The expected log trade when bothin = 0 AND rich_pair = 0. This is the baseline: non-member, less developed country-pairs. Coefficient on bothin (\\(\\beta_1\\)): The effect of WTO membership when rich_pair = 0. This is the WTO effect specifically for less developed country-pairs. Coefficient on rich_pair (\\(\\beta_2\\)): The difference between rich and poor pairs when bothin = 0. This is the development effect specifically for non-member pairs. Coefficient on bothin:rich_pair (\\(\\beta_3\\)): How much the WTO effect differs between rich and poor pairs. This is the key interaction term. 27.6.4 A Step-by-Step Method for Interpretation The clearest way to interpret interactions is to write out the prediction equation and plug in specific values. Our model is: \\[\\text{log(trade)} = \\beta_0 + \\beta_1 \\times \\text{bothin} + \\beta_2 \\times \\text{rich\\_pair} + \\beta_3 \\times (\\text{bothin} \\times \\text{rich\\_pair})\\] To understand what each coefficient means, let us work through the predictions for each group separately. For poor country-pairs (rich_pair = 0): When we plug in rich_pair = 0, the equation simplifies considerably: \\[\\text{log(trade)} = \\beta_0 + \\beta_1 \\times \\text{bothin} + \\beta_2 \\times 0 + \\beta_3 \\times \\text{bothin} \\times 0\\] \\[= \\beta_0 + \\beta_1 \\times \\text{bothin}\\] The terms involving rich_pair drop out entirely. For poor pairs, the intercept is \\(\\beta_0\\) (predicted trade when neither is a WTO member) and the effect of WTO membership is simply \\(\\beta_1\\). This is why we say the coefficient on bothin represents the WTO effect when rich_pair equals zero. For rich country-pairs (rich_pair = 1): Now plug in rich_pair = 1: \\[\\text{log(trade)} = \\beta_0 + \\beta_1 \\times \\text{bothin} + \\beta_2 \\times 1 + \\beta_3 \\times \\text{bothin} \\times 1\\] \\[= (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\times \\text{bothin}\\] For rich pairs, the intercept is \\((\\beta_0 + \\beta_2)\\) and the effect of WTO membership is \\((\\beta_1 + \\beta_3)\\). The WTO effect for rich pairs equals the WTO effect for poor pairs plus the interaction coefficient. The interaction coefficient tells us the difference: Comparing the two groups: WTO effect for poor pairs: \\(\\beta_1\\) WTO effect for rich pairs: \\(\\beta_1 + \\beta_3\\) Difference: \\(\\beta_3\\) The interaction coefficient \\(\\beta_3\\) directly measures how much larger (or smaller) the WTO effect is for rich pairs compared to poor pairs. If \\(\\beta_3\\) is positive, WTO membership has a stronger effect for rich pairs. If \\(\\beta_3\\) is negative, the effect is stronger for poor pairs. If \\(\\beta_3\\) is near zero, the effect is similar for both groups. This is the key insight: the interaction term is not the effect for any particular group. It is the difference in effects between groups. 27.6.5 Visualizing the Interaction A picture can make interactions much clearer. Let us plot the predicted values for all four combinations: # Create prediction data pred_data &lt;- expand.grid( bothin = c(0, 1), rich_pair = c(0, 1) ) # Get predictions pred_data$predicted &lt;- predict(model_interaction, newdata = pred_data) # Create labels pred_data &lt;- pred_data %&gt;% mutate( Membership = if_else(bothin == 1, &quot;Both WTO Members&quot;, &quot;Not Both Members&quot;), Development = if_else(rich_pair == 1, &quot;Rich Pairs&quot;, &quot;Poor Pairs&quot;) ) # Plot ggplot(pred_data, aes(x = Membership, y = predicted, color = Development, group = Development)) + geom_point(size = 4) + geom_line(linewidth = 1.2) + labs( title = &quot;WTO Membership Effect by Economic Development&quot;, subtitle = &quot;Interaction between membership and development level&quot;, x = &quot;WTO Membership Status&quot;, y = &quot;Predicted Log Trade&quot;, color = &quot;Country-Pair Type&quot; ) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) In this plot, parallel lines would indicate no interaction (the WTO effect is the same for both groups). Non-parallel lines indicate an interaction. The more the lines diverge or converge, the stronger the interaction. In this case, the plot clearly shows that WTO membership has a strong effect for wealthy countries. However, there is not measurable effect among poorer countries. 27.6.6 Statistical Significance of Interactions The p-value on the interaction term tests whether the difference in slopes is statistically significant. If the interaction is not significant, we cannot confidently conclude that the effect differs across groups, even if the point estimates differ. Keep in mind that interaction effects are often harder to detect than main effects. You need substantial variation in both variables and their combination to estimate interactions precisely. Non-significant interactions do not prove the effects are equal; they may simply reflect insufficient statistical power. 27.7 Summary and Key Takeaways We have covered a lot of ground in this module, but there is a unifying thread connecting everything: we are always making comparisons between groups. Binary indicators compare two groups. The coefficient is literally the difference in means. This is identical to what a t-test does. Categorical variables compare multiple groups to a baseline. Each coefficient tells you how that group differs from the reference category. We use k-1 dummies for k categories to avoid multicollinearity. Fixed effects are categorical variables with many categories, typically representing units or time periods in panel data. They control for everything specific to each group, allowing within-group comparisons that avoid confounding from group-level differences. Interactions allow effects to vary across groups. The interaction coefficient tells you how much the relationship between X and Y differs depending on Z. Understanding these connections helps you make better modeling decisions. When you add a variable to a regression, ask yourself: What comparison am I making? What is my baseline? Do I expect the effect to differ across groups? These questions have clear answers and can all be answered through the regression lens with the right model and the right interpretation. 27.7.1 Common Pitfalls to Avoid Forgetting the baseline. Every categorical variable has a reference category. Make sure you know what it is, or your interpretations will be wrong. Misinterpreting main effects in interaction models. When an interaction is present, the “main effect” of X is the effect when the other variable equals zero. It is not an average or overall effect. Adding fixed effects without understanding the implications. Fixed effects change what comparisons you are making. With unit fixed effects, you can only estimate effects from within-unit variation. If your key variable does not vary within units, you cannot estimate its effect. Treating specification choices as mechanical. Whether to include fixed effects or interactions depends on your research question and assumptions about confounding. There is no universally “correct” specification. 27.8 Review Questions 27.8.1 Conceptual Understanding Explain in your own words why a regression with a single binary predictor gives the same results as a t-test. What does the intercept represent? What does the slope represent? You have a categorical variable with four categories: North, South, East, and West. How many dummy variables do you need to include in a regression? Why not four? Your colleague runs a regression and gets a coefficient of 2.5 on a dummy variable for “South” with “North” as the baseline. They then re-run the regression with “South” as the baseline. Will the coefficient on “North” be -2.5, +2.5, or something else? Explain. In the WTO trade context, what does it mean to include year fixed effects? What types of confounding do they address? What types do they not address? 27.8.2 Interpretation Practice You estimate the following model: log(trade) ~ bothin and get: Intercept: 14.8 bothin coefficient: 1.2 Interpret both numbers in words. What is the predicted log trade for a pair where both are WTO members? You estimate a model with a three-category variable (Neither, One, Both members) using “Neither” as the baseline. You get: Intercept: 13.5 One member: 0.8 Both members: 1.5 What is the predicted log trade for pairs with one member? For pairs with both members? What is the difference in log trade between “One member” and “Both members” pairs? Consider this interaction model output: Intercept: 14.0 bothin: 0.9 rich_pair: 1.8 bothin:rich_pair: 0.5 What is the effect of WTO membership for poor pairs? What is the effect of WTO membership for rich pairs? Is the WTO effect larger for rich or poor pairs, and by how much? What does it mean if the p-value on bothin:rich_pair is 0.23? 27.8.3 Critical Thinking A researcher includes country-pair fixed effects in a trade model and finds that the WTO membership coefficient drops substantially and becomes statistically insignificant. What might explain this? Is this a problem with the analysis, or is it informative? In our interaction example, we split country-pairs into “rich” and “poor” based on the median GDP per capita product. What are some potential problems with this approach? Can you think of alternative ways to examine whether WTO effects vary by development level? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
